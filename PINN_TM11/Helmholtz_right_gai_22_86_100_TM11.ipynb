{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7ca2bb08858fa",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee8681b8789a2a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:14.938346Z",
     "iopub.status.busy": "2023-11-29T01:23:14.937345Z",
     "iopub.status.idle": "2023-11-29T01:23:15.884369Z",
     "shell.execute_reply": "2023-11-29T01:23:15.882367Z",
     "shell.execute_reply.started": "2023-11-29T01:23:14.938346Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-11T12:24:38.226405300Z",
     "start_time": "2024-11-11T12:24:35.314395300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a532958504f33",
   "metadata": {},
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315c2349a9852fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:15.886371Z",
     "iopub.status.busy": "2023-11-29T01:23:15.885370Z",
     "iopub.status.idle": "2023-11-29T01:23:15.897898Z",
     "shell.execute_reply": "2023-11-29T01:23:15.897898Z",
     "shell.execute_reply.started": "2023-11-29T01:23:15.886371Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-11T12:25:33.509340900Z",
     "start_time": "2024-11-11T12:25:33.496808100Z"
    }
   },
   "outputs": [],
   "source": [
    "x_1 = np.linspace(0,22.86,100)  # 256 points between -1 and 1 [256x1]\n",
    "x_2 = np.linspace(10.16,0,100)  # 256 points between 1 and -1 [256x1]\n",
    "\n",
    "X, Y = np.meshgrid(x_1,x_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470b517284feaae",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe31f5d3d3fb0fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:15.898897Z",
     "iopub.status.busy": "2023-11-29T01:23:15.898897Z",
     "iopub.status.idle": "2023-11-29T01:23:15.914899Z",
     "shell.execute_reply": "2023-11-29T01:23:15.913896Z",
     "shell.execute_reply.started": "2023-11-29T01:23:15.898897Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-11T12:25:35.767725200Z",
     "start_time": "2024-11-11T12:25:35.754756Z"
    }
   },
   "outputs": [],
   "source": [
    "X_u_test = np.hstack((X.flatten(order='F')[:,None], Y.flatten(order='F')[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array([0, 0]) #lower bound\n",
    "ub = np.array([22.86, 10.16])  #upper bound\n",
    "\n",
    "a_1 = 1.0/22.86\n",
    "a_2 = 1.0/10.16\n",
    "a_mid = 22.86/2\n",
    "b_mid = 10.16/2\n",
    "\n",
    "k2 = (a_1 * np.pi)**2 + (a_2 * np.pi)**2\n",
    "\n",
    "usol = np.sin(a_1 * np.pi * X) * np.sin(a_2 * np.pi * Y) #solution chosen for convinience  \n",
    "\n",
    "u_true = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f1322329d6b22",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ea1ba83707a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:15.915897Z",
     "iopub.status.busy": "2023-11-29T01:23:15.915897Z",
     "iopub.status.idle": "2023-11-29T01:23:15.928896Z",
     "shell.execute_reply": "2023-11-29T01:23:15.928896Z",
     "shell.execute_reply.started": "2023-11-29T01:23:15.915897Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-11T12:25:37.628353400Z",
     "start_time": "2024-11-11T12:25:37.612793500Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "    \n",
    "    leftedge_x = np.hstack((X[:,0][:,None], Y[:,0][:,None]))\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[:,-1][:,None], Y[:,-1][:,None]))\n",
    "    rightedge_u = usol[:,-1][:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[0,:][:,None], Y[0,:][:,None]))\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[-1,:][:,None], Y[-1,:][:,None]))\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "    \n",
    "    x_00 = np.array([[a_mid, b_mid]])\n",
    "    u_00 = np.array(1.0)  #???\n",
    "    \n",
    "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
    "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])  \n",
    "     \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "    \n",
    "    X_u_train = all_X_u_train[idx[0:N_u], :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx[0:N_u],:]      #choose corresponding u\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points #内部点\n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f = lb + (ub-lb)*lhs(2,N_f) \n",
    "    X_f_train = np.vstack((X_f, X_u_train)) # append training points to collocation points \n",
    "    \n",
    "    return X_f_train, X_u_train, u_train, x_00, u_00, leftedge_x, rightedge_x, topedge_x,bottomedge_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c798e872efc1ce",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522e71187bb50842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:15.929895Z",
     "iopub.status.busy": "2023-11-29T01:23:15.929895Z",
     "iopub.status.idle": "2023-11-29T01:23:15.960898Z",
     "shell.execute_reply": "2023-11-29T01:23:15.959901Z",
     "shell.execute_reply.started": "2023-11-29T01:23:15.929895Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-11T12:25:44.122715100Z",
     "start_time": "2024-11-11T12:25:44.099268700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "    \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "                        \n",
    "        '''     \n",
    "        Alternatively:\n",
    "        \n",
    "        a = self.activation(self.fc1(a))\n",
    "        a = self.activation(self.fc2(a))\n",
    "        a = self.activation(self.fc3(a))\n",
    "        a = self.fc4(a)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def loss_BC(self, x_left, x_right, y_top, y_bottom):\n",
    "        # Enable gradient tracking for the input tensors\n",
    "        x_left.requires_grad = True\n",
    "        x_right.requires_grad = True\n",
    "        y_top.requires_grad = True\n",
    "        y_bottom.requires_grad = True\n",
    "\n",
    "        # Evaluate the neural network predictions at the boundary points\n",
    "        u_left = self.forward(x_left)\n",
    "        u_right = self.forward(x_right)\n",
    "        u_top = self.forward(y_top)\n",
    "        u_bottom = self.forward(y_bottom)\n",
    "\n",
    "        # Calculate the derivatives with respect to x and y\n",
    "        #u_x_left = autograd.grad(u_left, x_left, torch.ones_like(u_left), create_graph=True)[0][:, 0]\n",
    "        #u_x_right = autograd.grad(u_right, x_right, torch.ones_like(u_right), create_graph=True)[0][:, 0]\n",
    "        #u_y_top = autograd.grad(u_top, y_top, torch.ones_like(u_top), create_graph=True)[0][:, 1]\n",
    "        #u_y_bottom = autograd.grad(u_bottom, y_bottom, torch.ones_like(u_bottom), create_graph=True)[0][:, 1]\n",
    "\n",
    "        #   Compute the loss for each boundary\n",
    "        loss_u_left = self.loss_function(u_left, torch.zeros_like(u_left))\n",
    "        loss_u_right = self.loss_function(u_right, torch.zeros_like(u_right))\n",
    "        loss_u_top = self.loss_function(u_top, torch.zeros_like(u_top))\n",
    "        loss_u_bottom = self.loss_function(u_bottom, torch.zeros_like(u_bottom))\n",
    "\n",
    "        # Sum up the losses from all boundaries\n",
    "        loss_u = loss_u_left + loss_u_right + loss_u_top + loss_u_bottom\n",
    "\n",
    "        return loss_u\n",
    "\n",
    "                        \n",
    "    \n",
    "    def loss_initial(self,x_0, y_0):\n",
    "        \n",
    "        loss_0 = self.loss_function(self.forward(x_0), y_0)\n",
    "        return loss_0\n",
    "        \n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "        x_1_f.requires_grad = True\n",
    "        x_2_f.requires_grad = True\n",
    "#         g = x_to_train_f.clone()\n",
    "                        \n",
    "#         g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(torch.cat([x_1_f,x_2_f],-1))\n",
    "                \n",
    "#         u_x = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "#         u_xx = autograd.grad(u_x,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "\n",
    "        u_x = autograd.grad(u,x_1_f,torch.ones_like(x_1_f), retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = autograd.grad(u_x,x_1_f,torch.ones_like(x_1_f), retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "        u_y = autograd.grad(u,x_2_f,torch.ones_like(x_2_f), retain_graph=True, create_graph=True)[0]\n",
    "        u_yy = autograd.grad(u_y,x_2_f,torch.ones_like(x_2_f), retain_graph=True, create_graph=True)[0]\n",
    "                                                            \n",
    "#         u_xx_1 = u_xx[:,[0]]\n",
    "        \n",
    "#         u_xx_2 = u_xx[:,[1]]\n",
    "        \n",
    "        #q = ( -(a_1*np.pi)**2 - (a_2*np.pi)**2 + k2 ) * torch.cos(a_1*np.pi*x_1_f) * torch.cos(a_2*np.pi*x_2_f)\n",
    "                        \n",
    "        f = u_xx + u_yy + k2 * u \n",
    "        \n",
    "        \n",
    "        loss_f = self.loss_function(f,torch.zeros_like(f))\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,x_0,y_0,x_left,x_right,y_top,y_bottom,x_to_train_f):\n",
    "        \n",
    "        loss_0 = self.loss_initial(x_0,y_0)\n",
    "        loss_u = self.loss_BC(x_left,x_right,y_top,y_bottom)\n",
    "        loss_f = self.loss_PDE(x_to_train_f)\n",
    "        \n",
    "        loss_val = loss_0  + loss_f  + loss_u\n",
    "        \n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_val = self.loss(xy_0,u_0,xy_left,xy_right,xy_top,xy_bottom, X_f_train)\n",
    "        \n",
    "        error_vec, _ = PINN.test()\n",
    "        \n",
    "        print(loss,error_vec)\n",
    "        \n",
    "        loss_val.backward()\n",
    "\n",
    "        return loss_val        \n",
    "    \n",
    "    def test(self):\n",
    "                \n",
    "        u_pred = self.forward(X_u_test_tensor)\n",
    "        \n",
    "        error_vec = torch.linalg.norm((u-u_pred),2)/torch.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
    "        \n",
    "        u_pred = np.reshape(u_pred.cpu().detach().numpy(),(100,100),order='F') \n",
    "        \n",
    "        return error_vec, u_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056b672867d5572",
   "metadata": {
    "colab_type": "text",
    "id": "bOjuHdzAhib-"
   },
   "source": [
    "# *Solution Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dac8cd44b689f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:15.962900Z",
     "iopub.status.busy": "2023-11-29T01:23:15.961898Z",
     "iopub.status.idle": "2023-11-29T01:23:15.976897Z",
     "shell.execute_reply": "2023-11-29T01:23:15.975896Z",
     "shell.execute_reply.started": "2023-11-29T01:23:15.962900Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-11T12:31:00.739534400Z",
     "start_time": "2024-11-11T12:31:00.647096300Z"
    }
   },
   "outputs": [],
   "source": [
    "def solutionplot(u_pred,X_u_train,u_train):\n",
    "\n",
    "    #Ground truth\n",
    "    fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pcolor(x_1, x_2, usol, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x(mm)$', fontsize=18)\n",
    "    plt.ylabel(r'$y(mm)$', fontsize=18)\n",
    "    plt.title('Analytical result', fontsize=18)\n",
    "    \n",
    "    \n",
    "    # Prediction\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pcolor(x_1, x_2, u_pred, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x(mm)$', fontsize=18)\n",
    "    plt.ylabel(r'$y(mm)$', fontsize=18)\n",
    "    plt.title('Predicted result by PINN', fontsize=18)\n",
    "    \n",
    "   \n",
    "    # Error\n",
    "    plt.subplot(1, 3, 3)\n",
    "    #plt.pcolor(x_1, x_2, np.abs(usol - u_pred), cmap='jet',vmin=0, vmax=1)\n",
    "    plt.pcolor(x_1, x_2, np.abs(usol - u_pred), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x(mm)$', fontsize=18)\n",
    "    plt.ylabel(r'$y(mm)$', fontsize=18)\n",
    "    plt.title('Absolute error', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('Helmholtz_rec_PINN_TM11_1.png', dpi = 1000, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cea7d3e32e7542",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66a945cf58a4551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T01:23:16.509281Z",
     "iopub.status.busy": "2023-11-29T01:23:16.509281Z",
     "iopub.status.idle": "2023-11-29T01:26:40.324984Z",
     "shell.execute_reply": "2023-11-29T01:26:40.324984Z",
     "shell.execute_reply.started": "2023-11-29T01:23:16.509281Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-11T12:29:36.789615700Z",
     "start_time": "2024-11-11T12:25:46.539612500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "epcho= 0 loss=  tensor(0.9145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lihuang\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9145, grad_fn=<AddBackward0>) tensor(2.9419, grad_fn=<DivBackward0>)\n",
      "epcho= 1 loss=  tensor(8.4619)\n",
      "epcho= 2 loss=  tensor(1.5064)\n",
      "epcho= 3 loss=  tensor(1.5375)\n",
      "epcho= 4 loss=  tensor(2.5045)\n",
      "epcho= 5 loss=  tensor(1.6605)\n",
      "epcho= 6 loss=  tensor(0.8913)\n",
      "epcho= 7 loss=  tensor(1.0880)\n",
      "epcho= 8 loss=  tensor(1.4887)\n",
      "epcho= 9 loss=  tensor(1.3510)\n",
      "epcho= 10 loss=  tensor(1.0017)\n",
      "epcho= 11 loss=  tensor(0.8420)\n",
      "epcho= 12 loss=  tensor(0.9211)\n",
      "epcho= 13 loss=  tensor(1.0506)\n",
      "epcho= 14 loss=  tensor(1.0697)\n",
      "epcho= 15 loss=  tensor(0.9803)\n",
      "epcho= 16 loss=  tensor(0.8759)\n",
      "epcho= 17 loss=  tensor(0.8282)\n",
      "epcho= 18 loss=  tensor(0.8418)\n",
      "epcho= 19 loss=  tensor(0.8767)\n",
      "epcho= 20 loss=  tensor(0.8936)\n",
      "epcho= 21 loss=  tensor(0.8808)\n",
      "epcho= 22 loss=  tensor(0.8514)\n",
      "epcho= 23 loss=  tensor(0.8248)\n",
      "epcho= 24 loss=  tensor(0.8122)\n",
      "epcho= 25 loss=  tensor(0.8130)\n",
      "epcho= 26 loss=  tensor(0.8192)\n",
      "epcho= 27 loss=  tensor(0.8226)\n",
      "epcho= 28 loss=  tensor(0.8189)\n",
      "epcho= 29 loss=  tensor(0.8092)\n",
      "epcho= 30 loss=  tensor(0.7978)\n",
      "epcho= 31 loss=  tensor(0.7891)\n",
      "epcho= 32 loss=  tensor(0.7854)\n",
      "epcho= 33 loss=  tensor(0.7862)\n",
      "epcho= 34 loss=  tensor(0.7889)\n",
      "epcho= 35 loss=  tensor(0.7902)\n",
      "epcho= 36 loss=  tensor(0.7881)\n",
      "epcho= 37 loss=  tensor(0.7823)\n",
      "epcho= 38 loss=  tensor(0.7748)\n",
      "epcho= 39 loss=  tensor(0.7681)\n",
      "epcho= 40 loss=  tensor(0.7639)\n",
      "epcho= 41 loss=  tensor(0.7625)\n",
      "epcho= 42 loss=  tensor(0.7626)\n",
      "epcho= 43 loss=  tensor(0.7621)\n",
      "epcho= 44 loss=  tensor(0.7593)\n",
      "epcho= 45 loss=  tensor(0.7538)\n",
      "epcho= 46 loss=  tensor(0.7465)\n",
      "epcho= 47 loss=  tensor(0.7390)\n",
      "epcho= 48 loss=  tensor(0.7327)\n",
      "epcho= 49 loss=  tensor(0.7278)\n",
      "epcho= 50 loss=  tensor(0.7236)\n",
      "tensor(0.7236, grad_fn=<AddBackward0>) tensor(0.6571, grad_fn=<DivBackward0>)\n",
      "epcho= 51 loss=  tensor(0.7189)\n",
      "epcho= 52 loss=  tensor(0.7122)\n",
      "epcho= 53 loss=  tensor(0.7034)\n",
      "epcho= 54 loss=  tensor(0.6931)\n",
      "epcho= 55 loss=  tensor(0.6827)\n",
      "epcho= 56 loss=  tensor(0.6732)\n",
      "epcho= 57 loss=  tensor(0.6642)\n",
      "epcho= 58 loss=  tensor(0.6549)\n",
      "epcho= 59 loss=  tensor(0.6439)\n",
      "epcho= 60 loss=  tensor(0.6311)\n",
      "epcho= 61 loss=  tensor(0.6175)\n",
      "epcho= 62 loss=  tensor(0.6045)\n",
      "epcho= 63 loss=  tensor(0.5927)\n",
      "epcho= 64 loss=  tensor(0.5813)\n",
      "epcho= 65 loss=  tensor(0.5692)\n",
      "epcho= 66 loss=  tensor(0.5564)\n",
      "epcho= 67 loss=  tensor(0.5439)\n",
      "epcho= 68 loss=  tensor(0.5328)\n",
      "epcho= 69 loss=  tensor(0.5233)\n",
      "epcho= 70 loss=  tensor(0.5143)\n",
      "epcho= 71 loss=  tensor(0.5051)\n",
      "epcho= 72 loss=  tensor(0.4964)\n",
      "epcho= 73 loss=  tensor(0.4890)\n",
      "epcho= 74 loss=  tensor(0.4827)\n",
      "epcho= 75 loss=  tensor(0.4764)\n",
      "epcho= 76 loss=  tensor(0.4697)\n",
      "epcho= 77 loss=  tensor(0.4631)\n",
      "epcho= 78 loss=  tensor(0.4569)\n",
      "epcho= 79 loss=  tensor(0.4504)\n",
      "epcho= 80 loss=  tensor(0.4430)\n",
      "epcho= 81 loss=  tensor(0.4352)\n",
      "epcho= 82 loss=  tensor(0.4274)\n",
      "epcho= 83 loss=  tensor(0.4192)\n",
      "epcho= 84 loss=  tensor(0.4105)\n",
      "epcho= 85 loss=  tensor(0.4016)\n",
      "epcho= 86 loss=  tensor(0.3931)\n",
      "epcho= 87 loss=  tensor(0.3845)\n",
      "epcho= 88 loss=  tensor(0.3757)\n",
      "epcho= 89 loss=  tensor(0.3674)\n",
      "epcho= 90 loss=  tensor(0.3594)\n",
      "epcho= 91 loss=  tensor(0.3515)\n",
      "epcho= 92 loss=  tensor(0.3436)\n",
      "epcho= 93 loss=  tensor(0.3361)\n",
      "epcho= 94 loss=  tensor(0.3285)\n",
      "epcho= 95 loss=  tensor(0.3209)\n",
      "epcho= 96 loss=  tensor(0.3132)\n",
      "epcho= 97 loss=  tensor(0.3055)\n",
      "epcho= 98 loss=  tensor(0.2976)\n",
      "epcho= 99 loss=  tensor(0.2896)\n",
      "epcho= 100 loss=  tensor(0.2815)\n",
      "tensor(0.2815, grad_fn=<AddBackward0>) tensor(0.6590, grad_fn=<DivBackward0>)\n",
      "epcho= 101 loss=  tensor(0.2734)\n",
      "epcho= 102 loss=  tensor(0.2652)\n",
      "epcho= 103 loss=  tensor(0.2570)\n",
      "epcho= 104 loss=  tensor(0.2487)\n",
      "epcho= 105 loss=  tensor(0.2405)\n",
      "epcho= 106 loss=  tensor(0.2324)\n",
      "epcho= 107 loss=  tensor(0.2242)\n",
      "epcho= 108 loss=  tensor(0.2162)\n",
      "epcho= 109 loss=  tensor(0.2083)\n",
      "epcho= 110 loss=  tensor(0.2004)\n",
      "epcho= 111 loss=  tensor(0.1927)\n",
      "epcho= 112 loss=  tensor(0.1852)\n",
      "epcho= 113 loss=  tensor(0.1778)\n",
      "epcho= 114 loss=  tensor(0.1706)\n",
      "epcho= 115 loss=  tensor(0.1635)\n",
      "epcho= 116 loss=  tensor(0.1567)\n",
      "epcho= 117 loss=  tensor(0.1501)\n",
      "epcho= 118 loss=  tensor(0.1437)\n",
      "epcho= 119 loss=  tensor(0.1376)\n",
      "epcho= 120 loss=  tensor(0.1317)\n",
      "epcho= 121 loss=  tensor(0.1259)\n",
      "epcho= 122 loss=  tensor(0.1204)\n",
      "epcho= 123 loss=  tensor(0.1150)\n",
      "epcho= 124 loss=  tensor(0.1098)\n",
      "epcho= 125 loss=  tensor(0.1048)\n",
      "epcho= 126 loss=  tensor(0.0999)\n",
      "epcho= 127 loss=  tensor(0.0951)\n",
      "epcho= 128 loss=  tensor(0.0904)\n",
      "epcho= 129 loss=  tensor(0.0860)\n",
      "epcho= 130 loss=  tensor(0.0817)\n",
      "epcho= 131 loss=  tensor(0.0775)\n",
      "epcho= 132 loss=  tensor(0.0736)\n",
      "epcho= 133 loss=  tensor(0.0698)\n",
      "epcho= 134 loss=  tensor(0.0662)\n",
      "epcho= 135 loss=  tensor(0.0629)\n",
      "epcho= 136 loss=  tensor(0.0597)\n",
      "epcho= 137 loss=  tensor(0.0567)\n",
      "epcho= 138 loss=  tensor(0.0539)\n",
      "epcho= 139 loss=  tensor(0.0513)\n",
      "epcho= 140 loss=  tensor(0.0489)\n",
      "epcho= 141 loss=  tensor(0.0466)\n",
      "epcho= 142 loss=  tensor(0.0445)\n",
      "epcho= 143 loss=  tensor(0.0426)\n",
      "epcho= 144 loss=  tensor(0.0409)\n",
      "epcho= 145 loss=  tensor(0.0393)\n",
      "epcho= 146 loss=  tensor(0.0379)\n",
      "epcho= 147 loss=  tensor(0.0366)\n",
      "epcho= 148 loss=  tensor(0.0354)\n",
      "epcho= 149 loss=  tensor(0.0342)\n",
      "epcho= 150 loss=  tensor(0.0332)\n",
      "tensor(0.0332, grad_fn=<AddBackward0>) tensor(0.4439, grad_fn=<DivBackward0>)\n",
      "epcho= 151 loss=  tensor(0.0322)\n",
      "epcho= 152 loss=  tensor(0.0312)\n",
      "epcho= 153 loss=  tensor(0.0303)\n",
      "epcho= 154 loss=  tensor(0.0294)\n",
      "epcho= 155 loss=  tensor(0.0285)\n",
      "epcho= 156 loss=  tensor(0.0276)\n",
      "epcho= 157 loss=  tensor(0.0268)\n",
      "epcho= 158 loss=  tensor(0.0259)\n",
      "epcho= 159 loss=  tensor(0.0251)\n",
      "epcho= 160 loss=  tensor(0.0243)\n",
      "epcho= 161 loss=  tensor(0.0235)\n",
      "epcho= 162 loss=  tensor(0.0228)\n",
      "epcho= 163 loss=  tensor(0.0221)\n",
      "epcho= 164 loss=  tensor(0.0214)\n",
      "epcho= 165 loss=  tensor(0.0208)\n",
      "epcho= 166 loss=  tensor(0.0202)\n",
      "epcho= 167 loss=  tensor(0.0197)\n",
      "epcho= 168 loss=  tensor(0.0191)\n",
      "epcho= 169 loss=  tensor(0.0187)\n",
      "epcho= 170 loss=  tensor(0.0182)\n",
      "epcho= 171 loss=  tensor(0.0178)\n",
      "epcho= 172 loss=  tensor(0.0174)\n",
      "epcho= 173 loss=  tensor(0.0170)\n",
      "epcho= 174 loss=  tensor(0.0167)\n",
      "epcho= 175 loss=  tensor(0.0163)\n",
      "epcho= 176 loss=  tensor(0.0160)\n",
      "epcho= 177 loss=  tensor(0.0157)\n",
      "epcho= 178 loss=  tensor(0.0154)\n",
      "epcho= 179 loss=  tensor(0.0151)\n",
      "epcho= 180 loss=  tensor(0.0149)\n",
      "epcho= 181 loss=  tensor(0.0146)\n",
      "epcho= 182 loss=  tensor(0.0143)\n",
      "epcho= 183 loss=  tensor(0.0141)\n",
      "epcho= 184 loss=  tensor(0.0138)\n",
      "epcho= 185 loss=  tensor(0.0136)\n",
      "epcho= 186 loss=  tensor(0.0134)\n",
      "epcho= 187 loss=  tensor(0.0132)\n",
      "epcho= 188 loss=  tensor(0.0129)\n",
      "epcho= 189 loss=  tensor(0.0127)\n",
      "epcho= 190 loss=  tensor(0.0125)\n",
      "epcho= 191 loss=  tensor(0.0123)\n",
      "epcho= 192 loss=  tensor(0.0121)\n",
      "epcho= 193 loss=  tensor(0.0119)\n",
      "epcho= 194 loss=  tensor(0.0117)\n",
      "epcho= 195 loss=  tensor(0.0115)\n",
      "epcho= 196 loss=  tensor(0.0114)\n",
      "epcho= 197 loss=  tensor(0.0112)\n",
      "epcho= 198 loss=  tensor(0.0110)\n",
      "epcho= 199 loss=  tensor(0.0108)\n",
      "epcho= 200 loss=  tensor(0.0107)\n",
      "tensor(0.0107, grad_fn=<AddBackward0>) tensor(0.3991, grad_fn=<DivBackward0>)\n",
      "epcho= 201 loss=  tensor(0.0105)\n",
      "epcho= 202 loss=  tensor(0.0104)\n",
      "epcho= 203 loss=  tensor(0.0102)\n",
      "epcho= 204 loss=  tensor(0.0101)\n",
      "epcho= 205 loss=  tensor(0.0100)\n",
      "epcho= 206 loss=  tensor(0.0098)\n",
      "epcho= 207 loss=  tensor(0.0097)\n",
      "epcho= 208 loss=  tensor(0.0096)\n",
      "epcho= 209 loss=  tensor(0.0094)\n",
      "epcho= 210 loss=  tensor(0.0093)\n",
      "epcho= 211 loss=  tensor(0.0092)\n",
      "epcho= 212 loss=  tensor(0.0091)\n",
      "epcho= 213 loss=  tensor(0.0090)\n",
      "epcho= 214 loss=  tensor(0.0088)\n",
      "epcho= 215 loss=  tensor(0.0087)\n",
      "epcho= 216 loss=  tensor(0.0086)\n",
      "epcho= 217 loss=  tensor(0.0085)\n",
      "epcho= 218 loss=  tensor(0.0084)\n",
      "epcho= 219 loss=  tensor(0.0083)\n",
      "epcho= 220 loss=  tensor(0.0082)\n",
      "epcho= 221 loss=  tensor(0.0081)\n",
      "epcho= 222 loss=  tensor(0.0080)\n",
      "epcho= 223 loss=  tensor(0.0079)\n",
      "epcho= 224 loss=  tensor(0.0078)\n",
      "epcho= 225 loss=  tensor(0.0077)\n",
      "epcho= 226 loss=  tensor(0.0076)\n",
      "epcho= 227 loss=  tensor(0.0076)\n",
      "epcho= 228 loss=  tensor(0.0075)\n",
      "epcho= 229 loss=  tensor(0.0074)\n",
      "epcho= 230 loss=  tensor(0.0073)\n",
      "epcho= 231 loss=  tensor(0.0072)\n",
      "epcho= 232 loss=  tensor(0.0071)\n",
      "epcho= 233 loss=  tensor(0.0070)\n",
      "epcho= 234 loss=  tensor(0.0070)\n",
      "epcho= 235 loss=  tensor(0.0069)\n",
      "epcho= 236 loss=  tensor(0.0068)\n",
      "epcho= 237 loss=  tensor(0.0067)\n",
      "epcho= 238 loss=  tensor(0.0067)\n",
      "epcho= 239 loss=  tensor(0.0066)\n",
      "epcho= 240 loss=  tensor(0.0065)\n",
      "epcho= 241 loss=  tensor(0.0064)\n",
      "epcho= 242 loss=  tensor(0.0064)\n",
      "epcho= 243 loss=  tensor(0.0063)\n",
      "epcho= 244 loss=  tensor(0.0062)\n",
      "epcho= 245 loss=  tensor(0.0062)\n",
      "epcho= 246 loss=  tensor(0.0061)\n",
      "epcho= 247 loss=  tensor(0.0060)\n",
      "epcho= 248 loss=  tensor(0.0060)\n",
      "epcho= 249 loss=  tensor(0.0059)\n",
      "epcho= 250 loss=  tensor(0.0059)\n",
      "tensor(0.0059, grad_fn=<AddBackward0>) tensor(0.3441, grad_fn=<DivBackward0>)\n",
      "epcho= 251 loss=  tensor(0.0058)\n",
      "epcho= 252 loss=  tensor(0.0057)\n",
      "epcho= 253 loss=  tensor(0.0057)\n",
      "epcho= 254 loss=  tensor(0.0056)\n",
      "epcho= 255 loss=  tensor(0.0056)\n",
      "epcho= 256 loss=  tensor(0.0055)\n",
      "epcho= 257 loss=  tensor(0.0055)\n",
      "epcho= 258 loss=  tensor(0.0054)\n",
      "epcho= 259 loss=  tensor(0.0053)\n",
      "epcho= 260 loss=  tensor(0.0053)\n",
      "epcho= 261 loss=  tensor(0.0052)\n",
      "epcho= 262 loss=  tensor(0.0052)\n",
      "epcho= 263 loss=  tensor(0.0051)\n",
      "epcho= 264 loss=  tensor(0.0051)\n",
      "epcho= 265 loss=  tensor(0.0050)\n",
      "epcho= 266 loss=  tensor(0.0050)\n",
      "epcho= 267 loss=  tensor(0.0049)\n",
      "epcho= 268 loss=  tensor(0.0049)\n",
      "epcho= 269 loss=  tensor(0.0049)\n",
      "epcho= 270 loss=  tensor(0.0048)\n",
      "epcho= 271 loss=  tensor(0.0048)\n",
      "epcho= 272 loss=  tensor(0.0047)\n",
      "epcho= 273 loss=  tensor(0.0047)\n",
      "epcho= 274 loss=  tensor(0.0046)\n",
      "epcho= 275 loss=  tensor(0.0046)\n",
      "epcho= 276 loss=  tensor(0.0045)\n",
      "epcho= 277 loss=  tensor(0.0045)\n",
      "epcho= 278 loss=  tensor(0.0045)\n",
      "epcho= 279 loss=  tensor(0.0044)\n",
      "epcho= 280 loss=  tensor(0.0044)\n",
      "epcho= 281 loss=  tensor(0.0043)\n",
      "epcho= 282 loss=  tensor(0.0043)\n",
      "epcho= 283 loss=  tensor(0.0043)\n",
      "epcho= 284 loss=  tensor(0.0042)\n",
      "epcho= 285 loss=  tensor(0.0042)\n",
      "epcho= 286 loss=  tensor(0.0041)\n",
      "epcho= 287 loss=  tensor(0.0041)\n",
      "epcho= 288 loss=  tensor(0.0041)\n",
      "epcho= 289 loss=  tensor(0.0040)\n",
      "epcho= 290 loss=  tensor(0.0040)\n",
      "epcho= 291 loss=  tensor(0.0040)\n",
      "epcho= 292 loss=  tensor(0.0039)\n",
      "epcho= 293 loss=  tensor(0.0039)\n",
      "epcho= 294 loss=  tensor(0.0039)\n",
      "epcho= 295 loss=  tensor(0.0038)\n",
      "epcho= 296 loss=  tensor(0.0038)\n",
      "epcho= 297 loss=  tensor(0.0038)\n",
      "epcho= 298 loss=  tensor(0.0037)\n",
      "epcho= 299 loss=  tensor(0.0037)\n",
      "epcho= 300 loss=  tensor(0.0037)\n",
      "tensor(0.0037, grad_fn=<AddBackward0>) tensor(0.3038, grad_fn=<DivBackward0>)\n",
      "epcho= 301 loss=  tensor(0.0036)\n",
      "epcho= 302 loss=  tensor(0.0036)\n",
      "epcho= 303 loss=  tensor(0.0036)\n",
      "epcho= 304 loss=  tensor(0.0036)\n",
      "epcho= 305 loss=  tensor(0.0035)\n",
      "epcho= 306 loss=  tensor(0.0035)\n",
      "epcho= 307 loss=  tensor(0.0035)\n",
      "epcho= 308 loss=  tensor(0.0034)\n",
      "epcho= 309 loss=  tensor(0.0034)\n",
      "epcho= 310 loss=  tensor(0.0034)\n",
      "epcho= 311 loss=  tensor(0.0034)\n",
      "epcho= 312 loss=  tensor(0.0033)\n",
      "epcho= 313 loss=  tensor(0.0033)\n",
      "epcho= 314 loss=  tensor(0.0033)\n",
      "epcho= 315 loss=  tensor(0.0033)\n",
      "epcho= 316 loss=  tensor(0.0032)\n",
      "epcho= 317 loss=  tensor(0.0032)\n",
      "epcho= 318 loss=  tensor(0.0032)\n",
      "epcho= 319 loss=  tensor(0.0032)\n",
      "epcho= 320 loss=  tensor(0.0031)\n",
      "epcho= 321 loss=  tensor(0.0031)\n",
      "epcho= 322 loss=  tensor(0.0031)\n",
      "epcho= 323 loss=  tensor(0.0031)\n",
      "epcho= 324 loss=  tensor(0.0031)\n",
      "epcho= 325 loss=  tensor(0.0030)\n",
      "epcho= 326 loss=  tensor(0.0030)\n",
      "epcho= 327 loss=  tensor(0.0030)\n",
      "epcho= 328 loss=  tensor(0.0030)\n",
      "epcho= 329 loss=  tensor(0.0029)\n",
      "epcho= 330 loss=  tensor(0.0029)\n",
      "epcho= 331 loss=  tensor(0.0029)\n",
      "epcho= 332 loss=  tensor(0.0029)\n",
      "epcho= 333 loss=  tensor(0.0029)\n",
      "epcho= 334 loss=  tensor(0.0028)\n",
      "epcho= 335 loss=  tensor(0.0028)\n",
      "epcho= 336 loss=  tensor(0.0028)\n",
      "epcho= 337 loss=  tensor(0.0028)\n",
      "epcho= 338 loss=  tensor(0.0028)\n",
      "epcho= 339 loss=  tensor(0.0028)\n",
      "epcho= 340 loss=  tensor(0.0027)\n",
      "epcho= 341 loss=  tensor(0.0027)\n",
      "epcho= 342 loss=  tensor(0.0027)\n",
      "epcho= 343 loss=  tensor(0.0027)\n",
      "epcho= 344 loss=  tensor(0.0027)\n",
      "epcho= 345 loss=  tensor(0.0027)\n",
      "epcho= 346 loss=  tensor(0.0026)\n",
      "epcho= 347 loss=  tensor(0.0026)\n",
      "epcho= 348 loss=  tensor(0.0026)\n",
      "epcho= 349 loss=  tensor(0.0026)\n",
      "epcho= 350 loss=  tensor(0.0026)\n",
      "tensor(0.0026, grad_fn=<AddBackward0>) tensor(0.2756, grad_fn=<DivBackward0>)\n",
      "epcho= 351 loss=  tensor(0.0026)\n",
      "epcho= 352 loss=  tensor(0.0025)\n",
      "epcho= 353 loss=  tensor(0.0025)\n",
      "epcho= 354 loss=  tensor(0.0025)\n",
      "epcho= 355 loss=  tensor(0.0025)\n",
      "epcho= 356 loss=  tensor(0.0025)\n",
      "epcho= 357 loss=  tensor(0.0025)\n",
      "epcho= 358 loss=  tensor(0.0025)\n",
      "epcho= 359 loss=  tensor(0.0024)\n",
      "epcho= 360 loss=  tensor(0.0024)\n",
      "epcho= 361 loss=  tensor(0.0024)\n",
      "epcho= 362 loss=  tensor(0.0024)\n",
      "epcho= 363 loss=  tensor(0.0024)\n",
      "epcho= 364 loss=  tensor(0.0024)\n",
      "epcho= 365 loss=  tensor(0.0024)\n",
      "epcho= 366 loss=  tensor(0.0024)\n",
      "epcho= 367 loss=  tensor(0.0023)\n",
      "epcho= 368 loss=  tensor(0.0023)\n",
      "epcho= 369 loss=  tensor(0.0023)\n",
      "epcho= 370 loss=  tensor(0.0023)\n",
      "epcho= 371 loss=  tensor(0.0023)\n",
      "epcho= 372 loss=  tensor(0.0023)\n",
      "epcho= 373 loss=  tensor(0.0023)\n",
      "epcho= 374 loss=  tensor(0.0023)\n",
      "epcho= 375 loss=  tensor(0.0023)\n",
      "epcho= 376 loss=  tensor(0.0022)\n",
      "epcho= 377 loss=  tensor(0.0022)\n",
      "epcho= 378 loss=  tensor(0.0022)\n",
      "epcho= 379 loss=  tensor(0.0022)\n",
      "epcho= 380 loss=  tensor(0.0022)\n",
      "epcho= 381 loss=  tensor(0.0022)\n",
      "epcho= 382 loss=  tensor(0.0022)\n",
      "epcho= 383 loss=  tensor(0.0022)\n",
      "epcho= 384 loss=  tensor(0.0022)\n",
      "epcho= 385 loss=  tensor(0.0022)\n",
      "epcho= 386 loss=  tensor(0.0021)\n",
      "epcho= 387 loss=  tensor(0.0021)\n",
      "epcho= 388 loss=  tensor(0.0021)\n",
      "epcho= 389 loss=  tensor(0.0021)\n",
      "epcho= 390 loss=  tensor(0.0021)\n",
      "epcho= 391 loss=  tensor(0.0021)\n",
      "epcho= 392 loss=  tensor(0.0021)\n",
      "epcho= 393 loss=  tensor(0.0021)\n",
      "epcho= 394 loss=  tensor(0.0021)\n",
      "epcho= 395 loss=  tensor(0.0021)\n",
      "epcho= 396 loss=  tensor(0.0021)\n",
      "epcho= 397 loss=  tensor(0.0020)\n",
      "epcho= 398 loss=  tensor(0.0020)\n",
      "epcho= 399 loss=  tensor(0.0020)\n",
      "epcho= 400 loss=  tensor(0.0020)\n",
      "tensor(0.0020, grad_fn=<AddBackward0>) tensor(0.2568, grad_fn=<DivBackward0>)\n",
      "epcho= 401 loss=  tensor(0.0020)\n",
      "epcho= 402 loss=  tensor(0.0020)\n",
      "epcho= 403 loss=  tensor(0.0020)\n",
      "epcho= 404 loss=  tensor(0.0020)\n",
      "epcho= 405 loss=  tensor(0.0020)\n",
      "epcho= 406 loss=  tensor(0.0020)\n",
      "epcho= 407 loss=  tensor(0.0020)\n",
      "epcho= 408 loss=  tensor(0.0020)\n",
      "epcho= 409 loss=  tensor(0.0020)\n",
      "epcho= 410 loss=  tensor(0.0019)\n",
      "epcho= 411 loss=  tensor(0.0019)\n",
      "epcho= 412 loss=  tensor(0.0019)\n",
      "epcho= 413 loss=  tensor(0.0019)\n",
      "epcho= 414 loss=  tensor(0.0019)\n",
      "epcho= 415 loss=  tensor(0.0019)\n",
      "epcho= 416 loss=  tensor(0.0019)\n",
      "epcho= 417 loss=  tensor(0.0019)\n",
      "epcho= 418 loss=  tensor(0.0019)\n",
      "epcho= 419 loss=  tensor(0.0019)\n",
      "epcho= 420 loss=  tensor(0.0019)\n",
      "epcho= 421 loss=  tensor(0.0019)\n",
      "epcho= 422 loss=  tensor(0.0019)\n",
      "epcho= 423 loss=  tensor(0.0019)\n",
      "epcho= 424 loss=  tensor(0.0019)\n",
      "epcho= 425 loss=  tensor(0.0019)\n",
      "epcho= 426 loss=  tensor(0.0018)\n",
      "epcho= 427 loss=  tensor(0.0018)\n",
      "epcho= 428 loss=  tensor(0.0018)\n",
      "epcho= 429 loss=  tensor(0.0018)\n",
      "epcho= 430 loss=  tensor(0.0018)\n",
      "epcho= 431 loss=  tensor(0.0018)\n",
      "epcho= 432 loss=  tensor(0.0018)\n",
      "epcho= 433 loss=  tensor(0.0018)\n",
      "epcho= 434 loss=  tensor(0.0018)\n",
      "epcho= 435 loss=  tensor(0.0018)\n",
      "epcho= 436 loss=  tensor(0.0018)\n",
      "epcho= 437 loss=  tensor(0.0018)\n",
      "epcho= 438 loss=  tensor(0.0018)\n",
      "epcho= 439 loss=  tensor(0.0018)\n",
      "epcho= 440 loss=  tensor(0.0018)\n",
      "epcho= 441 loss=  tensor(0.0018)\n",
      "epcho= 442 loss=  tensor(0.0018)\n",
      "epcho= 443 loss=  tensor(0.0018)\n",
      "epcho= 444 loss=  tensor(0.0017)\n",
      "epcho= 445 loss=  tensor(0.0017)\n",
      "epcho= 446 loss=  tensor(0.0017)\n",
      "epcho= 447 loss=  tensor(0.0017)\n",
      "epcho= 448 loss=  tensor(0.0017)\n",
      "epcho= 449 loss=  tensor(0.0017)\n",
      "epcho= 450 loss=  tensor(0.0017)\n",
      "tensor(0.0017, grad_fn=<AddBackward0>) tensor(0.2442, grad_fn=<DivBackward0>)\n",
      "epcho= 451 loss=  tensor(0.0017)\n",
      "epcho= 452 loss=  tensor(0.0017)\n",
      "epcho= 453 loss=  tensor(0.0017)\n",
      "epcho= 454 loss=  tensor(0.0017)\n",
      "epcho= 455 loss=  tensor(0.0017)\n",
      "epcho= 456 loss=  tensor(0.0017)\n",
      "epcho= 457 loss=  tensor(0.0017)\n",
      "epcho= 458 loss=  tensor(0.0017)\n",
      "epcho= 459 loss=  tensor(0.0017)\n",
      "epcho= 460 loss=  tensor(0.0017)\n",
      "epcho= 461 loss=  tensor(0.0017)\n",
      "epcho= 462 loss=  tensor(0.0017)\n",
      "epcho= 463 loss=  tensor(0.0017)\n",
      "epcho= 464 loss=  tensor(0.0017)\n",
      "epcho= 465 loss=  tensor(0.0017)\n",
      "epcho= 466 loss=  tensor(0.0017)\n",
      "epcho= 467 loss=  tensor(0.0016)\n",
      "epcho= 468 loss=  tensor(0.0016)\n",
      "epcho= 469 loss=  tensor(0.0016)\n",
      "epcho= 470 loss=  tensor(0.0016)\n",
      "epcho= 471 loss=  tensor(0.0016)\n",
      "epcho= 472 loss=  tensor(0.0016)\n",
      "epcho= 473 loss=  tensor(0.0016)\n",
      "epcho= 474 loss=  tensor(0.0016)\n",
      "epcho= 475 loss=  tensor(0.0016)\n",
      "epcho= 476 loss=  tensor(0.0016)\n",
      "epcho= 477 loss=  tensor(0.0016)\n",
      "epcho= 478 loss=  tensor(0.0016)\n",
      "epcho= 479 loss=  tensor(0.0016)\n",
      "epcho= 480 loss=  tensor(0.0016)\n",
      "epcho= 481 loss=  tensor(0.0016)\n",
      "epcho= 482 loss=  tensor(0.0016)\n",
      "epcho= 483 loss=  tensor(0.0016)\n",
      "epcho= 484 loss=  tensor(0.0016)\n",
      "epcho= 485 loss=  tensor(0.0016)\n",
      "epcho= 486 loss=  tensor(0.0016)\n",
      "epcho= 487 loss=  tensor(0.0016)\n",
      "epcho= 488 loss=  tensor(0.0016)\n",
      "epcho= 489 loss=  tensor(0.0016)\n",
      "epcho= 490 loss=  tensor(0.0016)\n",
      "epcho= 491 loss=  tensor(0.0016)\n",
      "epcho= 492 loss=  tensor(0.0016)\n",
      "epcho= 493 loss=  tensor(0.0016)\n",
      "epcho= 494 loss=  tensor(0.0015)\n",
      "epcho= 495 loss=  tensor(0.0015)\n",
      "epcho= 496 loss=  tensor(0.0015)\n",
      "epcho= 497 loss=  tensor(0.0015)\n",
      "epcho= 498 loss=  tensor(0.0015)\n",
      "epcho= 499 loss=  tensor(0.0015)\n",
      "epcho= 500 loss=  tensor(0.0015)\n",
      "tensor(0.0015, grad_fn=<AddBackward0>) tensor(0.2356, grad_fn=<DivBackward0>)\n",
      "epcho= 501 loss=  tensor(0.0015)\n",
      "epcho= 502 loss=  tensor(0.0015)\n",
      "epcho= 503 loss=  tensor(0.0015)\n",
      "epcho= 504 loss=  tensor(0.0015)\n",
      "epcho= 505 loss=  tensor(0.0015)\n",
      "epcho= 506 loss=  tensor(0.0015)\n",
      "epcho= 507 loss=  tensor(0.0015)\n",
      "epcho= 508 loss=  tensor(0.0015)\n",
      "epcho= 509 loss=  tensor(0.0015)\n",
      "epcho= 510 loss=  tensor(0.0015)\n",
      "epcho= 511 loss=  tensor(0.0015)\n",
      "epcho= 512 loss=  tensor(0.0015)\n",
      "epcho= 513 loss=  tensor(0.0015)\n",
      "epcho= 514 loss=  tensor(0.0015)\n",
      "epcho= 515 loss=  tensor(0.0015)\n",
      "epcho= 516 loss=  tensor(0.0015)\n",
      "epcho= 517 loss=  tensor(0.0015)\n",
      "epcho= 518 loss=  tensor(0.0015)\n",
      "epcho= 519 loss=  tensor(0.0015)\n",
      "epcho= 520 loss=  tensor(0.0015)\n",
      "epcho= 521 loss=  tensor(0.0015)\n",
      "epcho= 522 loss=  tensor(0.0015)\n",
      "epcho= 523 loss=  tensor(0.0015)\n",
      "epcho= 524 loss=  tensor(0.0015)\n",
      "epcho= 525 loss=  tensor(0.0015)\n",
      "epcho= 526 loss=  tensor(0.0014)\n",
      "epcho= 527 loss=  tensor(0.0014)\n",
      "epcho= 528 loss=  tensor(0.0014)\n",
      "epcho= 529 loss=  tensor(0.0014)\n",
      "epcho= 530 loss=  tensor(0.0014)\n",
      "epcho= 531 loss=  tensor(0.0014)\n",
      "epcho= 532 loss=  tensor(0.0014)\n",
      "epcho= 533 loss=  tensor(0.0014)\n",
      "epcho= 534 loss=  tensor(0.0014)\n",
      "epcho= 535 loss=  tensor(0.0014)\n",
      "epcho= 536 loss=  tensor(0.0014)\n",
      "epcho= 537 loss=  tensor(0.0014)\n",
      "epcho= 538 loss=  tensor(0.0014)\n",
      "epcho= 539 loss=  tensor(0.0014)\n",
      "epcho= 540 loss=  tensor(0.0014)\n",
      "epcho= 541 loss=  tensor(0.0014)\n",
      "epcho= 542 loss=  tensor(0.0014)\n",
      "epcho= 543 loss=  tensor(0.0014)\n",
      "epcho= 544 loss=  tensor(0.0014)\n",
      "epcho= 545 loss=  tensor(0.0014)\n",
      "epcho= 546 loss=  tensor(0.0014)\n",
      "epcho= 547 loss=  tensor(0.0014)\n",
      "epcho= 548 loss=  tensor(0.0014)\n",
      "epcho= 549 loss=  tensor(0.0014)\n",
      "epcho= 550 loss=  tensor(0.0014)\n",
      "tensor(0.0014, grad_fn=<AddBackward0>) tensor(0.2293, grad_fn=<DivBackward0>)\n",
      "epcho= 551 loss=  tensor(0.0014)\n",
      "epcho= 552 loss=  tensor(0.0014)\n",
      "epcho= 553 loss=  tensor(0.0014)\n",
      "epcho= 554 loss=  tensor(0.0014)\n",
      "epcho= 555 loss=  tensor(0.0014)\n",
      "epcho= 556 loss=  tensor(0.0014)\n",
      "epcho= 557 loss=  tensor(0.0014)\n",
      "epcho= 558 loss=  tensor(0.0014)\n",
      "epcho= 559 loss=  tensor(0.0014)\n",
      "epcho= 560 loss=  tensor(0.0014)\n",
      "epcho= 561 loss=  tensor(0.0014)\n",
      "epcho= 562 loss=  tensor(0.0014)\n",
      "epcho= 563 loss=  tensor(0.0014)\n",
      "epcho= 564 loss=  tensor(0.0014)\n",
      "epcho= 565 loss=  tensor(0.0014)\n",
      "epcho= 566 loss=  tensor(0.0014)\n",
      "epcho= 567 loss=  tensor(0.0013)\n",
      "epcho= 568 loss=  tensor(0.0013)\n",
      "epcho= 569 loss=  tensor(0.0013)\n",
      "epcho= 570 loss=  tensor(0.0013)\n",
      "epcho= 571 loss=  tensor(0.0013)\n",
      "epcho= 572 loss=  tensor(0.0013)\n",
      "epcho= 573 loss=  tensor(0.0013)\n",
      "epcho= 574 loss=  tensor(0.0013)\n",
      "epcho= 575 loss=  tensor(0.0013)\n",
      "epcho= 576 loss=  tensor(0.0013)\n",
      "epcho= 577 loss=  tensor(0.0013)\n",
      "epcho= 578 loss=  tensor(0.0013)\n",
      "epcho= 579 loss=  tensor(0.0013)\n",
      "epcho= 580 loss=  tensor(0.0013)\n",
      "epcho= 581 loss=  tensor(0.0013)\n",
      "epcho= 582 loss=  tensor(0.0013)\n",
      "epcho= 583 loss=  tensor(0.0013)\n",
      "epcho= 584 loss=  tensor(0.0013)\n",
      "epcho= 585 loss=  tensor(0.0013)\n",
      "epcho= 586 loss=  tensor(0.0013)\n",
      "epcho= 587 loss=  tensor(0.0013)\n",
      "epcho= 588 loss=  tensor(0.0013)\n",
      "epcho= 589 loss=  tensor(0.0013)\n",
      "epcho= 590 loss=  tensor(0.0013)\n",
      "epcho= 591 loss=  tensor(0.0013)\n",
      "epcho= 592 loss=  tensor(0.0013)\n",
      "epcho= 593 loss=  tensor(0.0013)\n",
      "epcho= 594 loss=  tensor(0.0013)\n",
      "epcho= 595 loss=  tensor(0.0013)\n",
      "epcho= 596 loss=  tensor(0.0013)\n",
      "epcho= 597 loss=  tensor(0.0013)\n",
      "epcho= 598 loss=  tensor(0.0013)\n",
      "epcho= 599 loss=  tensor(0.0013)\n",
      "epcho= 600 loss=  tensor(0.0013)\n",
      "tensor(0.0013, grad_fn=<AddBackward0>) tensor(0.2245, grad_fn=<DivBackward0>)\n",
      "epcho= 601 loss=  tensor(0.0013)\n",
      "epcho= 602 loss=  tensor(0.0013)\n",
      "epcho= 603 loss=  tensor(0.0013)\n",
      "epcho= 604 loss=  tensor(0.0013)\n",
      "epcho= 605 loss=  tensor(0.0013)\n",
      "epcho= 606 loss=  tensor(0.0013)\n",
      "epcho= 607 loss=  tensor(0.0013)\n",
      "epcho= 608 loss=  tensor(0.0013)\n",
      "epcho= 609 loss=  tensor(0.0013)\n",
      "epcho= 610 loss=  tensor(0.0013)\n",
      "epcho= 611 loss=  tensor(0.0013)\n",
      "epcho= 612 loss=  tensor(0.0013)\n",
      "epcho= 613 loss=  tensor(0.0013)\n",
      "epcho= 614 loss=  tensor(0.0013)\n",
      "epcho= 615 loss=  tensor(0.0013)\n",
      "epcho= 616 loss=  tensor(0.0012)\n",
      "epcho= 617 loss=  tensor(0.0012)\n",
      "epcho= 618 loss=  tensor(0.0012)\n",
      "epcho= 619 loss=  tensor(0.0012)\n",
      "epcho= 620 loss=  tensor(0.0012)\n",
      "epcho= 621 loss=  tensor(0.0012)\n",
      "epcho= 622 loss=  tensor(0.0012)\n",
      "epcho= 623 loss=  tensor(0.0012)\n",
      "epcho= 624 loss=  tensor(0.0012)\n",
      "epcho= 625 loss=  tensor(0.0012)\n",
      "epcho= 626 loss=  tensor(0.0012)\n",
      "epcho= 627 loss=  tensor(0.0012)\n",
      "epcho= 628 loss=  tensor(0.0012)\n",
      "epcho= 629 loss=  tensor(0.0012)\n",
      "epcho= 630 loss=  tensor(0.0012)\n",
      "epcho= 631 loss=  tensor(0.0012)\n",
      "epcho= 632 loss=  tensor(0.0012)\n",
      "epcho= 633 loss=  tensor(0.0012)\n",
      "epcho= 634 loss=  tensor(0.0012)\n",
      "epcho= 635 loss=  tensor(0.0012)\n",
      "epcho= 636 loss=  tensor(0.0012)\n",
      "epcho= 637 loss=  tensor(0.0012)\n",
      "epcho= 638 loss=  tensor(0.0012)\n",
      "epcho= 639 loss=  tensor(0.0012)\n",
      "epcho= 640 loss=  tensor(0.0012)\n",
      "epcho= 641 loss=  tensor(0.0012)\n",
      "epcho= 642 loss=  tensor(0.0012)\n",
      "epcho= 643 loss=  tensor(0.0012)\n",
      "epcho= 644 loss=  tensor(0.0012)\n",
      "epcho= 645 loss=  tensor(0.0012)\n",
      "epcho= 646 loss=  tensor(0.0012)\n",
      "epcho= 647 loss=  tensor(0.0012)\n",
      "epcho= 648 loss=  tensor(0.0012)\n",
      "epcho= 649 loss=  tensor(0.0012)\n",
      "epcho= 650 loss=  tensor(0.0012)\n",
      "tensor(0.0012, grad_fn=<AddBackward0>) tensor(0.2206, grad_fn=<DivBackward0>)\n",
      "epcho= 651 loss=  tensor(0.0012)\n",
      "epcho= 652 loss=  tensor(0.0012)\n",
      "epcho= 653 loss=  tensor(0.0012)\n",
      "epcho= 654 loss=  tensor(0.0012)\n",
      "epcho= 655 loss=  tensor(0.0012)\n",
      "epcho= 656 loss=  tensor(0.0012)\n",
      "epcho= 657 loss=  tensor(0.0012)\n",
      "epcho= 658 loss=  tensor(0.0012)\n",
      "epcho= 659 loss=  tensor(0.0012)\n",
      "epcho= 660 loss=  tensor(0.0012)\n",
      "epcho= 661 loss=  tensor(0.0012)\n",
      "epcho= 662 loss=  tensor(0.0012)\n",
      "epcho= 663 loss=  tensor(0.0012)\n",
      "epcho= 664 loss=  tensor(0.0012)\n",
      "epcho= 665 loss=  tensor(0.0012)\n",
      "epcho= 666 loss=  tensor(0.0012)\n",
      "epcho= 667 loss=  tensor(0.0012)\n",
      "epcho= 668 loss=  tensor(0.0012)\n",
      "epcho= 669 loss=  tensor(0.0012)\n",
      "epcho= 670 loss=  tensor(0.0012)\n",
      "epcho= 671 loss=  tensor(0.0012)\n",
      "epcho= 672 loss=  tensor(0.0012)\n",
      "epcho= 673 loss=  tensor(0.0012)\n",
      "epcho= 674 loss=  tensor(0.0012)\n",
      "epcho= 675 loss=  tensor(0.0012)\n",
      "epcho= 676 loss=  tensor(0.0012)\n",
      "epcho= 677 loss=  tensor(0.0011)\n",
      "epcho= 678 loss=  tensor(0.0011)\n",
      "epcho= 679 loss=  tensor(0.0011)\n",
      "epcho= 680 loss=  tensor(0.0011)\n",
      "epcho= 681 loss=  tensor(0.0011)\n",
      "epcho= 682 loss=  tensor(0.0011)\n",
      "epcho= 683 loss=  tensor(0.0011)\n",
      "epcho= 684 loss=  tensor(0.0011)\n",
      "epcho= 685 loss=  tensor(0.0011)\n",
      "epcho= 686 loss=  tensor(0.0011)\n",
      "epcho= 687 loss=  tensor(0.0011)\n",
      "epcho= 688 loss=  tensor(0.0011)\n",
      "epcho= 689 loss=  tensor(0.0012)\n",
      "epcho= 690 loss=  tensor(0.0012)\n",
      "epcho= 691 loss=  tensor(0.0013)\n",
      "epcho= 692 loss=  tensor(0.0014)\n",
      "epcho= 693 loss=  tensor(0.0017)\n",
      "epcho= 694 loss=  tensor(0.0023)\n",
      "epcho= 695 loss=  tensor(0.0036)\n",
      "epcho= 696 loss=  tensor(0.0060)\n",
      "epcho= 697 loss=  tensor(0.0105)\n",
      "epcho= 698 loss=  tensor(0.0167)\n",
      "epcho= 699 loss=  tensor(0.0228)\n",
      "epcho= 700 loss=  tensor(0.0221)\n",
      "tensor(0.0221, grad_fn=<AddBackward0>) tensor(0.1725, grad_fn=<DivBackward0>)\n",
      "epcho= 701 loss=  tensor(0.0122)\n",
      "epcho= 702 loss=  tensor(0.0020)\n",
      "epcho= 703 loss=  tensor(0.0038)\n",
      "epcho= 704 loss=  tensor(0.0114)\n",
      "epcho= 705 loss=  tensor(0.0103)\n",
      "epcho= 706 loss=  tensor(0.0029)\n",
      "epcho= 707 loss=  tensor(0.0020)\n",
      "epcho= 708 loss=  tensor(0.0070)\n",
      "epcho= 709 loss=  tensor(0.0067)\n",
      "epcho= 710 loss=  tensor(0.0019)\n",
      "epcho= 711 loss=  tensor(0.0022)\n",
      "epcho= 712 loss=  tensor(0.0054)\n",
      "epcho= 713 loss=  tensor(0.0038)\n",
      "epcho= 714 loss=  tensor(0.0012)\n",
      "epcho= 715 loss=  tensor(0.0028)\n",
      "epcho= 716 loss=  tensor(0.0039)\n",
      "epcho= 717 loss=  tensor(0.0019)\n",
      "epcho= 718 loss=  tensor(0.0014)\n",
      "epcho= 719 loss=  tensor(0.0029)\n",
      "epcho= 720 loss=  tensor(0.0025)\n",
      "epcho= 721 loss=  tensor(0.0011)\n",
      "epcho= 722 loss=  tensor(0.0019)\n",
      "epcho= 723 loss=  tensor(0.0024)\n",
      "epcho= 724 loss=  tensor(0.0014)\n",
      "epcho= 725 loss=  tensor(0.0013)\n",
      "epcho= 726 loss=  tensor(0.0020)\n",
      "epcho= 727 loss=  tensor(0.0017)\n",
      "epcho= 728 loss=  tensor(0.0011)\n",
      "epcho= 729 loss=  tensor(0.0016)\n",
      "epcho= 730 loss=  tensor(0.0017)\n",
      "epcho= 731 loss=  tensor(0.0012)\n",
      "epcho= 732 loss=  tensor(0.0012)\n",
      "epcho= 733 loss=  tensor(0.0016)\n",
      "epcho= 734 loss=  tensor(0.0013)\n",
      "epcho= 735 loss=  tensor(0.0011)\n",
      "epcho= 736 loss=  tensor(0.0014)\n",
      "epcho= 737 loss=  tensor(0.0014)\n",
      "epcho= 738 loss=  tensor(0.0011)\n",
      "epcho= 739 loss=  tensor(0.0012)\n",
      "epcho= 740 loss=  tensor(0.0013)\n",
      "epcho= 741 loss=  tensor(0.0012)\n",
      "epcho= 742 loss=  tensor(0.0011)\n",
      "epcho= 743 loss=  tensor(0.0012)\n",
      "epcho= 744 loss=  tensor(0.0012)\n",
      "epcho= 745 loss=  tensor(0.0011)\n",
      "epcho= 746 loss=  tensor(0.0011)\n",
      "epcho= 747 loss=  tensor(0.0012)\n",
      "epcho= 748 loss=  tensor(0.0011)\n",
      "epcho= 749 loss=  tensor(0.0011)\n",
      "epcho= 750 loss=  tensor(0.0011)\n",
      "tensor(0.0011, grad_fn=<AddBackward0>) tensor(0.2121, grad_fn=<DivBackward0>)\n",
      "epcho= 751 loss=  tensor(0.0011)\n",
      "epcho= 752 loss=  tensor(0.0011)\n",
      "epcho= 753 loss=  tensor(0.0011)\n",
      "epcho= 754 loss=  tensor(0.0011)\n",
      "epcho= 755 loss=  tensor(0.0011)\n",
      "epcho= 756 loss=  tensor(0.0011)\n",
      "epcho= 757 loss=  tensor(0.0011)\n",
      "epcho= 758 loss=  tensor(0.0011)\n",
      "epcho= 759 loss=  tensor(0.0011)\n",
      "epcho= 760 loss=  tensor(0.0011)\n",
      "epcho= 761 loss=  tensor(0.0011)\n",
      "epcho= 762 loss=  tensor(0.0011)\n",
      "epcho= 763 loss=  tensor(0.0011)\n",
      "epcho= 764 loss=  tensor(0.0011)\n",
      "epcho= 765 loss=  tensor(0.0011)\n",
      "epcho= 766 loss=  tensor(0.0011)\n",
      "epcho= 767 loss=  tensor(0.0011)\n",
      "epcho= 768 loss=  tensor(0.0011)\n",
      "epcho= 769 loss=  tensor(0.0011)\n",
      "epcho= 770 loss=  tensor(0.0011)\n",
      "epcho= 771 loss=  tensor(0.0011)\n",
      "epcho= 772 loss=  tensor(0.0011)\n",
      "epcho= 773 loss=  tensor(0.0011)\n",
      "epcho= 774 loss=  tensor(0.0011)\n",
      "epcho= 775 loss=  tensor(0.0011)\n",
      "epcho= 776 loss=  tensor(0.0011)\n",
      "epcho= 777 loss=  tensor(0.0011)\n",
      "epcho= 778 loss=  tensor(0.0011)\n",
      "epcho= 779 loss=  tensor(0.0011)\n",
      "epcho= 780 loss=  tensor(0.0010)\n",
      "epcho= 781 loss=  tensor(0.0010)\n",
      "epcho= 782 loss=  tensor(0.0010)\n",
      "epcho= 783 loss=  tensor(0.0010)\n",
      "epcho= 784 loss=  tensor(0.0010)\n",
      "epcho= 785 loss=  tensor(0.0010)\n",
      "epcho= 786 loss=  tensor(0.0010)\n",
      "epcho= 787 loss=  tensor(0.0010)\n",
      "epcho= 788 loss=  tensor(0.0010)\n",
      "epcho= 789 loss=  tensor(0.0010)\n",
      "epcho= 790 loss=  tensor(0.0010)\n",
      "epcho= 791 loss=  tensor(0.0010)\n",
      "epcho= 792 loss=  tensor(0.0010)\n",
      "epcho= 793 loss=  tensor(0.0010)\n",
      "epcho= 794 loss=  tensor(0.0010)\n",
      "epcho= 795 loss=  tensor(0.0010)\n",
      "epcho= 796 loss=  tensor(0.0010)\n",
      "epcho= 797 loss=  tensor(0.0010)\n",
      "epcho= 798 loss=  tensor(0.0010)\n",
      "epcho= 799 loss=  tensor(0.0010)\n",
      "epcho= 800 loss=  tensor(0.0010)\n",
      "tensor(0.0010, grad_fn=<AddBackward0>) tensor(0.2131, grad_fn=<DivBackward0>)\n",
      "epcho= 801 loss=  tensor(0.0010)\n",
      "epcho= 802 loss=  tensor(0.0010)\n",
      "epcho= 803 loss=  tensor(0.0010)\n",
      "epcho= 804 loss=  tensor(0.0010)\n",
      "epcho= 805 loss=  tensor(0.0010)\n",
      "epcho= 806 loss=  tensor(0.0010)\n",
      "epcho= 807 loss=  tensor(0.0010)\n",
      "epcho= 808 loss=  tensor(0.0010)\n",
      "epcho= 809 loss=  tensor(0.0010)\n",
      "epcho= 810 loss=  tensor(0.0010)\n",
      "epcho= 811 loss=  tensor(0.0010)\n",
      "epcho= 812 loss=  tensor(0.0010)\n",
      "epcho= 813 loss=  tensor(0.0010)\n",
      "epcho= 814 loss=  tensor(0.0010)\n",
      "epcho= 815 loss=  tensor(0.0010)\n",
      "epcho= 816 loss=  tensor(0.0010)\n",
      "epcho= 817 loss=  tensor(0.0010)\n",
      "epcho= 818 loss=  tensor(0.0010)\n",
      "epcho= 819 loss=  tensor(0.0010)\n",
      "epcho= 820 loss=  tensor(0.0010)\n",
      "epcho= 821 loss=  tensor(0.0010)\n",
      "epcho= 822 loss=  tensor(0.0010)\n",
      "epcho= 823 loss=  tensor(0.0010)\n",
      "epcho= 824 loss=  tensor(0.0010)\n",
      "epcho= 825 loss=  tensor(0.0010)\n",
      "epcho= 826 loss=  tensor(0.0010)\n",
      "epcho= 827 loss=  tensor(0.0010)\n",
      "epcho= 828 loss=  tensor(0.0010)\n",
      "epcho= 829 loss=  tensor(0.0010)\n",
      "epcho= 830 loss=  tensor(0.0010)\n",
      "epcho= 831 loss=  tensor(0.0010)\n",
      "epcho= 832 loss=  tensor(0.0010)\n",
      "epcho= 833 loss=  tensor(0.0010)\n",
      "epcho= 834 loss=  tensor(0.0010)\n",
      "epcho= 835 loss=  tensor(0.0010)\n",
      "epcho= 836 loss=  tensor(0.0010)\n",
      "epcho= 837 loss=  tensor(0.0010)\n",
      "epcho= 838 loss=  tensor(0.0010)\n",
      "epcho= 839 loss=  tensor(0.0010)\n",
      "epcho= 840 loss=  tensor(0.0010)\n",
      "epcho= 841 loss=  tensor(0.0010)\n",
      "epcho= 842 loss=  tensor(0.0010)\n",
      "epcho= 843 loss=  tensor(0.0010)\n",
      "epcho= 844 loss=  tensor(0.0010)\n",
      "epcho= 845 loss=  tensor(0.0010)\n",
      "epcho= 846 loss=  tensor(0.0010)\n",
      "epcho= 847 loss=  tensor(0.0010)\n",
      "epcho= 848 loss=  tensor(0.0010)\n",
      "epcho= 849 loss=  tensor(0.0010)\n",
      "epcho= 850 loss=  tensor(0.0010)\n",
      "tensor(0.0010, grad_fn=<AddBackward0>) tensor(0.2109, grad_fn=<DivBackward0>)\n",
      "epcho= 851 loss=  tensor(0.0010)\n",
      "epcho= 852 loss=  tensor(0.0010)\n",
      "epcho= 853 loss=  tensor(0.0010)\n",
      "epcho= 854 loss=  tensor(0.0010)\n",
      "epcho= 855 loss=  tensor(0.0010)\n",
      "epcho= 856 loss=  tensor(0.0010)\n",
      "epcho= 857 loss=  tensor(0.0010)\n",
      "epcho= 858 loss=  tensor(0.0010)\n",
      "epcho= 859 loss=  tensor(0.0010)\n",
      "epcho= 860 loss=  tensor(0.0010)\n",
      "epcho= 861 loss=  tensor(0.0010)\n",
      "epcho= 862 loss=  tensor(0.0010)\n",
      "epcho= 863 loss=  tensor(0.0010)\n",
      "epcho= 864 loss=  tensor(0.0010)\n",
      "epcho= 865 loss=  tensor(0.0010)\n",
      "epcho= 866 loss=  tensor(0.0010)\n",
      "epcho= 867 loss=  tensor(0.0010)\n",
      "epcho= 868 loss=  tensor(0.0010)\n",
      "epcho= 869 loss=  tensor(0.0010)\n",
      "epcho= 870 loss=  tensor(0.0010)\n",
      "epcho= 871 loss=  tensor(0.0010)\n",
      "epcho= 872 loss=  tensor(0.0010)\n",
      "epcho= 873 loss=  tensor(0.0010)\n",
      "epcho= 874 loss=  tensor(0.0010)\n",
      "epcho= 875 loss=  tensor(0.0010)\n",
      "epcho= 876 loss=  tensor(0.0010)\n",
      "epcho= 877 loss=  tensor(0.0010)\n",
      "epcho= 878 loss=  tensor(0.0010)\n",
      "epcho= 879 loss=  tensor(0.0009)\n",
      "epcho= 880 loss=  tensor(0.0009)\n",
      "epcho= 881 loss=  tensor(0.0009)\n",
      "epcho= 882 loss=  tensor(0.0009)\n",
      "epcho= 883 loss=  tensor(0.0009)\n",
      "epcho= 884 loss=  tensor(0.0009)\n",
      "epcho= 885 loss=  tensor(0.0009)\n",
      "epcho= 886 loss=  tensor(0.0009)\n",
      "epcho= 887 loss=  tensor(0.0009)\n",
      "epcho= 888 loss=  tensor(0.0009)\n",
      "epcho= 889 loss=  tensor(0.0009)\n",
      "epcho= 890 loss=  tensor(0.0009)\n",
      "epcho= 891 loss=  tensor(0.0009)\n",
      "epcho= 892 loss=  tensor(0.0009)\n",
      "epcho= 893 loss=  tensor(0.0009)\n",
      "epcho= 894 loss=  tensor(0.0009)\n",
      "epcho= 895 loss=  tensor(0.0009)\n",
      "epcho= 896 loss=  tensor(0.0009)\n",
      "epcho= 897 loss=  tensor(0.0009)\n",
      "epcho= 898 loss=  tensor(0.0009)\n",
      "epcho= 899 loss=  tensor(0.0009)\n",
      "epcho= 900 loss=  tensor(0.0009)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>) tensor(0.2088, grad_fn=<DivBackward0>)\n",
      "epcho= 901 loss=  tensor(0.0009)\n",
      "epcho= 902 loss=  tensor(0.0009)\n",
      "epcho= 903 loss=  tensor(0.0009)\n",
      "epcho= 904 loss=  tensor(0.0009)\n",
      "epcho= 905 loss=  tensor(0.0009)\n",
      "epcho= 906 loss=  tensor(0.0009)\n",
      "epcho= 907 loss=  tensor(0.0009)\n",
      "epcho= 908 loss=  tensor(0.0009)\n",
      "epcho= 909 loss=  tensor(0.0009)\n",
      "epcho= 910 loss=  tensor(0.0009)\n",
      "epcho= 911 loss=  tensor(0.0009)\n",
      "epcho= 912 loss=  tensor(0.0009)\n",
      "epcho= 913 loss=  tensor(0.0009)\n",
      "epcho= 914 loss=  tensor(0.0009)\n",
      "epcho= 915 loss=  tensor(0.0009)\n",
      "epcho= 916 loss=  tensor(0.0009)\n",
      "epcho= 917 loss=  tensor(0.0009)\n",
      "epcho= 918 loss=  tensor(0.0009)\n",
      "epcho= 919 loss=  tensor(0.0009)\n",
      "epcho= 920 loss=  tensor(0.0009)\n",
      "epcho= 921 loss=  tensor(0.0009)\n",
      "epcho= 922 loss=  tensor(0.0009)\n",
      "epcho= 923 loss=  tensor(0.0009)\n",
      "epcho= 924 loss=  tensor(0.0009)\n",
      "epcho= 925 loss=  tensor(0.0009)\n",
      "epcho= 926 loss=  tensor(0.0009)\n",
      "epcho= 927 loss=  tensor(0.0009)\n",
      "epcho= 928 loss=  tensor(0.0009)\n",
      "epcho= 929 loss=  tensor(0.0009)\n",
      "epcho= 930 loss=  tensor(0.0009)\n",
      "epcho= 931 loss=  tensor(0.0009)\n",
      "epcho= 932 loss=  tensor(0.0009)\n",
      "epcho= 933 loss=  tensor(0.0009)\n",
      "epcho= 934 loss=  tensor(0.0009)\n",
      "epcho= 935 loss=  tensor(0.0009)\n",
      "epcho= 936 loss=  tensor(0.0009)\n",
      "epcho= 937 loss=  tensor(0.0009)\n",
      "epcho= 938 loss=  tensor(0.0009)\n",
      "epcho= 939 loss=  tensor(0.0009)\n",
      "epcho= 940 loss=  tensor(0.0009)\n",
      "epcho= 941 loss=  tensor(0.0009)\n",
      "epcho= 942 loss=  tensor(0.0009)\n",
      "epcho= 943 loss=  tensor(0.0009)\n",
      "epcho= 944 loss=  tensor(0.0009)\n",
      "epcho= 945 loss=  tensor(0.0009)\n",
      "epcho= 946 loss=  tensor(0.0009)\n",
      "epcho= 947 loss=  tensor(0.0009)\n",
      "epcho= 948 loss=  tensor(0.0009)\n",
      "epcho= 949 loss=  tensor(0.0009)\n",
      "epcho= 950 loss=  tensor(0.0009)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>) tensor(0.2069, grad_fn=<DivBackward0>)\n",
      "epcho= 951 loss=  tensor(0.0009)\n",
      "epcho= 952 loss=  tensor(0.0009)\n",
      "epcho= 953 loss=  tensor(0.0009)\n",
      "epcho= 954 loss=  tensor(0.0009)\n",
      "epcho= 955 loss=  tensor(0.0009)\n",
      "epcho= 956 loss=  tensor(0.0009)\n",
      "epcho= 957 loss=  tensor(0.0009)\n",
      "epcho= 958 loss=  tensor(0.0009)\n",
      "epcho= 959 loss=  tensor(0.0009)\n",
      "epcho= 960 loss=  tensor(0.0009)\n",
      "epcho= 961 loss=  tensor(0.0009)\n",
      "epcho= 962 loss=  tensor(0.0009)\n",
      "epcho= 963 loss=  tensor(0.0009)\n",
      "epcho= 964 loss=  tensor(0.0009)\n",
      "epcho= 965 loss=  tensor(0.0009)\n",
      "epcho= 966 loss=  tensor(0.0009)\n",
      "epcho= 967 loss=  tensor(0.0009)\n",
      "epcho= 968 loss=  tensor(0.0009)\n",
      "epcho= 969 loss=  tensor(0.0009)\n",
      "epcho= 970 loss=  tensor(0.0009)\n",
      "epcho= 971 loss=  tensor(0.0009)\n",
      "epcho= 972 loss=  tensor(0.0009)\n",
      "epcho= 973 loss=  tensor(0.0009)\n",
      "epcho= 974 loss=  tensor(0.0009)\n",
      "epcho= 975 loss=  tensor(0.0009)\n",
      "epcho= 976 loss=  tensor(0.0009)\n",
      "epcho= 977 loss=  tensor(0.0009)\n",
      "epcho= 978 loss=  tensor(0.0009)\n",
      "epcho= 979 loss=  tensor(0.0009)\n",
      "epcho= 980 loss=  tensor(0.0009)\n",
      "epcho= 981 loss=  tensor(0.0009)\n",
      "epcho= 982 loss=  tensor(0.0009)\n",
      "epcho= 983 loss=  tensor(0.0009)\n",
      "epcho= 984 loss=  tensor(0.0009)\n",
      "epcho= 985 loss=  tensor(0.0009)\n",
      "epcho= 986 loss=  tensor(0.0009)\n",
      "epcho= 987 loss=  tensor(0.0009)\n",
      "epcho= 988 loss=  tensor(0.0009)\n",
      "epcho= 989 loss=  tensor(0.0009)\n",
      "epcho= 990 loss=  tensor(0.0009)\n",
      "epcho= 991 loss=  tensor(0.0009)\n",
      "epcho= 992 loss=  tensor(0.0009)\n",
      "epcho= 993 loss=  tensor(0.0009)\n",
      "epcho= 994 loss=  tensor(0.0009)\n",
      "epcho= 995 loss=  tensor(0.0009)\n",
      "epcho= 996 loss=  tensor(0.0009)\n",
      "epcho= 997 loss=  tensor(0.0009)\n",
      "epcho= 998 loss=  tensor(0.0009)\n",
      "epcho= 999 loss=  tensor(0.0009)\n",
      "epcho= 1000 loss=  tensor(0.0009)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>) tensor(0.2053, grad_fn=<DivBackward0>)\n",
      "epcho= 1001 loss=  tensor(0.0009)\n",
      "epcho= 1002 loss=  tensor(0.0009)\n",
      "epcho= 1003 loss=  tensor(0.0009)\n",
      "epcho= 1004 loss=  tensor(0.0009)\n",
      "epcho= 1005 loss=  tensor(0.0009)\n",
      "epcho= 1006 loss=  tensor(0.0009)\n",
      "epcho= 1007 loss=  tensor(0.0009)\n",
      "epcho= 1008 loss=  tensor(0.0009)\n",
      "epcho= 1009 loss=  tensor(0.0009)\n",
      "epcho= 1010 loss=  tensor(0.0009)\n",
      "epcho= 1011 loss=  tensor(0.0009)\n",
      "epcho= 1012 loss=  tensor(0.0009)\n",
      "epcho= 1013 loss=  tensor(0.0008)\n",
      "epcho= 1014 loss=  tensor(0.0008)\n",
      "epcho= 1015 loss=  tensor(0.0008)\n",
      "epcho= 1016 loss=  tensor(0.0008)\n",
      "epcho= 1017 loss=  tensor(0.0008)\n",
      "epcho= 1018 loss=  tensor(0.0008)\n",
      "epcho= 1019 loss=  tensor(0.0008)\n",
      "epcho= 1020 loss=  tensor(0.0008)\n",
      "epcho= 1021 loss=  tensor(0.0008)\n",
      "epcho= 1022 loss=  tensor(0.0008)\n",
      "epcho= 1023 loss=  tensor(0.0008)\n",
      "epcho= 1024 loss=  tensor(0.0008)\n",
      "epcho= 1025 loss=  tensor(0.0008)\n",
      "epcho= 1026 loss=  tensor(0.0008)\n",
      "epcho= 1027 loss=  tensor(0.0008)\n",
      "epcho= 1028 loss=  tensor(0.0008)\n",
      "epcho= 1029 loss=  tensor(0.0008)\n",
      "epcho= 1030 loss=  tensor(0.0008)\n",
      "epcho= 1031 loss=  tensor(0.0008)\n",
      "epcho= 1032 loss=  tensor(0.0008)\n",
      "epcho= 1033 loss=  tensor(0.0008)\n",
      "epcho= 1034 loss=  tensor(0.0008)\n",
      "epcho= 1035 loss=  tensor(0.0008)\n",
      "epcho= 1036 loss=  tensor(0.0008)\n",
      "epcho= 1037 loss=  tensor(0.0008)\n",
      "epcho= 1038 loss=  tensor(0.0008)\n",
      "epcho= 1039 loss=  tensor(0.0008)\n",
      "epcho= 1040 loss=  tensor(0.0008)\n",
      "epcho= 1041 loss=  tensor(0.0008)\n",
      "epcho= 1042 loss=  tensor(0.0008)\n",
      "epcho= 1043 loss=  tensor(0.0008)\n",
      "epcho= 1044 loss=  tensor(0.0008)\n",
      "epcho= 1045 loss=  tensor(0.0008)\n",
      "epcho= 1046 loss=  tensor(0.0008)\n",
      "epcho= 1047 loss=  tensor(0.0008)\n",
      "epcho= 1048 loss=  tensor(0.0008)\n",
      "epcho= 1049 loss=  tensor(0.0008)\n",
      "epcho= 1050 loss=  tensor(0.0008)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>) tensor(0.2038, grad_fn=<DivBackward0>)\n",
      "epcho= 1051 loss=  tensor(0.0008)\n",
      "epcho= 1052 loss=  tensor(0.0008)\n",
      "epcho= 1053 loss=  tensor(0.0008)\n",
      "epcho= 1054 loss=  tensor(0.0008)\n",
      "epcho= 1055 loss=  tensor(0.0008)\n",
      "epcho= 1056 loss=  tensor(0.0008)\n",
      "epcho= 1057 loss=  tensor(0.0008)\n",
      "epcho= 1058 loss=  tensor(0.0008)\n",
      "epcho= 1059 loss=  tensor(0.0008)\n",
      "epcho= 1060 loss=  tensor(0.0008)\n",
      "epcho= 1061 loss=  tensor(0.0008)\n",
      "epcho= 1062 loss=  tensor(0.0008)\n",
      "epcho= 1063 loss=  tensor(0.0008)\n",
      "epcho= 1064 loss=  tensor(0.0008)\n",
      "epcho= 1065 loss=  tensor(0.0008)\n",
      "epcho= 1066 loss=  tensor(0.0008)\n",
      "epcho= 1067 loss=  tensor(0.0008)\n",
      "epcho= 1068 loss=  tensor(0.0008)\n",
      "epcho= 1069 loss=  tensor(0.0008)\n",
      "epcho= 1070 loss=  tensor(0.0008)\n",
      "epcho= 1071 loss=  tensor(0.0008)\n",
      "epcho= 1072 loss=  tensor(0.0008)\n",
      "epcho= 1073 loss=  tensor(0.0008)\n",
      "epcho= 1074 loss=  tensor(0.0008)\n",
      "epcho= 1075 loss=  tensor(0.0008)\n",
      "epcho= 1076 loss=  tensor(0.0008)\n",
      "epcho= 1077 loss=  tensor(0.0008)\n",
      "epcho= 1078 loss=  tensor(0.0008)\n",
      "epcho= 1079 loss=  tensor(0.0008)\n",
      "epcho= 1080 loss=  tensor(0.0008)\n",
      "epcho= 1081 loss=  tensor(0.0008)\n",
      "epcho= 1082 loss=  tensor(0.0008)\n",
      "epcho= 1083 loss=  tensor(0.0008)\n",
      "epcho= 1084 loss=  tensor(0.0008)\n",
      "epcho= 1085 loss=  tensor(0.0008)\n",
      "epcho= 1086 loss=  tensor(0.0008)\n",
      "epcho= 1087 loss=  tensor(0.0008)\n",
      "epcho= 1088 loss=  tensor(0.0008)\n",
      "epcho= 1089 loss=  tensor(0.0008)\n",
      "epcho= 1090 loss=  tensor(0.0008)\n",
      "epcho= 1091 loss=  tensor(0.0008)\n",
      "epcho= 1092 loss=  tensor(0.0008)\n",
      "epcho= 1093 loss=  tensor(0.0008)\n",
      "epcho= 1094 loss=  tensor(0.0008)\n",
      "epcho= 1095 loss=  tensor(0.0008)\n",
      "epcho= 1096 loss=  tensor(0.0008)\n",
      "epcho= 1097 loss=  tensor(0.0008)\n",
      "epcho= 1098 loss=  tensor(0.0008)\n",
      "epcho= 1099 loss=  tensor(0.0008)\n",
      "epcho= 1100 loss=  tensor(0.0008)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>) tensor(0.2037, grad_fn=<DivBackward0>)\n",
      "epcho= 1101 loss=  tensor(0.0008)\n",
      "epcho= 1102 loss=  tensor(0.0008)\n",
      "epcho= 1103 loss=  tensor(0.0008)\n",
      "epcho= 1104 loss=  tensor(0.0008)\n",
      "epcho= 1105 loss=  tensor(0.0008)\n",
      "epcho= 1106 loss=  tensor(0.0008)\n",
      "epcho= 1107 loss=  tensor(0.0008)\n",
      "epcho= 1108 loss=  tensor(0.0009)\n",
      "epcho= 1109 loss=  tensor(0.0009)\n",
      "epcho= 1110 loss=  tensor(0.0010)\n",
      "epcho= 1111 loss=  tensor(0.0011)\n",
      "epcho= 1112 loss=  tensor(0.0013)\n",
      "epcho= 1113 loss=  tensor(0.0016)\n",
      "epcho= 1114 loss=  tensor(0.0020)\n",
      "epcho= 1115 loss=  tensor(0.0027)\n",
      "epcho= 1116 loss=  tensor(0.0038)\n",
      "epcho= 1117 loss=  tensor(0.0053)\n",
      "epcho= 1118 loss=  tensor(0.0072)\n",
      "epcho= 1119 loss=  tensor(0.0090)\n",
      "epcho= 1120 loss=  tensor(0.0103)\n",
      "epcho= 1121 loss=  tensor(0.0097)\n",
      "epcho= 1122 loss=  tensor(0.0074)\n",
      "epcho= 1123 loss=  tensor(0.0040)\n",
      "epcho= 1124 loss=  tensor(0.0015)\n",
      "epcho= 1125 loss=  tensor(0.0011)\n",
      "epcho= 1126 loss=  tensor(0.0023)\n",
      "epcho= 1127 loss=  tensor(0.0038)\n",
      "epcho= 1128 loss=  tensor(0.0040)\n",
      "epcho= 1129 loss=  tensor(0.0029)\n",
      "epcho= 1130 loss=  tensor(0.0014)\n",
      "epcho= 1131 loss=  tensor(0.0009)\n",
      "epcho= 1132 loss=  tensor(0.0016)\n",
      "epcho= 1133 loss=  tensor(0.0023)\n",
      "epcho= 1134 loss=  tensor(0.0024)\n",
      "epcho= 1135 loss=  tensor(0.0016)\n",
      "epcho= 1136 loss=  tensor(0.0009)\n",
      "epcho= 1137 loss=  tensor(0.0010)\n",
      "epcho= 1138 loss=  tensor(0.0014)\n",
      "epcho= 1139 loss=  tensor(0.0017)\n",
      "epcho= 1140 loss=  tensor(0.0014)\n",
      "epcho= 1141 loss=  tensor(0.0010)\n",
      "epcho= 1142 loss=  tensor(0.0008)\n",
      "epcho= 1143 loss=  tensor(0.0011)\n",
      "epcho= 1144 loss=  tensor(0.0013)\n",
      "epcho= 1145 loss=  tensor(0.0012)\n",
      "epcho= 1146 loss=  tensor(0.0010)\n",
      "epcho= 1147 loss=  tensor(0.0008)\n",
      "epcho= 1148 loss=  tensor(0.0009)\n",
      "epcho= 1149 loss=  tensor(0.0011)\n",
      "epcho= 1150 loss=  tensor(0.0011)\n",
      "tensor(0.0011, grad_fn=<AddBackward0>) tensor(0.1987, grad_fn=<DivBackward0>)\n",
      "epcho= 1151 loss=  tensor(0.0009)\n",
      "epcho= 1152 loss=  tensor(0.0008)\n",
      "epcho= 1153 loss=  tensor(0.0008)\n",
      "epcho= 1154 loss=  tensor(0.0010)\n",
      "epcho= 1155 loss=  tensor(0.0010)\n",
      "epcho= 1156 loss=  tensor(0.0008)\n",
      "epcho= 1157 loss=  tensor(0.0008)\n",
      "epcho= 1158 loss=  tensor(0.0008)\n",
      "epcho= 1159 loss=  tensor(0.0009)\n",
      "epcho= 1160 loss=  tensor(0.0009)\n",
      "epcho= 1161 loss=  tensor(0.0008)\n",
      "epcho= 1162 loss=  tensor(0.0008)\n",
      "epcho= 1163 loss=  tensor(0.0008)\n",
      "epcho= 1164 loss=  tensor(0.0008)\n",
      "epcho= 1165 loss=  tensor(0.0008)\n",
      "epcho= 1166 loss=  tensor(0.0008)\n",
      "epcho= 1167 loss=  tensor(0.0008)\n",
      "epcho= 1168 loss=  tensor(0.0008)\n",
      "epcho= 1169 loss=  tensor(0.0008)\n",
      "epcho= 1170 loss=  tensor(0.0008)\n",
      "epcho= 1171 loss=  tensor(0.0008)\n",
      "epcho= 1172 loss=  tensor(0.0008)\n",
      "epcho= 1173 loss=  tensor(0.0008)\n",
      "epcho= 1174 loss=  tensor(0.0008)\n",
      "epcho= 1175 loss=  tensor(0.0008)\n",
      "epcho= 1176 loss=  tensor(0.0008)\n",
      "epcho= 1177 loss=  tensor(0.0008)\n",
      "epcho= 1178 loss=  tensor(0.0008)\n",
      "epcho= 1179 loss=  tensor(0.0008)\n",
      "epcho= 1180 loss=  tensor(0.0008)\n",
      "epcho= 1181 loss=  tensor(0.0008)\n",
      "epcho= 1182 loss=  tensor(0.0008)\n",
      "epcho= 1183 loss=  tensor(0.0008)\n",
      "epcho= 1184 loss=  tensor(0.0008)\n",
      "epcho= 1185 loss=  tensor(0.0008)\n",
      "epcho= 1186 loss=  tensor(0.0008)\n",
      "epcho= 1187 loss=  tensor(0.0008)\n",
      "epcho= 1188 loss=  tensor(0.0008)\n",
      "epcho= 1189 loss=  tensor(0.0008)\n",
      "epcho= 1190 loss=  tensor(0.0008)\n",
      "epcho= 1191 loss=  tensor(0.0008)\n",
      "epcho= 1192 loss=  tensor(0.0008)\n",
      "epcho= 1193 loss=  tensor(0.0008)\n",
      "epcho= 1194 loss=  tensor(0.0008)\n",
      "epcho= 1195 loss=  tensor(0.0008)\n",
      "epcho= 1196 loss=  tensor(0.0008)\n",
      "epcho= 1197 loss=  tensor(0.0008)\n",
      "epcho= 1198 loss=  tensor(0.0008)\n",
      "epcho= 1199 loss=  tensor(0.0008)\n",
      "epcho= 1200 loss=  tensor(0.0008)\n",
      "tensor(0.0008, grad_fn=<AddBackward0>) tensor(0.2022, grad_fn=<DivBackward0>)\n",
      "epcho= 1201 loss=  tensor(0.0008)\n",
      "epcho= 1202 loss=  tensor(0.0008)\n",
      "epcho= 1203 loss=  tensor(0.0008)\n",
      "epcho= 1204 loss=  tensor(0.0008)\n",
      "epcho= 1205 loss=  tensor(0.0008)\n",
      "epcho= 1206 loss=  tensor(0.0008)\n",
      "epcho= 1207 loss=  tensor(0.0008)\n",
      "epcho= 1208 loss=  tensor(0.0008)\n",
      "epcho= 1209 loss=  tensor(0.0008)\n",
      "epcho= 1210 loss=  tensor(0.0008)\n",
      "epcho= 1211 loss=  tensor(0.0008)\n",
      "epcho= 1212 loss=  tensor(0.0008)\n",
      "epcho= 1213 loss=  tensor(0.0008)\n",
      "epcho= 1214 loss=  tensor(0.0008)\n",
      "epcho= 1215 loss=  tensor(0.0008)\n",
      "epcho= 1216 loss=  tensor(0.0008)\n",
      "epcho= 1217 loss=  tensor(0.0008)\n",
      "epcho= 1218 loss=  tensor(0.0008)\n",
      "epcho= 1219 loss=  tensor(0.0008)\n",
      "epcho= 1220 loss=  tensor(0.0008)\n",
      "epcho= 1221 loss=  tensor(0.0008)\n",
      "epcho= 1222 loss=  tensor(0.0008)\n",
      "epcho= 1223 loss=  tensor(0.0007)\n",
      "epcho= 1224 loss=  tensor(0.0007)\n",
      "epcho= 1225 loss=  tensor(0.0007)\n",
      "epcho= 1226 loss=  tensor(0.0007)\n",
      "epcho= 1227 loss=  tensor(0.0007)\n",
      "epcho= 1228 loss=  tensor(0.0007)\n",
      "epcho= 1229 loss=  tensor(0.0007)\n",
      "epcho= 1230 loss=  tensor(0.0007)\n",
      "epcho= 1231 loss=  tensor(0.0007)\n",
      "epcho= 1232 loss=  tensor(0.0007)\n",
      "epcho= 1233 loss=  tensor(0.0007)\n",
      "epcho= 1234 loss=  tensor(0.0007)\n",
      "epcho= 1235 loss=  tensor(0.0007)\n",
      "epcho= 1236 loss=  tensor(0.0007)\n",
      "epcho= 1237 loss=  tensor(0.0007)\n",
      "epcho= 1238 loss=  tensor(0.0007)\n",
      "epcho= 1239 loss=  tensor(0.0007)\n",
      "epcho= 1240 loss=  tensor(0.0007)\n",
      "epcho= 1241 loss=  tensor(0.0007)\n",
      "epcho= 1242 loss=  tensor(0.0007)\n",
      "epcho= 1243 loss=  tensor(0.0007)\n",
      "epcho= 1244 loss=  tensor(0.0007)\n",
      "epcho= 1245 loss=  tensor(0.0007)\n",
      "epcho= 1246 loss=  tensor(0.0007)\n",
      "epcho= 1247 loss=  tensor(0.0007)\n",
      "epcho= 1248 loss=  tensor(0.0007)\n",
      "epcho= 1249 loss=  tensor(0.0007)\n",
      "epcho= 1250 loss=  tensor(0.0007)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>) tensor(0.2013, grad_fn=<DivBackward0>)\n",
      "epcho= 1251 loss=  tensor(0.0007)\n",
      "epcho= 1252 loss=  tensor(0.0007)\n",
      "epcho= 1253 loss=  tensor(0.0007)\n",
      "epcho= 1254 loss=  tensor(0.0007)\n",
      "epcho= 1255 loss=  tensor(0.0007)\n",
      "epcho= 1256 loss=  tensor(0.0007)\n",
      "epcho= 1257 loss=  tensor(0.0007)\n",
      "epcho= 1258 loss=  tensor(0.0007)\n",
      "epcho= 1259 loss=  tensor(0.0007)\n",
      "epcho= 1260 loss=  tensor(0.0007)\n",
      "epcho= 1261 loss=  tensor(0.0007)\n",
      "epcho= 1262 loss=  tensor(0.0007)\n",
      "epcho= 1263 loss=  tensor(0.0007)\n",
      "epcho= 1264 loss=  tensor(0.0007)\n",
      "epcho= 1265 loss=  tensor(0.0007)\n",
      "epcho= 1266 loss=  tensor(0.0007)\n",
      "epcho= 1267 loss=  tensor(0.0007)\n",
      "epcho= 1268 loss=  tensor(0.0007)\n",
      "epcho= 1269 loss=  tensor(0.0007)\n",
      "epcho= 1270 loss=  tensor(0.0007)\n",
      "epcho= 1271 loss=  tensor(0.0007)\n",
      "epcho= 1272 loss=  tensor(0.0007)\n",
      "epcho= 1273 loss=  tensor(0.0007)\n",
      "epcho= 1274 loss=  tensor(0.0007)\n",
      "epcho= 1275 loss=  tensor(0.0007)\n",
      "epcho= 1276 loss=  tensor(0.0007)\n",
      "epcho= 1277 loss=  tensor(0.0007)\n",
      "epcho= 1278 loss=  tensor(0.0007)\n",
      "epcho= 1279 loss=  tensor(0.0007)\n",
      "epcho= 1280 loss=  tensor(0.0007)\n",
      "epcho= 1281 loss=  tensor(0.0007)\n",
      "epcho= 1282 loss=  tensor(0.0007)\n",
      "epcho= 1283 loss=  tensor(0.0007)\n",
      "epcho= 1284 loss=  tensor(0.0007)\n",
      "epcho= 1285 loss=  tensor(0.0007)\n",
      "epcho= 1286 loss=  tensor(0.0007)\n",
      "epcho= 1287 loss=  tensor(0.0007)\n",
      "epcho= 1288 loss=  tensor(0.0007)\n",
      "epcho= 1289 loss=  tensor(0.0007)\n",
      "epcho= 1290 loss=  tensor(0.0007)\n",
      "epcho= 1291 loss=  tensor(0.0007)\n",
      "epcho= 1292 loss=  tensor(0.0007)\n",
      "epcho= 1293 loss=  tensor(0.0007)\n",
      "epcho= 1294 loss=  tensor(0.0007)\n",
      "epcho= 1295 loss=  tensor(0.0007)\n",
      "epcho= 1296 loss=  tensor(0.0007)\n",
      "epcho= 1297 loss=  tensor(0.0007)\n",
      "epcho= 1298 loss=  tensor(0.0007)\n",
      "epcho= 1299 loss=  tensor(0.0007)\n",
      "epcho= 1300 loss=  tensor(0.0007)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>) tensor(0.2003, grad_fn=<DivBackward0>)\n",
      "epcho= 1301 loss=  tensor(0.0007)\n",
      "epcho= 1302 loss=  tensor(0.0007)\n",
      "epcho= 1303 loss=  tensor(0.0007)\n",
      "epcho= 1304 loss=  tensor(0.0007)\n",
      "epcho= 1305 loss=  tensor(0.0007)\n",
      "epcho= 1306 loss=  tensor(0.0007)\n",
      "epcho= 1307 loss=  tensor(0.0007)\n",
      "epcho= 1308 loss=  tensor(0.0007)\n",
      "epcho= 1309 loss=  tensor(0.0007)\n",
      "epcho= 1310 loss=  tensor(0.0007)\n",
      "epcho= 1311 loss=  tensor(0.0007)\n",
      "epcho= 1312 loss=  tensor(0.0007)\n",
      "epcho= 1313 loss=  tensor(0.0007)\n",
      "epcho= 1314 loss=  tensor(0.0007)\n",
      "epcho= 1315 loss=  tensor(0.0007)\n",
      "epcho= 1316 loss=  tensor(0.0007)\n",
      "epcho= 1317 loss=  tensor(0.0007)\n",
      "epcho= 1318 loss=  tensor(0.0007)\n",
      "epcho= 1319 loss=  tensor(0.0007)\n",
      "epcho= 1320 loss=  tensor(0.0007)\n",
      "epcho= 1321 loss=  tensor(0.0007)\n",
      "epcho= 1322 loss=  tensor(0.0007)\n",
      "epcho= 1323 loss=  tensor(0.0007)\n",
      "epcho= 1324 loss=  tensor(0.0007)\n",
      "epcho= 1325 loss=  tensor(0.0007)\n",
      "epcho= 1326 loss=  tensor(0.0007)\n",
      "epcho= 1327 loss=  tensor(0.0007)\n",
      "epcho= 1328 loss=  tensor(0.0007)\n",
      "epcho= 1329 loss=  tensor(0.0007)\n",
      "epcho= 1330 loss=  tensor(0.0007)\n",
      "epcho= 1331 loss=  tensor(0.0007)\n",
      "epcho= 1332 loss=  tensor(0.0007)\n",
      "epcho= 1333 loss=  tensor(0.0007)\n",
      "epcho= 1334 loss=  tensor(0.0007)\n",
      "epcho= 1335 loss=  tensor(0.0007)\n",
      "epcho= 1336 loss=  tensor(0.0007)\n",
      "epcho= 1337 loss=  tensor(0.0007)\n",
      "epcho= 1338 loss=  tensor(0.0007)\n",
      "epcho= 1339 loss=  tensor(0.0007)\n",
      "epcho= 1340 loss=  tensor(0.0007)\n",
      "epcho= 1341 loss=  tensor(0.0007)\n",
      "epcho= 1342 loss=  tensor(0.0007)\n",
      "epcho= 1343 loss=  tensor(0.0007)\n",
      "epcho= 1344 loss=  tensor(0.0007)\n",
      "epcho= 1345 loss=  tensor(0.0007)\n",
      "epcho= 1346 loss=  tensor(0.0007)\n",
      "epcho= 1347 loss=  tensor(0.0007)\n",
      "epcho= 1348 loss=  tensor(0.0007)\n",
      "epcho= 1349 loss=  tensor(0.0007)\n",
      "epcho= 1350 loss=  tensor(0.0007)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>) tensor(0.1993, grad_fn=<DivBackward0>)\n",
      "epcho= 1351 loss=  tensor(0.0007)\n",
      "epcho= 1352 loss=  tensor(0.0007)\n",
      "epcho= 1353 loss=  tensor(0.0007)\n",
      "epcho= 1354 loss=  tensor(0.0007)\n",
      "epcho= 1355 loss=  tensor(0.0007)\n",
      "epcho= 1356 loss=  tensor(0.0007)\n",
      "epcho= 1357 loss=  tensor(0.0007)\n",
      "epcho= 1358 loss=  tensor(0.0007)\n",
      "epcho= 1359 loss=  tensor(0.0007)\n",
      "epcho= 1360 loss=  tensor(0.0007)\n",
      "epcho= 1361 loss=  tensor(0.0007)\n",
      "epcho= 1362 loss=  tensor(0.0007)\n",
      "epcho= 1363 loss=  tensor(0.0007)\n",
      "epcho= 1364 loss=  tensor(0.0007)\n",
      "epcho= 1365 loss=  tensor(0.0007)\n",
      "epcho= 1366 loss=  tensor(0.0007)\n",
      "epcho= 1367 loss=  tensor(0.0007)\n",
      "epcho= 1368 loss=  tensor(0.0007)\n",
      "epcho= 1369 loss=  tensor(0.0007)\n",
      "epcho= 1370 loss=  tensor(0.0007)\n",
      "epcho= 1371 loss=  tensor(0.0007)\n",
      "epcho= 1372 loss=  tensor(0.0007)\n",
      "epcho= 1373 loss=  tensor(0.0007)\n",
      "epcho= 1374 loss=  tensor(0.0007)\n",
      "epcho= 1375 loss=  tensor(0.0007)\n",
      "epcho= 1376 loss=  tensor(0.0007)\n",
      "epcho= 1377 loss=  tensor(0.0007)\n",
      "epcho= 1378 loss=  tensor(0.0007)\n",
      "epcho= 1379 loss=  tensor(0.0007)\n",
      "epcho= 1380 loss=  tensor(0.0007)\n",
      "epcho= 1381 loss=  tensor(0.0007)\n",
      "epcho= 1382 loss=  tensor(0.0007)\n",
      "epcho= 1383 loss=  tensor(0.0007)\n",
      "epcho= 1384 loss=  tensor(0.0007)\n",
      "epcho= 1385 loss=  tensor(0.0007)\n",
      "epcho= 1386 loss=  tensor(0.0007)\n",
      "epcho= 1387 loss=  tensor(0.0007)\n",
      "epcho= 1388 loss=  tensor(0.0007)\n",
      "epcho= 1389 loss=  tensor(0.0007)\n",
      "epcho= 1390 loss=  tensor(0.0007)\n",
      "epcho= 1391 loss=  tensor(0.0007)\n",
      "epcho= 1392 loss=  tensor(0.0007)\n",
      "epcho= 1393 loss=  tensor(0.0007)\n",
      "epcho= 1394 loss=  tensor(0.0007)\n",
      "epcho= 1395 loss=  tensor(0.0007)\n",
      "epcho= 1396 loss=  tensor(0.0007)\n",
      "epcho= 1397 loss=  tensor(0.0007)\n",
      "epcho= 1398 loss=  tensor(0.0007)\n",
      "epcho= 1399 loss=  tensor(0.0007)\n",
      "epcho= 1400 loss=  tensor(0.0007)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>) tensor(0.1985, grad_fn=<DivBackward0>)\n",
      "epcho= 1401 loss=  tensor(0.0007)\n",
      "epcho= 1402 loss=  tensor(0.0007)\n",
      "epcho= 1403 loss=  tensor(0.0007)\n",
      "epcho= 1404 loss=  tensor(0.0007)\n",
      "epcho= 1405 loss=  tensor(0.0007)\n",
      "epcho= 1406 loss=  tensor(0.0007)\n",
      "epcho= 1407 loss=  tensor(0.0007)\n",
      "epcho= 1408 loss=  tensor(0.0007)\n",
      "epcho= 1409 loss=  tensor(0.0007)\n",
      "epcho= 1410 loss=  tensor(0.0007)\n",
      "epcho= 1411 loss=  tensor(0.0007)\n",
      "epcho= 1412 loss=  tensor(0.0007)\n",
      "epcho= 1413 loss=  tensor(0.0007)\n",
      "epcho= 1414 loss=  tensor(0.0007)\n",
      "epcho= 1415 loss=  tensor(0.0007)\n",
      "epcho= 1416 loss=  tensor(0.0007)\n",
      "epcho= 1417 loss=  tensor(0.0007)\n",
      "epcho= 1418 loss=  tensor(0.0007)\n",
      "epcho= 1419 loss=  tensor(0.0007)\n",
      "epcho= 1420 loss=  tensor(0.0007)\n",
      "epcho= 1421 loss=  tensor(0.0007)\n",
      "epcho= 1422 loss=  tensor(0.0007)\n",
      "epcho= 1423 loss=  tensor(0.0007)\n",
      "epcho= 1424 loss=  tensor(0.0007)\n",
      "epcho= 1425 loss=  tensor(0.0007)\n",
      "epcho= 1426 loss=  tensor(0.0007)\n",
      "epcho= 1427 loss=  tensor(0.0007)\n",
      "epcho= 1428 loss=  tensor(0.0007)\n",
      "epcho= 1429 loss=  tensor(0.0007)\n",
      "epcho= 1430 loss=  tensor(0.0007)\n",
      "epcho= 1431 loss=  tensor(0.0007)\n",
      "epcho= 1432 loss=  tensor(0.0007)\n",
      "epcho= 1433 loss=  tensor(0.0007)\n",
      "epcho= 1434 loss=  tensor(0.0007)\n",
      "epcho= 1435 loss=  tensor(0.0007)\n",
      "epcho= 1436 loss=  tensor(0.0008)\n",
      "epcho= 1437 loss=  tensor(0.0009)\n",
      "epcho= 1438 loss=  tensor(0.0011)\n",
      "epcho= 1439 loss=  tensor(0.0015)\n",
      "epcho= 1440 loss=  tensor(0.0022)\n",
      "epcho= 1441 loss=  tensor(0.0036)\n",
      "epcho= 1442 loss=  tensor(0.0060)\n",
      "epcho= 1443 loss=  tensor(0.0098)\n",
      "epcho= 1444 loss=  tensor(0.0155)\n",
      "epcho= 1445 loss=  tensor(0.0209)\n",
      "epcho= 1446 loss=  tensor(0.0237)\n",
      "epcho= 1447 loss=  tensor(0.0177)\n",
      "epcho= 1448 loss=  tensor(0.0075)\n",
      "epcho= 1449 loss=  tensor(0.0016)\n",
      "epcho= 1450 loss=  tensor(0.0044)\n",
      "tensor(0.0044, grad_fn=<AddBackward0>) tensor(0.1578, grad_fn=<DivBackward0>)\n",
      "epcho= 1451 loss=  tensor(0.0089)\n",
      "epcho= 1452 loss=  tensor(0.0063)\n",
      "epcho= 1453 loss=  tensor(0.0020)\n",
      "epcho= 1454 loss=  tensor(0.0028)\n",
      "epcho= 1455 loss=  tensor(0.0053)\n",
      "epcho= 1456 loss=  tensor(0.0037)\n",
      "epcho= 1457 loss=  tensor(0.0014)\n",
      "epcho= 1458 loss=  tensor(0.0028)\n",
      "epcho= 1459 loss=  tensor(0.0038)\n",
      "epcho= 1460 loss=  tensor(0.0017)\n",
      "epcho= 1461 loss=  tensor(0.0011)\n",
      "epcho= 1462 loss=  tensor(0.0028)\n",
      "epcho= 1463 loss=  tensor(0.0023)\n",
      "epcho= 1464 loss=  tensor(0.0008)\n",
      "epcho= 1465 loss=  tensor(0.0014)\n",
      "epcho= 1466 loss=  tensor(0.0023)\n",
      "epcho= 1467 loss=  tensor(0.0012)\n",
      "epcho= 1468 loss=  tensor(0.0007)\n",
      "epcho= 1469 loss=  tensor(0.0016)\n",
      "epcho= 1470 loss=  tensor(0.0015)\n",
      "epcho= 1471 loss=  tensor(0.0008)\n",
      "epcho= 1472 loss=  tensor(0.0011)\n",
      "epcho= 1473 loss=  tensor(0.0014)\n",
      "epcho= 1474 loss=  tensor(0.0009)\n",
      "epcho= 1475 loss=  tensor(0.0008)\n",
      "epcho= 1476 loss=  tensor(0.0012)\n",
      "epcho= 1477 loss=  tensor(0.0009)\n",
      "epcho= 1478 loss=  tensor(0.0007)\n",
      "epcho= 1479 loss=  tensor(0.0010)\n",
      "epcho= 1480 loss=  tensor(0.0010)\n",
      "epcho= 1481 loss=  tensor(0.0007)\n",
      "epcho= 1482 loss=  tensor(0.0008)\n",
      "epcho= 1483 loss=  tensor(0.0009)\n",
      "epcho= 1484 loss=  tensor(0.0008)\n",
      "epcho= 1485 loss=  tensor(0.0007)\n",
      "epcho= 1486 loss=  tensor(0.0008)\n",
      "epcho= 1487 loss=  tensor(0.0008)\n",
      "epcho= 1488 loss=  tensor(0.0007)\n",
      "epcho= 1489 loss=  tensor(0.0008)\n",
      "epcho= 1490 loss=  tensor(0.0008)\n",
      "epcho= 1491 loss=  tensor(0.0007)\n",
      "epcho= 1492 loss=  tensor(0.0007)\n",
      "epcho= 1493 loss=  tensor(0.0008)\n",
      "epcho= 1494 loss=  tensor(0.0007)\n",
      "epcho= 1495 loss=  tensor(0.0007)\n",
      "epcho= 1496 loss=  tensor(0.0007)\n",
      "epcho= 1497 loss=  tensor(0.0007)\n",
      "epcho= 1498 loss=  tensor(0.0007)\n",
      "epcho= 1499 loss=  tensor(0.0007)\n",
      "epcho= 1500 loss=  tensor(0.0007)\n",
      "tensor(0.0007, grad_fn=<AddBackward0>) tensor(0.2034, grad_fn=<DivBackward0>)\n",
      "epcho= 1501 loss=  tensor(0.0007)\n",
      "epcho= 1502 loss=  tensor(0.0007)\n",
      "epcho= 1503 loss=  tensor(0.0007)\n",
      "epcho= 1504 loss=  tensor(0.0007)\n",
      "epcho= 1505 loss=  tensor(0.0007)\n",
      "epcho= 1506 loss=  tensor(0.0007)\n",
      "epcho= 1507 loss=  tensor(0.0007)\n",
      "epcho= 1508 loss=  tensor(0.0007)\n",
      "epcho= 1509 loss=  tensor(0.0007)\n",
      "epcho= 1510 loss=  tensor(0.0007)\n",
      "epcho= 1511 loss=  tensor(0.0007)\n",
      "epcho= 1512 loss=  tensor(0.0007)\n",
      "epcho= 1513 loss=  tensor(0.0007)\n",
      "epcho= 1514 loss=  tensor(0.0007)\n",
      "epcho= 1515 loss=  tensor(0.0007)\n",
      "epcho= 1516 loss=  tensor(0.0007)\n",
      "epcho= 1517 loss=  tensor(0.0007)\n",
      "epcho= 1518 loss=  tensor(0.0007)\n",
      "epcho= 1519 loss=  tensor(0.0007)\n",
      "epcho= 1520 loss=  tensor(0.0007)\n",
      "epcho= 1521 loss=  tensor(0.0007)\n",
      "epcho= 1522 loss=  tensor(0.0007)\n",
      "epcho= 1523 loss=  tensor(0.0007)\n",
      "epcho= 1524 loss=  tensor(0.0007)\n",
      "epcho= 1525 loss=  tensor(0.0007)\n",
      "epcho= 1526 loss=  tensor(0.0007)\n",
      "epcho= 1527 loss=  tensor(0.0007)\n",
      "epcho= 1528 loss=  tensor(0.0007)\n",
      "epcho= 1529 loss=  tensor(0.0007)\n",
      "epcho= 1530 loss=  tensor(0.0007)\n",
      "epcho= 1531 loss=  tensor(0.0007)\n",
      "epcho= 1532 loss=  tensor(0.0007)\n",
      "epcho= 1533 loss=  tensor(0.0007)\n",
      "epcho= 1534 loss=  tensor(0.0007)\n",
      "epcho= 1535 loss=  tensor(0.0007)\n",
      "epcho= 1536 loss=  tensor(0.0007)\n",
      "epcho= 1537 loss=  tensor(0.0007)\n",
      "epcho= 1538 loss=  tensor(0.0007)\n",
      "epcho= 1539 loss=  tensor(0.0007)\n",
      "epcho= 1540 loss=  tensor(0.0007)\n",
      "epcho= 1541 loss=  tensor(0.0007)\n",
      "epcho= 1542 loss=  tensor(0.0007)\n",
      "epcho= 1543 loss=  tensor(0.0007)\n",
      "epcho= 1544 loss=  tensor(0.0007)\n",
      "epcho= 1545 loss=  tensor(0.0007)\n",
      "epcho= 1546 loss=  tensor(0.0007)\n",
      "epcho= 1547 loss=  tensor(0.0007)\n",
      "epcho= 1548 loss=  tensor(0.0006)\n",
      "epcho= 1549 loss=  tensor(0.0006)\n",
      "epcho= 1550 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.2028, grad_fn=<DivBackward0>)\n",
      "epcho= 1551 loss=  tensor(0.0006)\n",
      "epcho= 1552 loss=  tensor(0.0006)\n",
      "epcho= 1553 loss=  tensor(0.0006)\n",
      "epcho= 1554 loss=  tensor(0.0006)\n",
      "epcho= 1555 loss=  tensor(0.0006)\n",
      "epcho= 1556 loss=  tensor(0.0006)\n",
      "epcho= 1557 loss=  tensor(0.0006)\n",
      "epcho= 1558 loss=  tensor(0.0006)\n",
      "epcho= 1559 loss=  tensor(0.0006)\n",
      "epcho= 1560 loss=  tensor(0.0006)\n",
      "epcho= 1561 loss=  tensor(0.0006)\n",
      "epcho= 1562 loss=  tensor(0.0006)\n",
      "epcho= 1563 loss=  tensor(0.0006)\n",
      "epcho= 1564 loss=  tensor(0.0006)\n",
      "epcho= 1565 loss=  tensor(0.0006)\n",
      "epcho= 1566 loss=  tensor(0.0006)\n",
      "epcho= 1567 loss=  tensor(0.0006)\n",
      "epcho= 1568 loss=  tensor(0.0006)\n",
      "epcho= 1569 loss=  tensor(0.0006)\n",
      "epcho= 1570 loss=  tensor(0.0006)\n",
      "epcho= 1571 loss=  tensor(0.0006)\n",
      "epcho= 1572 loss=  tensor(0.0006)\n",
      "epcho= 1573 loss=  tensor(0.0006)\n",
      "epcho= 1574 loss=  tensor(0.0006)\n",
      "epcho= 1575 loss=  tensor(0.0006)\n",
      "epcho= 1576 loss=  tensor(0.0006)\n",
      "epcho= 1577 loss=  tensor(0.0006)\n",
      "epcho= 1578 loss=  tensor(0.0006)\n",
      "epcho= 1579 loss=  tensor(0.0006)\n",
      "epcho= 1580 loss=  tensor(0.0006)\n",
      "epcho= 1581 loss=  tensor(0.0006)\n",
      "epcho= 1582 loss=  tensor(0.0006)\n",
      "epcho= 1583 loss=  tensor(0.0006)\n",
      "epcho= 1584 loss=  tensor(0.0006)\n",
      "epcho= 1585 loss=  tensor(0.0006)\n",
      "epcho= 1586 loss=  tensor(0.0006)\n",
      "epcho= 1587 loss=  tensor(0.0006)\n",
      "epcho= 1588 loss=  tensor(0.0006)\n",
      "epcho= 1589 loss=  tensor(0.0006)\n",
      "epcho= 1590 loss=  tensor(0.0006)\n",
      "epcho= 1591 loss=  tensor(0.0006)\n",
      "epcho= 1592 loss=  tensor(0.0006)\n",
      "epcho= 1593 loss=  tensor(0.0006)\n",
      "epcho= 1594 loss=  tensor(0.0006)\n",
      "epcho= 1595 loss=  tensor(0.0006)\n",
      "epcho= 1596 loss=  tensor(0.0006)\n",
      "epcho= 1597 loss=  tensor(0.0006)\n",
      "epcho= 1598 loss=  tensor(0.0006)\n",
      "epcho= 1599 loss=  tensor(0.0006)\n",
      "epcho= 1600 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.2010, grad_fn=<DivBackward0>)\n",
      "epcho= 1601 loss=  tensor(0.0006)\n",
      "epcho= 1602 loss=  tensor(0.0006)\n",
      "epcho= 1603 loss=  tensor(0.0006)\n",
      "epcho= 1604 loss=  tensor(0.0006)\n",
      "epcho= 1605 loss=  tensor(0.0006)\n",
      "epcho= 1606 loss=  tensor(0.0006)\n",
      "epcho= 1607 loss=  tensor(0.0006)\n",
      "epcho= 1608 loss=  tensor(0.0006)\n",
      "epcho= 1609 loss=  tensor(0.0006)\n",
      "epcho= 1610 loss=  tensor(0.0006)\n",
      "epcho= 1611 loss=  tensor(0.0006)\n",
      "epcho= 1612 loss=  tensor(0.0006)\n",
      "epcho= 1613 loss=  tensor(0.0006)\n",
      "epcho= 1614 loss=  tensor(0.0006)\n",
      "epcho= 1615 loss=  tensor(0.0006)\n",
      "epcho= 1616 loss=  tensor(0.0006)\n",
      "epcho= 1617 loss=  tensor(0.0006)\n",
      "epcho= 1618 loss=  tensor(0.0006)\n",
      "epcho= 1619 loss=  tensor(0.0006)\n",
      "epcho= 1620 loss=  tensor(0.0006)\n",
      "epcho= 1621 loss=  tensor(0.0006)\n",
      "epcho= 1622 loss=  tensor(0.0006)\n",
      "epcho= 1623 loss=  tensor(0.0006)\n",
      "epcho= 1624 loss=  tensor(0.0006)\n",
      "epcho= 1625 loss=  tensor(0.0006)\n",
      "epcho= 1626 loss=  tensor(0.0006)\n",
      "epcho= 1627 loss=  tensor(0.0006)\n",
      "epcho= 1628 loss=  tensor(0.0006)\n",
      "epcho= 1629 loss=  tensor(0.0006)\n",
      "epcho= 1630 loss=  tensor(0.0006)\n",
      "epcho= 1631 loss=  tensor(0.0006)\n",
      "epcho= 1632 loss=  tensor(0.0006)\n",
      "epcho= 1633 loss=  tensor(0.0006)\n",
      "epcho= 1634 loss=  tensor(0.0006)\n",
      "epcho= 1635 loss=  tensor(0.0006)\n",
      "epcho= 1636 loss=  tensor(0.0006)\n",
      "epcho= 1637 loss=  tensor(0.0006)\n",
      "epcho= 1638 loss=  tensor(0.0006)\n",
      "epcho= 1639 loss=  tensor(0.0006)\n",
      "epcho= 1640 loss=  tensor(0.0006)\n",
      "epcho= 1641 loss=  tensor(0.0006)\n",
      "epcho= 1642 loss=  tensor(0.0006)\n",
      "epcho= 1643 loss=  tensor(0.0006)\n",
      "epcho= 1644 loss=  tensor(0.0006)\n",
      "epcho= 1645 loss=  tensor(0.0006)\n",
      "epcho= 1646 loss=  tensor(0.0006)\n",
      "epcho= 1647 loss=  tensor(0.0006)\n",
      "epcho= 1648 loss=  tensor(0.0006)\n",
      "epcho= 1649 loss=  tensor(0.0006)\n",
      "epcho= 1650 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.1998, grad_fn=<DivBackward0>)\n",
      "epcho= 1651 loss=  tensor(0.0006)\n",
      "epcho= 1652 loss=  tensor(0.0006)\n",
      "epcho= 1653 loss=  tensor(0.0006)\n",
      "epcho= 1654 loss=  tensor(0.0006)\n",
      "epcho= 1655 loss=  tensor(0.0006)\n",
      "epcho= 1656 loss=  tensor(0.0006)\n",
      "epcho= 1657 loss=  tensor(0.0006)\n",
      "epcho= 1658 loss=  tensor(0.0006)\n",
      "epcho= 1659 loss=  tensor(0.0006)\n",
      "epcho= 1660 loss=  tensor(0.0006)\n",
      "epcho= 1661 loss=  tensor(0.0006)\n",
      "epcho= 1662 loss=  tensor(0.0006)\n",
      "epcho= 1663 loss=  tensor(0.0006)\n",
      "epcho= 1664 loss=  tensor(0.0006)\n",
      "epcho= 1665 loss=  tensor(0.0006)\n",
      "epcho= 1666 loss=  tensor(0.0006)\n",
      "epcho= 1667 loss=  tensor(0.0006)\n",
      "epcho= 1668 loss=  tensor(0.0006)\n",
      "epcho= 1669 loss=  tensor(0.0006)\n",
      "epcho= 1670 loss=  tensor(0.0006)\n",
      "epcho= 1671 loss=  tensor(0.0006)\n",
      "epcho= 1672 loss=  tensor(0.0006)\n",
      "epcho= 1673 loss=  tensor(0.0006)\n",
      "epcho= 1674 loss=  tensor(0.0006)\n",
      "epcho= 1675 loss=  tensor(0.0006)\n",
      "epcho= 1676 loss=  tensor(0.0006)\n",
      "epcho= 1677 loss=  tensor(0.0006)\n",
      "epcho= 1678 loss=  tensor(0.0006)\n",
      "epcho= 1679 loss=  tensor(0.0006)\n",
      "epcho= 1680 loss=  tensor(0.0006)\n",
      "epcho= 1681 loss=  tensor(0.0006)\n",
      "epcho= 1682 loss=  tensor(0.0006)\n",
      "epcho= 1683 loss=  tensor(0.0006)\n",
      "epcho= 1684 loss=  tensor(0.0006)\n",
      "epcho= 1685 loss=  tensor(0.0006)\n",
      "epcho= 1686 loss=  tensor(0.0006)\n",
      "epcho= 1687 loss=  tensor(0.0006)\n",
      "epcho= 1688 loss=  tensor(0.0006)\n",
      "epcho= 1689 loss=  tensor(0.0006)\n",
      "epcho= 1690 loss=  tensor(0.0006)\n",
      "epcho= 1691 loss=  tensor(0.0006)\n",
      "epcho= 1692 loss=  tensor(0.0006)\n",
      "epcho= 1693 loss=  tensor(0.0006)\n",
      "epcho= 1694 loss=  tensor(0.0006)\n",
      "epcho= 1695 loss=  tensor(0.0006)\n",
      "epcho= 1696 loss=  tensor(0.0006)\n",
      "epcho= 1697 loss=  tensor(0.0006)\n",
      "epcho= 1698 loss=  tensor(0.0006)\n",
      "epcho= 1699 loss=  tensor(0.0006)\n",
      "epcho= 1700 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.1988, grad_fn=<DivBackward0>)\n",
      "epcho= 1701 loss=  tensor(0.0006)\n",
      "epcho= 1702 loss=  tensor(0.0006)\n",
      "epcho= 1703 loss=  tensor(0.0006)\n",
      "epcho= 1704 loss=  tensor(0.0006)\n",
      "epcho= 1705 loss=  tensor(0.0006)\n",
      "epcho= 1706 loss=  tensor(0.0006)\n",
      "epcho= 1707 loss=  tensor(0.0006)\n",
      "epcho= 1708 loss=  tensor(0.0006)\n",
      "epcho= 1709 loss=  tensor(0.0006)\n",
      "epcho= 1710 loss=  tensor(0.0006)\n",
      "epcho= 1711 loss=  tensor(0.0006)\n",
      "epcho= 1712 loss=  tensor(0.0006)\n",
      "epcho= 1713 loss=  tensor(0.0006)\n",
      "epcho= 1714 loss=  tensor(0.0006)\n",
      "epcho= 1715 loss=  tensor(0.0006)\n",
      "epcho= 1716 loss=  tensor(0.0006)\n",
      "epcho= 1717 loss=  tensor(0.0006)\n",
      "epcho= 1718 loss=  tensor(0.0006)\n",
      "epcho= 1719 loss=  tensor(0.0006)\n",
      "epcho= 1720 loss=  tensor(0.0006)\n",
      "epcho= 1721 loss=  tensor(0.0006)\n",
      "epcho= 1722 loss=  tensor(0.0006)\n",
      "epcho= 1723 loss=  tensor(0.0006)\n",
      "epcho= 1724 loss=  tensor(0.0006)\n",
      "epcho= 1725 loss=  tensor(0.0006)\n",
      "epcho= 1726 loss=  tensor(0.0006)\n",
      "epcho= 1727 loss=  tensor(0.0006)\n",
      "epcho= 1728 loss=  tensor(0.0006)\n",
      "epcho= 1729 loss=  tensor(0.0006)\n",
      "epcho= 1730 loss=  tensor(0.0006)\n",
      "epcho= 1731 loss=  tensor(0.0006)\n",
      "epcho= 1732 loss=  tensor(0.0006)\n",
      "epcho= 1733 loss=  tensor(0.0006)\n",
      "epcho= 1734 loss=  tensor(0.0006)\n",
      "epcho= 1735 loss=  tensor(0.0006)\n",
      "epcho= 1736 loss=  tensor(0.0006)\n",
      "epcho= 1737 loss=  tensor(0.0006)\n",
      "epcho= 1738 loss=  tensor(0.0006)\n",
      "epcho= 1739 loss=  tensor(0.0006)\n",
      "epcho= 1740 loss=  tensor(0.0006)\n",
      "epcho= 1741 loss=  tensor(0.0006)\n",
      "epcho= 1742 loss=  tensor(0.0006)\n",
      "epcho= 1743 loss=  tensor(0.0006)\n",
      "epcho= 1744 loss=  tensor(0.0006)\n",
      "epcho= 1745 loss=  tensor(0.0006)\n",
      "epcho= 1746 loss=  tensor(0.0006)\n",
      "epcho= 1747 loss=  tensor(0.0006)\n",
      "epcho= 1748 loss=  tensor(0.0006)\n",
      "epcho= 1749 loss=  tensor(0.0006)\n",
      "epcho= 1750 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.1980, grad_fn=<DivBackward0>)\n",
      "epcho= 1751 loss=  tensor(0.0006)\n",
      "epcho= 1752 loss=  tensor(0.0006)\n",
      "epcho= 1753 loss=  tensor(0.0006)\n",
      "epcho= 1754 loss=  tensor(0.0006)\n",
      "epcho= 1755 loss=  tensor(0.0006)\n",
      "epcho= 1756 loss=  tensor(0.0006)\n",
      "epcho= 1757 loss=  tensor(0.0006)\n",
      "epcho= 1758 loss=  tensor(0.0006)\n",
      "epcho= 1759 loss=  tensor(0.0006)\n",
      "epcho= 1760 loss=  tensor(0.0006)\n",
      "epcho= 1761 loss=  tensor(0.0006)\n",
      "epcho= 1762 loss=  tensor(0.0006)\n",
      "epcho= 1763 loss=  tensor(0.0006)\n",
      "epcho= 1764 loss=  tensor(0.0006)\n",
      "epcho= 1765 loss=  tensor(0.0006)\n",
      "epcho= 1766 loss=  tensor(0.0006)\n",
      "epcho= 1767 loss=  tensor(0.0006)\n",
      "epcho= 1768 loss=  tensor(0.0006)\n",
      "epcho= 1769 loss=  tensor(0.0006)\n",
      "epcho= 1770 loss=  tensor(0.0006)\n",
      "epcho= 1771 loss=  tensor(0.0006)\n",
      "epcho= 1772 loss=  tensor(0.0006)\n",
      "epcho= 1773 loss=  tensor(0.0006)\n",
      "epcho= 1774 loss=  tensor(0.0006)\n",
      "epcho= 1775 loss=  tensor(0.0006)\n",
      "epcho= 1776 loss=  tensor(0.0006)\n",
      "epcho= 1777 loss=  tensor(0.0006)\n",
      "epcho= 1778 loss=  tensor(0.0006)\n",
      "epcho= 1779 loss=  tensor(0.0006)\n",
      "epcho= 1780 loss=  tensor(0.0006)\n",
      "epcho= 1781 loss=  tensor(0.0006)\n",
      "epcho= 1782 loss=  tensor(0.0006)\n",
      "epcho= 1783 loss=  tensor(0.0006)\n",
      "epcho= 1784 loss=  tensor(0.0006)\n",
      "epcho= 1785 loss=  tensor(0.0006)\n",
      "epcho= 1786 loss=  tensor(0.0006)\n",
      "epcho= 1787 loss=  tensor(0.0006)\n",
      "epcho= 1788 loss=  tensor(0.0006)\n",
      "epcho= 1789 loss=  tensor(0.0006)\n",
      "epcho= 1790 loss=  tensor(0.0006)\n",
      "epcho= 1791 loss=  tensor(0.0006)\n",
      "epcho= 1792 loss=  tensor(0.0006)\n",
      "epcho= 1793 loss=  tensor(0.0006)\n",
      "epcho= 1794 loss=  tensor(0.0006)\n",
      "epcho= 1795 loss=  tensor(0.0006)\n",
      "epcho= 1796 loss=  tensor(0.0006)\n",
      "epcho= 1797 loss=  tensor(0.0006)\n",
      "epcho= 1798 loss=  tensor(0.0006)\n",
      "epcho= 1799 loss=  tensor(0.0006)\n",
      "epcho= 1800 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.1972, grad_fn=<DivBackward0>)\n",
      "epcho= 1801 loss=  tensor(0.0006)\n",
      "epcho= 1802 loss=  tensor(0.0006)\n",
      "epcho= 1803 loss=  tensor(0.0006)\n",
      "epcho= 1804 loss=  tensor(0.0006)\n",
      "epcho= 1805 loss=  tensor(0.0006)\n",
      "epcho= 1806 loss=  tensor(0.0006)\n",
      "epcho= 1807 loss=  tensor(0.0006)\n",
      "epcho= 1808 loss=  tensor(0.0006)\n",
      "epcho= 1809 loss=  tensor(0.0006)\n",
      "epcho= 1810 loss=  tensor(0.0006)\n",
      "epcho= 1811 loss=  tensor(0.0006)\n",
      "epcho= 1812 loss=  tensor(0.0006)\n",
      "epcho= 1813 loss=  tensor(0.0006)\n",
      "epcho= 1814 loss=  tensor(0.0006)\n",
      "epcho= 1815 loss=  tensor(0.0006)\n",
      "epcho= 1816 loss=  tensor(0.0006)\n",
      "epcho= 1817 loss=  tensor(0.0006)\n",
      "epcho= 1818 loss=  tensor(0.0006)\n",
      "epcho= 1819 loss=  tensor(0.0006)\n",
      "epcho= 1820 loss=  tensor(0.0006)\n",
      "epcho= 1821 loss=  tensor(0.0006)\n",
      "epcho= 1822 loss=  tensor(0.0006)\n",
      "epcho= 1823 loss=  tensor(0.0006)\n",
      "epcho= 1824 loss=  tensor(0.0006)\n",
      "epcho= 1825 loss=  tensor(0.0006)\n",
      "epcho= 1826 loss=  tensor(0.0006)\n",
      "epcho= 1827 loss=  tensor(0.0006)\n",
      "epcho= 1828 loss=  tensor(0.0006)\n",
      "epcho= 1829 loss=  tensor(0.0006)\n",
      "epcho= 1830 loss=  tensor(0.0006)\n",
      "epcho= 1831 loss=  tensor(0.0006)\n",
      "epcho= 1832 loss=  tensor(0.0006)\n",
      "epcho= 1833 loss=  tensor(0.0006)\n",
      "epcho= 1834 loss=  tensor(0.0006)\n",
      "epcho= 1835 loss=  tensor(0.0006)\n",
      "epcho= 1836 loss=  tensor(0.0006)\n",
      "epcho= 1837 loss=  tensor(0.0006)\n",
      "epcho= 1838 loss=  tensor(0.0006)\n",
      "epcho= 1839 loss=  tensor(0.0006)\n",
      "epcho= 1840 loss=  tensor(0.0006)\n",
      "epcho= 1841 loss=  tensor(0.0006)\n",
      "epcho= 1842 loss=  tensor(0.0006)\n",
      "epcho= 1843 loss=  tensor(0.0006)\n",
      "epcho= 1844 loss=  tensor(0.0006)\n",
      "epcho= 1845 loss=  tensor(0.0006)\n",
      "epcho= 1846 loss=  tensor(0.0006)\n",
      "epcho= 1847 loss=  tensor(0.0006)\n",
      "epcho= 1848 loss=  tensor(0.0006)\n",
      "epcho= 1849 loss=  tensor(0.0006)\n",
      "epcho= 1850 loss=  tensor(0.0006)\n",
      "tensor(0.0006, grad_fn=<AddBackward0>) tensor(0.1965, grad_fn=<DivBackward0>)\n",
      "epcho= 1851 loss=  tensor(0.0006)\n",
      "epcho= 1852 loss=  tensor(0.0006)\n",
      "epcho= 1853 loss=  tensor(0.0006)\n",
      "epcho= 1854 loss=  tensor(0.0006)\n",
      "epcho= 1855 loss=  tensor(0.0006)\n",
      "epcho= 1856 loss=  tensor(0.0006)\n",
      "epcho= 1857 loss=  tensor(0.0006)\n",
      "epcho= 1858 loss=  tensor(0.0006)\n",
      "epcho= 1859 loss=  tensor(0.0006)\n",
      "epcho= 1860 loss=  tensor(0.0006)\n",
      "epcho= 1861 loss=  tensor(0.0006)\n",
      "epcho= 1862 loss=  tensor(0.0006)\n",
      "epcho= 1863 loss=  tensor(0.0006)\n",
      "epcho= 1864 loss=  tensor(0.0006)\n",
      "epcho= 1865 loss=  tensor(0.0006)\n",
      "epcho= 1866 loss=  tensor(0.0006)\n",
      "epcho= 1867 loss=  tensor(0.0006)\n",
      "epcho= 1868 loss=  tensor(0.0006)\n",
      "epcho= 1869 loss=  tensor(0.0006)\n",
      "epcho= 1870 loss=  tensor(0.0006)\n",
      "epcho= 1871 loss=  tensor(0.0006)\n",
      "epcho= 1872 loss=  tensor(0.0006)\n",
      "epcho= 1873 loss=  tensor(0.0006)\n",
      "epcho= 1874 loss=  tensor(0.0006)\n",
      "epcho= 1875 loss=  tensor(0.0006)\n",
      "epcho= 1876 loss=  tensor(0.0006)\n",
      "epcho= 1877 loss=  tensor(0.0006)\n",
      "epcho= 1878 loss=  tensor(0.0006)\n",
      "epcho= 1879 loss=  tensor(0.0006)\n",
      "epcho= 1880 loss=  tensor(0.0006)\n",
      "epcho= 1881 loss=  tensor(0.0006)\n",
      "epcho= 1882 loss=  tensor(0.0006)\n",
      "epcho= 1883 loss=  tensor(0.0006)\n",
      "epcho= 1884 loss=  tensor(0.0006)\n",
      "epcho= 1885 loss=  tensor(0.0006)\n",
      "epcho= 1886 loss=  tensor(0.0006)\n",
      "epcho= 1887 loss=  tensor(0.0006)\n",
      "epcho= 1888 loss=  tensor(0.0006)\n",
      "epcho= 1889 loss=  tensor(0.0006)\n",
      "epcho= 1890 loss=  tensor(0.0005)\n",
      "epcho= 1891 loss=  tensor(0.0005)\n",
      "epcho= 1892 loss=  tensor(0.0005)\n",
      "epcho= 1893 loss=  tensor(0.0005)\n",
      "epcho= 1894 loss=  tensor(0.0005)\n",
      "epcho= 1895 loss=  tensor(0.0005)\n",
      "epcho= 1896 loss=  tensor(0.0005)\n",
      "epcho= 1897 loss=  tensor(0.0005)\n",
      "epcho= 1898 loss=  tensor(0.0005)\n",
      "epcho= 1899 loss=  tensor(0.0005)\n",
      "epcho= 1900 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1959, grad_fn=<DivBackward0>)\n",
      "epcho= 1901 loss=  tensor(0.0005)\n",
      "epcho= 1902 loss=  tensor(0.0005)\n",
      "epcho= 1903 loss=  tensor(0.0005)\n",
      "epcho= 1904 loss=  tensor(0.0005)\n",
      "epcho= 1905 loss=  tensor(0.0005)\n",
      "epcho= 1906 loss=  tensor(0.0005)\n",
      "epcho= 1907 loss=  tensor(0.0005)\n",
      "epcho= 1908 loss=  tensor(0.0005)\n",
      "epcho= 1909 loss=  tensor(0.0005)\n",
      "epcho= 1910 loss=  tensor(0.0005)\n",
      "epcho= 1911 loss=  tensor(0.0005)\n",
      "epcho= 1912 loss=  tensor(0.0005)\n",
      "epcho= 1913 loss=  tensor(0.0005)\n",
      "epcho= 1914 loss=  tensor(0.0005)\n",
      "epcho= 1915 loss=  tensor(0.0005)\n",
      "epcho= 1916 loss=  tensor(0.0005)\n",
      "epcho= 1917 loss=  tensor(0.0005)\n",
      "epcho= 1918 loss=  tensor(0.0005)\n",
      "epcho= 1919 loss=  tensor(0.0005)\n",
      "epcho= 1920 loss=  tensor(0.0005)\n",
      "epcho= 1921 loss=  tensor(0.0005)\n",
      "epcho= 1922 loss=  tensor(0.0005)\n",
      "epcho= 1923 loss=  tensor(0.0005)\n",
      "epcho= 1924 loss=  tensor(0.0005)\n",
      "epcho= 1925 loss=  tensor(0.0005)\n",
      "epcho= 1926 loss=  tensor(0.0005)\n",
      "epcho= 1927 loss=  tensor(0.0005)\n",
      "epcho= 1928 loss=  tensor(0.0005)\n",
      "epcho= 1929 loss=  tensor(0.0005)\n",
      "epcho= 1930 loss=  tensor(0.0005)\n",
      "epcho= 1931 loss=  tensor(0.0005)\n",
      "epcho= 1932 loss=  tensor(0.0005)\n",
      "epcho= 1933 loss=  tensor(0.0005)\n",
      "epcho= 1934 loss=  tensor(0.0005)\n",
      "epcho= 1935 loss=  tensor(0.0005)\n",
      "epcho= 1936 loss=  tensor(0.0005)\n",
      "epcho= 1937 loss=  tensor(0.0005)\n",
      "epcho= 1938 loss=  tensor(0.0005)\n",
      "epcho= 1939 loss=  tensor(0.0005)\n",
      "epcho= 1940 loss=  tensor(0.0005)\n",
      "epcho= 1941 loss=  tensor(0.0005)\n",
      "epcho= 1942 loss=  tensor(0.0005)\n",
      "epcho= 1943 loss=  tensor(0.0005)\n",
      "epcho= 1944 loss=  tensor(0.0005)\n",
      "epcho= 1945 loss=  tensor(0.0005)\n",
      "epcho= 1946 loss=  tensor(0.0005)\n",
      "epcho= 1947 loss=  tensor(0.0005)\n",
      "epcho= 1948 loss=  tensor(0.0005)\n",
      "epcho= 1949 loss=  tensor(0.0005)\n",
      "epcho= 1950 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1953, grad_fn=<DivBackward0>)\n",
      "epcho= 1951 loss=  tensor(0.0005)\n",
      "epcho= 1952 loss=  tensor(0.0005)\n",
      "epcho= 1953 loss=  tensor(0.0005)\n",
      "epcho= 1954 loss=  tensor(0.0005)\n",
      "epcho= 1955 loss=  tensor(0.0005)\n",
      "epcho= 1956 loss=  tensor(0.0005)\n",
      "epcho= 1957 loss=  tensor(0.0005)\n",
      "epcho= 1958 loss=  tensor(0.0005)\n",
      "epcho= 1959 loss=  tensor(0.0005)\n",
      "epcho= 1960 loss=  tensor(0.0005)\n",
      "epcho= 1961 loss=  tensor(0.0005)\n",
      "epcho= 1962 loss=  tensor(0.0005)\n",
      "epcho= 1963 loss=  tensor(0.0005)\n",
      "epcho= 1964 loss=  tensor(0.0005)\n",
      "epcho= 1965 loss=  tensor(0.0005)\n",
      "epcho= 1966 loss=  tensor(0.0005)\n",
      "epcho= 1967 loss=  tensor(0.0005)\n",
      "epcho= 1968 loss=  tensor(0.0005)\n",
      "epcho= 1969 loss=  tensor(0.0005)\n",
      "epcho= 1970 loss=  tensor(0.0005)\n",
      "epcho= 1971 loss=  tensor(0.0005)\n",
      "epcho= 1972 loss=  tensor(0.0005)\n",
      "epcho= 1973 loss=  tensor(0.0005)\n",
      "epcho= 1974 loss=  tensor(0.0005)\n",
      "epcho= 1975 loss=  tensor(0.0005)\n",
      "epcho= 1976 loss=  tensor(0.0005)\n",
      "epcho= 1977 loss=  tensor(0.0005)\n",
      "epcho= 1978 loss=  tensor(0.0005)\n",
      "epcho= 1979 loss=  tensor(0.0005)\n",
      "epcho= 1980 loss=  tensor(0.0005)\n",
      "epcho= 1981 loss=  tensor(0.0005)\n",
      "epcho= 1982 loss=  tensor(0.0005)\n",
      "epcho= 1983 loss=  tensor(0.0005)\n",
      "epcho= 1984 loss=  tensor(0.0005)\n",
      "epcho= 1985 loss=  tensor(0.0005)\n",
      "epcho= 1986 loss=  tensor(0.0005)\n",
      "epcho= 1987 loss=  tensor(0.0005)\n",
      "epcho= 1988 loss=  tensor(0.0005)\n",
      "epcho= 1989 loss=  tensor(0.0005)\n",
      "epcho= 1990 loss=  tensor(0.0005)\n",
      "epcho= 1991 loss=  tensor(0.0005)\n",
      "epcho= 1992 loss=  tensor(0.0005)\n",
      "epcho= 1993 loss=  tensor(0.0005)\n",
      "epcho= 1994 loss=  tensor(0.0005)\n",
      "epcho= 1995 loss=  tensor(0.0005)\n",
      "epcho= 1996 loss=  tensor(0.0005)\n",
      "epcho= 1997 loss=  tensor(0.0005)\n",
      "epcho= 1998 loss=  tensor(0.0005)\n",
      "epcho= 1999 loss=  tensor(0.0005)\n",
      "epcho= 2000 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1947, grad_fn=<DivBackward0>)\n",
      "epcho= 2001 loss=  tensor(0.0005)\n",
      "epcho= 2002 loss=  tensor(0.0005)\n",
      "epcho= 2003 loss=  tensor(0.0005)\n",
      "epcho= 2004 loss=  tensor(0.0005)\n",
      "epcho= 2005 loss=  tensor(0.0005)\n",
      "epcho= 2006 loss=  tensor(0.0005)\n",
      "epcho= 2007 loss=  tensor(0.0005)\n",
      "epcho= 2008 loss=  tensor(0.0005)\n",
      "epcho= 2009 loss=  tensor(0.0005)\n",
      "epcho= 2010 loss=  tensor(0.0005)\n",
      "epcho= 2011 loss=  tensor(0.0005)\n",
      "epcho= 2012 loss=  tensor(0.0005)\n",
      "epcho= 2013 loss=  tensor(0.0005)\n",
      "epcho= 2014 loss=  tensor(0.0005)\n",
      "epcho= 2015 loss=  tensor(0.0005)\n",
      "epcho= 2016 loss=  tensor(0.0005)\n",
      "epcho= 2017 loss=  tensor(0.0005)\n",
      "epcho= 2018 loss=  tensor(0.0005)\n",
      "epcho= 2019 loss=  tensor(0.0005)\n",
      "epcho= 2020 loss=  tensor(0.0005)\n",
      "epcho= 2021 loss=  tensor(0.0005)\n",
      "epcho= 2022 loss=  tensor(0.0005)\n",
      "epcho= 2023 loss=  tensor(0.0005)\n",
      "epcho= 2024 loss=  tensor(0.0005)\n",
      "epcho= 2025 loss=  tensor(0.0005)\n",
      "epcho= 2026 loss=  tensor(0.0005)\n",
      "epcho= 2027 loss=  tensor(0.0005)\n",
      "epcho= 2028 loss=  tensor(0.0005)\n",
      "epcho= 2029 loss=  tensor(0.0005)\n",
      "epcho= 2030 loss=  tensor(0.0005)\n",
      "epcho= 2031 loss=  tensor(0.0005)\n",
      "epcho= 2032 loss=  tensor(0.0005)\n",
      "epcho= 2033 loss=  tensor(0.0005)\n",
      "epcho= 2034 loss=  tensor(0.0005)\n",
      "epcho= 2035 loss=  tensor(0.0005)\n",
      "epcho= 2036 loss=  tensor(0.0005)\n",
      "epcho= 2037 loss=  tensor(0.0005)\n",
      "epcho= 2038 loss=  tensor(0.0005)\n",
      "epcho= 2039 loss=  tensor(0.0005)\n",
      "epcho= 2040 loss=  tensor(0.0005)\n",
      "epcho= 2041 loss=  tensor(0.0005)\n",
      "epcho= 2042 loss=  tensor(0.0005)\n",
      "epcho= 2043 loss=  tensor(0.0005)\n",
      "epcho= 2044 loss=  tensor(0.0005)\n",
      "epcho= 2045 loss=  tensor(0.0005)\n",
      "epcho= 2046 loss=  tensor(0.0005)\n",
      "epcho= 2047 loss=  tensor(0.0005)\n",
      "epcho= 2048 loss=  tensor(0.0005)\n",
      "epcho= 2049 loss=  tensor(0.0005)\n",
      "epcho= 2050 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1942, grad_fn=<DivBackward0>)\n",
      "epcho= 2051 loss=  tensor(0.0005)\n",
      "epcho= 2052 loss=  tensor(0.0005)\n",
      "epcho= 2053 loss=  tensor(0.0005)\n",
      "epcho= 2054 loss=  tensor(0.0005)\n",
      "epcho= 2055 loss=  tensor(0.0005)\n",
      "epcho= 2056 loss=  tensor(0.0005)\n",
      "epcho= 2057 loss=  tensor(0.0005)\n",
      "epcho= 2058 loss=  tensor(0.0005)\n",
      "epcho= 2059 loss=  tensor(0.0005)\n",
      "epcho= 2060 loss=  tensor(0.0005)\n",
      "epcho= 2061 loss=  tensor(0.0005)\n",
      "epcho= 2062 loss=  tensor(0.0005)\n",
      "epcho= 2063 loss=  tensor(0.0005)\n",
      "epcho= 2064 loss=  tensor(0.0005)\n",
      "epcho= 2065 loss=  tensor(0.0005)\n",
      "epcho= 2066 loss=  tensor(0.0005)\n",
      "epcho= 2067 loss=  tensor(0.0005)\n",
      "epcho= 2068 loss=  tensor(0.0005)\n",
      "epcho= 2069 loss=  tensor(0.0005)\n",
      "epcho= 2070 loss=  tensor(0.0005)\n",
      "epcho= 2071 loss=  tensor(0.0005)\n",
      "epcho= 2072 loss=  tensor(0.0005)\n",
      "epcho= 2073 loss=  tensor(0.0005)\n",
      "epcho= 2074 loss=  tensor(0.0005)\n",
      "epcho= 2075 loss=  tensor(0.0005)\n",
      "epcho= 2076 loss=  tensor(0.0005)\n",
      "epcho= 2077 loss=  tensor(0.0005)\n",
      "epcho= 2078 loss=  tensor(0.0005)\n",
      "epcho= 2079 loss=  tensor(0.0005)\n",
      "epcho= 2080 loss=  tensor(0.0005)\n",
      "epcho= 2081 loss=  tensor(0.0005)\n",
      "epcho= 2082 loss=  tensor(0.0005)\n",
      "epcho= 2083 loss=  tensor(0.0005)\n",
      "epcho= 2084 loss=  tensor(0.0005)\n",
      "epcho= 2085 loss=  tensor(0.0005)\n",
      "epcho= 2086 loss=  tensor(0.0005)\n",
      "epcho= 2087 loss=  tensor(0.0005)\n",
      "epcho= 2088 loss=  tensor(0.0005)\n",
      "epcho= 2089 loss=  tensor(0.0005)\n",
      "epcho= 2090 loss=  tensor(0.0005)\n",
      "epcho= 2091 loss=  tensor(0.0006)\n",
      "epcho= 2092 loss=  tensor(0.0006)\n",
      "epcho= 2093 loss=  tensor(0.0007)\n",
      "epcho= 2094 loss=  tensor(0.0008)\n",
      "epcho= 2095 loss=  tensor(0.0010)\n",
      "epcho= 2096 loss=  tensor(0.0014)\n",
      "epcho= 2097 loss=  tensor(0.0020)\n",
      "epcho= 2098 loss=  tensor(0.0030)\n",
      "epcho= 2099 loss=  tensor(0.0045)\n",
      "epcho= 2100 loss=  tensor(0.0067)\n",
      "tensor(0.0067, grad_fn=<AddBackward0>) tensor(0.2533, grad_fn=<DivBackward0>)\n",
      "epcho= 2101 loss=  tensor(0.0085)\n",
      "epcho= 2102 loss=  tensor(0.0086)\n",
      "epcho= 2103 loss=  tensor(0.0062)\n",
      "epcho= 2104 loss=  tensor(0.0029)\n",
      "epcho= 2105 loss=  tensor(0.0009)\n",
      "epcho= 2106 loss=  tensor(0.0012)\n",
      "epcho= 2107 loss=  tensor(0.0028)\n",
      "epcho= 2108 loss=  tensor(0.0037)\n",
      "epcho= 2109 loss=  tensor(0.0030)\n",
      "epcho= 2110 loss=  tensor(0.0015)\n",
      "epcho= 2111 loss=  tensor(0.0009)\n",
      "epcho= 2112 loss=  tensor(0.0014)\n",
      "epcho= 2113 loss=  tensor(0.0019)\n",
      "epcho= 2114 loss=  tensor(0.0018)\n",
      "epcho= 2115 loss=  tensor(0.0012)\n",
      "epcho= 2116 loss=  tensor(0.0010)\n",
      "epcho= 2117 loss=  tensor(0.0012)\n",
      "epcho= 2118 loss=  tensor(0.0013)\n",
      "epcho= 2119 loss=  tensor(0.0011)\n",
      "epcho= 2120 loss=  tensor(0.0008)\n",
      "epcho= 2121 loss=  tensor(0.0008)\n",
      "epcho= 2122 loss=  tensor(0.0010)\n",
      "epcho= 2123 loss=  tensor(0.0010)\n",
      "epcho= 2124 loss=  tensor(0.0007)\n",
      "epcho= 2125 loss=  tensor(0.0006)\n",
      "epcho= 2126 loss=  tensor(0.0007)\n",
      "epcho= 2127 loss=  tensor(0.0009)\n",
      "epcho= 2128 loss=  tensor(0.0008)\n",
      "epcho= 2129 loss=  tensor(0.0006)\n",
      "epcho= 2130 loss=  tensor(0.0005)\n",
      "epcho= 2131 loss=  tensor(0.0007)\n",
      "epcho= 2132 loss=  tensor(0.0008)\n",
      "epcho= 2133 loss=  tensor(0.0007)\n",
      "epcho= 2134 loss=  tensor(0.0005)\n",
      "epcho= 2135 loss=  tensor(0.0005)\n",
      "epcho= 2136 loss=  tensor(0.0006)\n",
      "epcho= 2137 loss=  tensor(0.0007)\n",
      "epcho= 2138 loss=  tensor(0.0006)\n",
      "epcho= 2139 loss=  tensor(0.0005)\n",
      "epcho= 2140 loss=  tensor(0.0005)\n",
      "epcho= 2141 loss=  tensor(0.0006)\n",
      "epcho= 2142 loss=  tensor(0.0006)\n",
      "epcho= 2143 loss=  tensor(0.0006)\n",
      "epcho= 2144 loss=  tensor(0.0005)\n",
      "epcho= 2145 loss=  tensor(0.0005)\n",
      "epcho= 2146 loss=  tensor(0.0005)\n",
      "epcho= 2147 loss=  tensor(0.0005)\n",
      "epcho= 2148 loss=  tensor(0.0005)\n",
      "epcho= 2149 loss=  tensor(0.0005)\n",
      "epcho= 2150 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1965, grad_fn=<DivBackward0>)\n",
      "epcho= 2151 loss=  tensor(0.0005)\n",
      "epcho= 2152 loss=  tensor(0.0005)\n",
      "epcho= 2153 loss=  tensor(0.0005)\n",
      "epcho= 2154 loss=  tensor(0.0005)\n",
      "epcho= 2155 loss=  tensor(0.0005)\n",
      "epcho= 2156 loss=  tensor(0.0005)\n",
      "epcho= 2157 loss=  tensor(0.0005)\n",
      "epcho= 2158 loss=  tensor(0.0005)\n",
      "epcho= 2159 loss=  tensor(0.0005)\n",
      "epcho= 2160 loss=  tensor(0.0005)\n",
      "epcho= 2161 loss=  tensor(0.0005)\n",
      "epcho= 2162 loss=  tensor(0.0005)\n",
      "epcho= 2163 loss=  tensor(0.0005)\n",
      "epcho= 2164 loss=  tensor(0.0005)\n",
      "epcho= 2165 loss=  tensor(0.0005)\n",
      "epcho= 2166 loss=  tensor(0.0005)\n",
      "epcho= 2167 loss=  tensor(0.0005)\n",
      "epcho= 2168 loss=  tensor(0.0005)\n",
      "epcho= 2169 loss=  tensor(0.0005)\n",
      "epcho= 2170 loss=  tensor(0.0005)\n",
      "epcho= 2171 loss=  tensor(0.0005)\n",
      "epcho= 2172 loss=  tensor(0.0005)\n",
      "epcho= 2173 loss=  tensor(0.0005)\n",
      "epcho= 2174 loss=  tensor(0.0005)\n",
      "epcho= 2175 loss=  tensor(0.0005)\n",
      "epcho= 2176 loss=  tensor(0.0005)\n",
      "epcho= 2177 loss=  tensor(0.0005)\n",
      "epcho= 2178 loss=  tensor(0.0005)\n",
      "epcho= 2179 loss=  tensor(0.0005)\n",
      "epcho= 2180 loss=  tensor(0.0005)\n",
      "epcho= 2181 loss=  tensor(0.0005)\n",
      "epcho= 2182 loss=  tensor(0.0005)\n",
      "epcho= 2183 loss=  tensor(0.0005)\n",
      "epcho= 2184 loss=  tensor(0.0005)\n",
      "epcho= 2185 loss=  tensor(0.0005)\n",
      "epcho= 2186 loss=  tensor(0.0005)\n",
      "epcho= 2187 loss=  tensor(0.0005)\n",
      "epcho= 2188 loss=  tensor(0.0005)\n",
      "epcho= 2189 loss=  tensor(0.0005)\n",
      "epcho= 2190 loss=  tensor(0.0005)\n",
      "epcho= 2191 loss=  tensor(0.0005)\n",
      "epcho= 2192 loss=  tensor(0.0005)\n",
      "epcho= 2193 loss=  tensor(0.0005)\n",
      "epcho= 2194 loss=  tensor(0.0005)\n",
      "epcho= 2195 loss=  tensor(0.0005)\n",
      "epcho= 2196 loss=  tensor(0.0005)\n",
      "epcho= 2197 loss=  tensor(0.0005)\n",
      "epcho= 2198 loss=  tensor(0.0005)\n",
      "epcho= 2199 loss=  tensor(0.0005)\n",
      "epcho= 2200 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1932, grad_fn=<DivBackward0>)\n",
      "epcho= 2201 loss=  tensor(0.0005)\n",
      "epcho= 2202 loss=  tensor(0.0005)\n",
      "epcho= 2203 loss=  tensor(0.0005)\n",
      "epcho= 2204 loss=  tensor(0.0005)\n",
      "epcho= 2205 loss=  tensor(0.0005)\n",
      "epcho= 2206 loss=  tensor(0.0005)\n",
      "epcho= 2207 loss=  tensor(0.0005)\n",
      "epcho= 2208 loss=  tensor(0.0005)\n",
      "epcho= 2209 loss=  tensor(0.0005)\n",
      "epcho= 2210 loss=  tensor(0.0005)\n",
      "epcho= 2211 loss=  tensor(0.0005)\n",
      "epcho= 2212 loss=  tensor(0.0005)\n",
      "epcho= 2213 loss=  tensor(0.0005)\n",
      "epcho= 2214 loss=  tensor(0.0005)\n",
      "epcho= 2215 loss=  tensor(0.0005)\n",
      "epcho= 2216 loss=  tensor(0.0005)\n",
      "epcho= 2217 loss=  tensor(0.0005)\n",
      "epcho= 2218 loss=  tensor(0.0005)\n",
      "epcho= 2219 loss=  tensor(0.0005)\n",
      "epcho= 2220 loss=  tensor(0.0005)\n",
      "epcho= 2221 loss=  tensor(0.0005)\n",
      "epcho= 2222 loss=  tensor(0.0005)\n",
      "epcho= 2223 loss=  tensor(0.0005)\n",
      "epcho= 2224 loss=  tensor(0.0005)\n",
      "epcho= 2225 loss=  tensor(0.0005)\n",
      "epcho= 2226 loss=  tensor(0.0005)\n",
      "epcho= 2227 loss=  tensor(0.0005)\n",
      "epcho= 2228 loss=  tensor(0.0005)\n",
      "epcho= 2229 loss=  tensor(0.0005)\n",
      "epcho= 2230 loss=  tensor(0.0005)\n",
      "epcho= 2231 loss=  tensor(0.0005)\n",
      "epcho= 2232 loss=  tensor(0.0005)\n",
      "epcho= 2233 loss=  tensor(0.0005)\n",
      "epcho= 2234 loss=  tensor(0.0005)\n",
      "epcho= 2235 loss=  tensor(0.0005)\n",
      "epcho= 2236 loss=  tensor(0.0005)\n",
      "epcho= 2237 loss=  tensor(0.0005)\n",
      "epcho= 2238 loss=  tensor(0.0005)\n",
      "epcho= 2239 loss=  tensor(0.0005)\n",
      "epcho= 2240 loss=  tensor(0.0005)\n",
      "epcho= 2241 loss=  tensor(0.0005)\n",
      "epcho= 2242 loss=  tensor(0.0005)\n",
      "epcho= 2243 loss=  tensor(0.0005)\n",
      "epcho= 2244 loss=  tensor(0.0005)\n",
      "epcho= 2245 loss=  tensor(0.0005)\n",
      "epcho= 2246 loss=  tensor(0.0005)\n",
      "epcho= 2247 loss=  tensor(0.0005)\n",
      "epcho= 2248 loss=  tensor(0.0005)\n",
      "epcho= 2249 loss=  tensor(0.0005)\n",
      "epcho= 2250 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1925, grad_fn=<DivBackward0>)\n",
      "epcho= 2251 loss=  tensor(0.0005)\n",
      "epcho= 2252 loss=  tensor(0.0005)\n",
      "epcho= 2253 loss=  tensor(0.0005)\n",
      "epcho= 2254 loss=  tensor(0.0005)\n",
      "epcho= 2255 loss=  tensor(0.0005)\n",
      "epcho= 2256 loss=  tensor(0.0005)\n",
      "epcho= 2257 loss=  tensor(0.0005)\n",
      "epcho= 2258 loss=  tensor(0.0005)\n",
      "epcho= 2259 loss=  tensor(0.0005)\n",
      "epcho= 2260 loss=  tensor(0.0005)\n",
      "epcho= 2261 loss=  tensor(0.0005)\n",
      "epcho= 2262 loss=  tensor(0.0005)\n",
      "epcho= 2263 loss=  tensor(0.0005)\n",
      "epcho= 2264 loss=  tensor(0.0005)\n",
      "epcho= 2265 loss=  tensor(0.0005)\n",
      "epcho= 2266 loss=  tensor(0.0005)\n",
      "epcho= 2267 loss=  tensor(0.0005)\n",
      "epcho= 2268 loss=  tensor(0.0005)\n",
      "epcho= 2269 loss=  tensor(0.0005)\n",
      "epcho= 2270 loss=  tensor(0.0005)\n",
      "epcho= 2271 loss=  tensor(0.0005)\n",
      "epcho= 2272 loss=  tensor(0.0005)\n",
      "epcho= 2273 loss=  tensor(0.0005)\n",
      "epcho= 2274 loss=  tensor(0.0005)\n",
      "epcho= 2275 loss=  tensor(0.0005)\n",
      "epcho= 2276 loss=  tensor(0.0005)\n",
      "epcho= 2277 loss=  tensor(0.0005)\n",
      "epcho= 2278 loss=  tensor(0.0005)\n",
      "epcho= 2279 loss=  tensor(0.0005)\n",
      "epcho= 2280 loss=  tensor(0.0005)\n",
      "epcho= 2281 loss=  tensor(0.0005)\n",
      "epcho= 2282 loss=  tensor(0.0005)\n",
      "epcho= 2283 loss=  tensor(0.0005)\n",
      "epcho= 2284 loss=  tensor(0.0005)\n",
      "epcho= 2285 loss=  tensor(0.0005)\n",
      "epcho= 2286 loss=  tensor(0.0005)\n",
      "epcho= 2287 loss=  tensor(0.0005)\n",
      "epcho= 2288 loss=  tensor(0.0005)\n",
      "epcho= 2289 loss=  tensor(0.0005)\n",
      "epcho= 2290 loss=  tensor(0.0005)\n",
      "epcho= 2291 loss=  tensor(0.0005)\n",
      "epcho= 2292 loss=  tensor(0.0005)\n",
      "epcho= 2293 loss=  tensor(0.0005)\n",
      "epcho= 2294 loss=  tensor(0.0005)\n",
      "epcho= 2295 loss=  tensor(0.0005)\n",
      "epcho= 2296 loss=  tensor(0.0005)\n",
      "epcho= 2297 loss=  tensor(0.0005)\n",
      "epcho= 2298 loss=  tensor(0.0005)\n",
      "epcho= 2299 loss=  tensor(0.0005)\n",
      "epcho= 2300 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1919, grad_fn=<DivBackward0>)\n",
      "epcho= 2301 loss=  tensor(0.0005)\n",
      "epcho= 2302 loss=  tensor(0.0005)\n",
      "epcho= 2303 loss=  tensor(0.0005)\n",
      "epcho= 2304 loss=  tensor(0.0005)\n",
      "epcho= 2305 loss=  tensor(0.0005)\n",
      "epcho= 2306 loss=  tensor(0.0005)\n",
      "epcho= 2307 loss=  tensor(0.0005)\n",
      "epcho= 2308 loss=  tensor(0.0005)\n",
      "epcho= 2309 loss=  tensor(0.0005)\n",
      "epcho= 2310 loss=  tensor(0.0005)\n",
      "epcho= 2311 loss=  tensor(0.0005)\n",
      "epcho= 2312 loss=  tensor(0.0005)\n",
      "epcho= 2313 loss=  tensor(0.0005)\n",
      "epcho= 2314 loss=  tensor(0.0005)\n",
      "epcho= 2315 loss=  tensor(0.0005)\n",
      "epcho= 2316 loss=  tensor(0.0005)\n",
      "epcho= 2317 loss=  tensor(0.0005)\n",
      "epcho= 2318 loss=  tensor(0.0005)\n",
      "epcho= 2319 loss=  tensor(0.0005)\n",
      "epcho= 2320 loss=  tensor(0.0005)\n",
      "epcho= 2321 loss=  tensor(0.0005)\n",
      "epcho= 2322 loss=  tensor(0.0005)\n",
      "epcho= 2323 loss=  tensor(0.0005)\n",
      "epcho= 2324 loss=  tensor(0.0005)\n",
      "epcho= 2325 loss=  tensor(0.0005)\n",
      "epcho= 2326 loss=  tensor(0.0005)\n",
      "epcho= 2327 loss=  tensor(0.0005)\n",
      "epcho= 2328 loss=  tensor(0.0005)\n",
      "epcho= 2329 loss=  tensor(0.0005)\n",
      "epcho= 2330 loss=  tensor(0.0005)\n",
      "epcho= 2331 loss=  tensor(0.0005)\n",
      "epcho= 2332 loss=  tensor(0.0005)\n",
      "epcho= 2333 loss=  tensor(0.0005)\n",
      "epcho= 2334 loss=  tensor(0.0005)\n",
      "epcho= 2335 loss=  tensor(0.0005)\n",
      "epcho= 2336 loss=  tensor(0.0005)\n",
      "epcho= 2337 loss=  tensor(0.0005)\n",
      "epcho= 2338 loss=  tensor(0.0005)\n",
      "epcho= 2339 loss=  tensor(0.0005)\n",
      "epcho= 2340 loss=  tensor(0.0005)\n",
      "epcho= 2341 loss=  tensor(0.0005)\n",
      "epcho= 2342 loss=  tensor(0.0005)\n",
      "epcho= 2343 loss=  tensor(0.0005)\n",
      "epcho= 2344 loss=  tensor(0.0005)\n",
      "epcho= 2345 loss=  tensor(0.0005)\n",
      "epcho= 2346 loss=  tensor(0.0005)\n",
      "epcho= 2347 loss=  tensor(0.0005)\n",
      "epcho= 2348 loss=  tensor(0.0005)\n",
      "epcho= 2349 loss=  tensor(0.0005)\n",
      "epcho= 2350 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1912, grad_fn=<DivBackward0>)\n",
      "epcho= 2351 loss=  tensor(0.0005)\n",
      "epcho= 2352 loss=  tensor(0.0005)\n",
      "epcho= 2353 loss=  tensor(0.0005)\n",
      "epcho= 2354 loss=  tensor(0.0005)\n",
      "epcho= 2355 loss=  tensor(0.0005)\n",
      "epcho= 2356 loss=  tensor(0.0005)\n",
      "epcho= 2357 loss=  tensor(0.0005)\n",
      "epcho= 2358 loss=  tensor(0.0005)\n",
      "epcho= 2359 loss=  tensor(0.0005)\n",
      "epcho= 2360 loss=  tensor(0.0005)\n",
      "epcho= 2361 loss=  tensor(0.0005)\n",
      "epcho= 2362 loss=  tensor(0.0005)\n",
      "epcho= 2363 loss=  tensor(0.0005)\n",
      "epcho= 2364 loss=  tensor(0.0005)\n",
      "epcho= 2365 loss=  tensor(0.0005)\n",
      "epcho= 2366 loss=  tensor(0.0005)\n",
      "epcho= 2367 loss=  tensor(0.0005)\n",
      "epcho= 2368 loss=  tensor(0.0005)\n",
      "epcho= 2369 loss=  tensor(0.0005)\n",
      "epcho= 2370 loss=  tensor(0.0005)\n",
      "epcho= 2371 loss=  tensor(0.0005)\n",
      "epcho= 2372 loss=  tensor(0.0005)\n",
      "epcho= 2373 loss=  tensor(0.0005)\n",
      "epcho= 2374 loss=  tensor(0.0005)\n",
      "epcho= 2375 loss=  tensor(0.0005)\n",
      "epcho= 2376 loss=  tensor(0.0005)\n",
      "epcho= 2377 loss=  tensor(0.0005)\n",
      "epcho= 2378 loss=  tensor(0.0005)\n",
      "epcho= 2379 loss=  tensor(0.0005)\n",
      "epcho= 2380 loss=  tensor(0.0005)\n",
      "epcho= 2381 loss=  tensor(0.0005)\n",
      "epcho= 2382 loss=  tensor(0.0005)\n",
      "epcho= 2383 loss=  tensor(0.0005)\n",
      "epcho= 2384 loss=  tensor(0.0005)\n",
      "epcho= 2385 loss=  tensor(0.0005)\n",
      "epcho= 2386 loss=  tensor(0.0005)\n",
      "epcho= 2387 loss=  tensor(0.0005)\n",
      "epcho= 2388 loss=  tensor(0.0005)\n",
      "epcho= 2389 loss=  tensor(0.0005)\n",
      "epcho= 2390 loss=  tensor(0.0005)\n",
      "epcho= 2391 loss=  tensor(0.0005)\n",
      "epcho= 2392 loss=  tensor(0.0005)\n",
      "epcho= 2393 loss=  tensor(0.0005)\n",
      "epcho= 2394 loss=  tensor(0.0005)\n",
      "epcho= 2395 loss=  tensor(0.0005)\n",
      "epcho= 2396 loss=  tensor(0.0005)\n",
      "epcho= 2397 loss=  tensor(0.0005)\n",
      "epcho= 2398 loss=  tensor(0.0005)\n",
      "epcho= 2399 loss=  tensor(0.0005)\n",
      "epcho= 2400 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1906, grad_fn=<DivBackward0>)\n",
      "epcho= 2401 loss=  tensor(0.0005)\n",
      "epcho= 2402 loss=  tensor(0.0005)\n",
      "epcho= 2403 loss=  tensor(0.0005)\n",
      "epcho= 2404 loss=  tensor(0.0005)\n",
      "epcho= 2405 loss=  tensor(0.0005)\n",
      "epcho= 2406 loss=  tensor(0.0004)\n",
      "epcho= 2407 loss=  tensor(0.0004)\n",
      "epcho= 2408 loss=  tensor(0.0004)\n",
      "epcho= 2409 loss=  tensor(0.0004)\n",
      "epcho= 2410 loss=  tensor(0.0004)\n",
      "epcho= 2411 loss=  tensor(0.0004)\n",
      "epcho= 2412 loss=  tensor(0.0004)\n",
      "epcho= 2413 loss=  tensor(0.0004)\n",
      "epcho= 2414 loss=  tensor(0.0004)\n",
      "epcho= 2415 loss=  tensor(0.0004)\n",
      "epcho= 2416 loss=  tensor(0.0004)\n",
      "epcho= 2417 loss=  tensor(0.0004)\n",
      "epcho= 2418 loss=  tensor(0.0004)\n",
      "epcho= 2419 loss=  tensor(0.0004)\n",
      "epcho= 2420 loss=  tensor(0.0004)\n",
      "epcho= 2421 loss=  tensor(0.0004)\n",
      "epcho= 2422 loss=  tensor(0.0004)\n",
      "epcho= 2423 loss=  tensor(0.0004)\n",
      "epcho= 2424 loss=  tensor(0.0004)\n",
      "epcho= 2425 loss=  tensor(0.0004)\n",
      "epcho= 2426 loss=  tensor(0.0004)\n",
      "epcho= 2427 loss=  tensor(0.0004)\n",
      "epcho= 2428 loss=  tensor(0.0004)\n",
      "epcho= 2429 loss=  tensor(0.0004)\n",
      "epcho= 2430 loss=  tensor(0.0004)\n",
      "epcho= 2431 loss=  tensor(0.0004)\n",
      "epcho= 2432 loss=  tensor(0.0004)\n",
      "epcho= 2433 loss=  tensor(0.0004)\n",
      "epcho= 2434 loss=  tensor(0.0004)\n",
      "epcho= 2435 loss=  tensor(0.0004)\n",
      "epcho= 2436 loss=  tensor(0.0004)\n",
      "epcho= 2437 loss=  tensor(0.0004)\n",
      "epcho= 2438 loss=  tensor(0.0004)\n",
      "epcho= 2439 loss=  tensor(0.0004)\n",
      "epcho= 2440 loss=  tensor(0.0004)\n",
      "epcho= 2441 loss=  tensor(0.0004)\n",
      "epcho= 2442 loss=  tensor(0.0004)\n",
      "epcho= 2443 loss=  tensor(0.0004)\n",
      "epcho= 2444 loss=  tensor(0.0004)\n",
      "epcho= 2445 loss=  tensor(0.0004)\n",
      "epcho= 2446 loss=  tensor(0.0004)\n",
      "epcho= 2447 loss=  tensor(0.0004)\n",
      "epcho= 2448 loss=  tensor(0.0004)\n",
      "epcho= 2449 loss=  tensor(0.0004)\n",
      "epcho= 2450 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1899, grad_fn=<DivBackward0>)\n",
      "epcho= 2451 loss=  tensor(0.0004)\n",
      "epcho= 2452 loss=  tensor(0.0004)\n",
      "epcho= 2453 loss=  tensor(0.0004)\n",
      "epcho= 2454 loss=  tensor(0.0004)\n",
      "epcho= 2455 loss=  tensor(0.0004)\n",
      "epcho= 2456 loss=  tensor(0.0004)\n",
      "epcho= 2457 loss=  tensor(0.0004)\n",
      "epcho= 2458 loss=  tensor(0.0004)\n",
      "epcho= 2459 loss=  tensor(0.0004)\n",
      "epcho= 2460 loss=  tensor(0.0004)\n",
      "epcho= 2461 loss=  tensor(0.0004)\n",
      "epcho= 2462 loss=  tensor(0.0004)\n",
      "epcho= 2463 loss=  tensor(0.0004)\n",
      "epcho= 2464 loss=  tensor(0.0004)\n",
      "epcho= 2465 loss=  tensor(0.0004)\n",
      "epcho= 2466 loss=  tensor(0.0004)\n",
      "epcho= 2467 loss=  tensor(0.0004)\n",
      "epcho= 2468 loss=  tensor(0.0004)\n",
      "epcho= 2469 loss=  tensor(0.0004)\n",
      "epcho= 2470 loss=  tensor(0.0004)\n",
      "epcho= 2471 loss=  tensor(0.0004)\n",
      "epcho= 2472 loss=  tensor(0.0004)\n",
      "epcho= 2473 loss=  tensor(0.0004)\n",
      "epcho= 2474 loss=  tensor(0.0004)\n",
      "epcho= 2475 loss=  tensor(0.0004)\n",
      "epcho= 2476 loss=  tensor(0.0004)\n",
      "epcho= 2477 loss=  tensor(0.0004)\n",
      "epcho= 2478 loss=  tensor(0.0004)\n",
      "epcho= 2479 loss=  tensor(0.0004)\n",
      "epcho= 2480 loss=  tensor(0.0004)\n",
      "epcho= 2481 loss=  tensor(0.0004)\n",
      "epcho= 2482 loss=  tensor(0.0004)\n",
      "epcho= 2483 loss=  tensor(0.0004)\n",
      "epcho= 2484 loss=  tensor(0.0004)\n",
      "epcho= 2485 loss=  tensor(0.0004)\n",
      "epcho= 2486 loss=  tensor(0.0004)\n",
      "epcho= 2487 loss=  tensor(0.0004)\n",
      "epcho= 2488 loss=  tensor(0.0004)\n",
      "epcho= 2489 loss=  tensor(0.0004)\n",
      "epcho= 2490 loss=  tensor(0.0004)\n",
      "epcho= 2491 loss=  tensor(0.0004)\n",
      "epcho= 2492 loss=  tensor(0.0004)\n",
      "epcho= 2493 loss=  tensor(0.0004)\n",
      "epcho= 2494 loss=  tensor(0.0004)\n",
      "epcho= 2495 loss=  tensor(0.0004)\n",
      "epcho= 2496 loss=  tensor(0.0004)\n",
      "epcho= 2497 loss=  tensor(0.0004)\n",
      "epcho= 2498 loss=  tensor(0.0004)\n",
      "epcho= 2499 loss=  tensor(0.0004)\n",
      "epcho= 2500 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1893, grad_fn=<DivBackward0>)\n",
      "epcho= 2501 loss=  tensor(0.0004)\n",
      "epcho= 2502 loss=  tensor(0.0004)\n",
      "epcho= 2503 loss=  tensor(0.0004)\n",
      "epcho= 2504 loss=  tensor(0.0004)\n",
      "epcho= 2505 loss=  tensor(0.0004)\n",
      "epcho= 2506 loss=  tensor(0.0004)\n",
      "epcho= 2507 loss=  tensor(0.0004)\n",
      "epcho= 2508 loss=  tensor(0.0004)\n",
      "epcho= 2509 loss=  tensor(0.0004)\n",
      "epcho= 2510 loss=  tensor(0.0004)\n",
      "epcho= 2511 loss=  tensor(0.0004)\n",
      "epcho= 2512 loss=  tensor(0.0004)\n",
      "epcho= 2513 loss=  tensor(0.0004)\n",
      "epcho= 2514 loss=  tensor(0.0004)\n",
      "epcho= 2515 loss=  tensor(0.0004)\n",
      "epcho= 2516 loss=  tensor(0.0004)\n",
      "epcho= 2517 loss=  tensor(0.0004)\n",
      "epcho= 2518 loss=  tensor(0.0004)\n",
      "epcho= 2519 loss=  tensor(0.0004)\n",
      "epcho= 2520 loss=  tensor(0.0004)\n",
      "epcho= 2521 loss=  tensor(0.0004)\n",
      "epcho= 2522 loss=  tensor(0.0004)\n",
      "epcho= 2523 loss=  tensor(0.0004)\n",
      "epcho= 2524 loss=  tensor(0.0004)\n",
      "epcho= 2525 loss=  tensor(0.0004)\n",
      "epcho= 2526 loss=  tensor(0.0004)\n",
      "epcho= 2527 loss=  tensor(0.0004)\n",
      "epcho= 2528 loss=  tensor(0.0004)\n",
      "epcho= 2529 loss=  tensor(0.0004)\n",
      "epcho= 2530 loss=  tensor(0.0004)\n",
      "epcho= 2531 loss=  tensor(0.0004)\n",
      "epcho= 2532 loss=  tensor(0.0005)\n",
      "epcho= 2533 loss=  tensor(0.0005)\n",
      "epcho= 2534 loss=  tensor(0.0005)\n",
      "epcho= 2535 loss=  tensor(0.0006)\n",
      "epcho= 2536 loss=  tensor(0.0007)\n",
      "epcho= 2537 loss=  tensor(0.0009)\n",
      "epcho= 2538 loss=  tensor(0.0012)\n",
      "epcho= 2539 loss=  tensor(0.0018)\n",
      "epcho= 2540 loss=  tensor(0.0028)\n",
      "epcho= 2541 loss=  tensor(0.0043)\n",
      "epcho= 2542 loss=  tensor(0.0064)\n",
      "epcho= 2543 loss=  tensor(0.0089)\n",
      "epcho= 2544 loss=  tensor(0.0105)\n",
      "epcho= 2545 loss=  tensor(0.0102)\n",
      "epcho= 2546 loss=  tensor(0.0067)\n",
      "epcho= 2547 loss=  tensor(0.0025)\n",
      "epcho= 2548 loss=  tensor(0.0006)\n",
      "epcho= 2549 loss=  tensor(0.0019)\n",
      "epcho= 2550 loss=  tensor(0.0041)\n",
      "tensor(0.0041, grad_fn=<AddBackward0>) tensor(0.2353, grad_fn=<DivBackward0>)\n",
      "epcho= 2551 loss=  tensor(0.0039)\n",
      "epcho= 2552 loss=  tensor(0.0017)\n",
      "epcho= 2553 loss=  tensor(0.0007)\n",
      "epcho= 2554 loss=  tensor(0.0018)\n",
      "epcho= 2555 loss=  tensor(0.0026)\n",
      "epcho= 2556 loss=  tensor(0.0017)\n",
      "epcho= 2557 loss=  tensor(0.0008)\n",
      "epcho= 2558 loss=  tensor(0.0011)\n",
      "epcho= 2559 loss=  tensor(0.0016)\n",
      "epcho= 2560 loss=  tensor(0.0013)\n",
      "epcho= 2561 loss=  tensor(0.0008)\n",
      "epcho= 2562 loss=  tensor(0.0009)\n",
      "epcho= 2563 loss=  tensor(0.0011)\n",
      "epcho= 2564 loss=  tensor(0.0009)\n",
      "epcho= 2565 loss=  tensor(0.0007)\n",
      "epcho= 2566 loss=  tensor(0.0008)\n",
      "epcho= 2567 loss=  tensor(0.0009)\n",
      "epcho= 2568 loss=  tensor(0.0007)\n",
      "epcho= 2569 loss=  tensor(0.0006)\n",
      "epcho= 2570 loss=  tensor(0.0007)\n",
      "epcho= 2571 loss=  tensor(0.0008)\n",
      "epcho= 2572 loss=  tensor(0.0006)\n",
      "epcho= 2573 loss=  tensor(0.0005)\n",
      "epcho= 2574 loss=  tensor(0.0006)\n",
      "epcho= 2575 loss=  tensor(0.0007)\n",
      "epcho= 2576 loss=  tensor(0.0005)\n",
      "epcho= 2577 loss=  tensor(0.0004)\n",
      "epcho= 2578 loss=  tensor(0.0006)\n",
      "epcho= 2579 loss=  tensor(0.0006)\n",
      "epcho= 2580 loss=  tensor(0.0005)\n",
      "epcho= 2581 loss=  tensor(0.0004)\n",
      "epcho= 2582 loss=  tensor(0.0005)\n",
      "epcho= 2583 loss=  tensor(0.0006)\n",
      "epcho= 2584 loss=  tensor(0.0005)\n",
      "epcho= 2585 loss=  tensor(0.0004)\n",
      "epcho= 2586 loss=  tensor(0.0005)\n",
      "epcho= 2587 loss=  tensor(0.0005)\n",
      "epcho= 2588 loss=  tensor(0.0005)\n",
      "epcho= 2589 loss=  tensor(0.0004)\n",
      "epcho= 2590 loss=  tensor(0.0005)\n",
      "epcho= 2591 loss=  tensor(0.0005)\n",
      "epcho= 2592 loss=  tensor(0.0005)\n",
      "epcho= 2593 loss=  tensor(0.0004)\n",
      "epcho= 2594 loss=  tensor(0.0004)\n",
      "epcho= 2595 loss=  tensor(0.0005)\n",
      "epcho= 2596 loss=  tensor(0.0004)\n",
      "epcho= 2597 loss=  tensor(0.0004)\n",
      "epcho= 2598 loss=  tensor(0.0004)\n",
      "epcho= 2599 loss=  tensor(0.0004)\n",
      "epcho= 2600 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1922, grad_fn=<DivBackward0>)\n",
      "epcho= 2601 loss=  tensor(0.0004)\n",
      "epcho= 2602 loss=  tensor(0.0004)\n",
      "epcho= 2603 loss=  tensor(0.0004)\n",
      "epcho= 2604 loss=  tensor(0.0004)\n",
      "epcho= 2605 loss=  tensor(0.0004)\n",
      "epcho= 2606 loss=  tensor(0.0004)\n",
      "epcho= 2607 loss=  tensor(0.0004)\n",
      "epcho= 2608 loss=  tensor(0.0004)\n",
      "epcho= 2609 loss=  tensor(0.0004)\n",
      "epcho= 2610 loss=  tensor(0.0004)\n",
      "epcho= 2611 loss=  tensor(0.0004)\n",
      "epcho= 2612 loss=  tensor(0.0004)\n",
      "epcho= 2613 loss=  tensor(0.0004)\n",
      "epcho= 2614 loss=  tensor(0.0004)\n",
      "epcho= 2615 loss=  tensor(0.0004)\n",
      "epcho= 2616 loss=  tensor(0.0004)\n",
      "epcho= 2617 loss=  tensor(0.0004)\n",
      "epcho= 2618 loss=  tensor(0.0004)\n",
      "epcho= 2619 loss=  tensor(0.0004)\n",
      "epcho= 2620 loss=  tensor(0.0004)\n",
      "epcho= 2621 loss=  tensor(0.0004)\n",
      "epcho= 2622 loss=  tensor(0.0004)\n",
      "epcho= 2623 loss=  tensor(0.0004)\n",
      "epcho= 2624 loss=  tensor(0.0004)\n",
      "epcho= 2625 loss=  tensor(0.0004)\n",
      "epcho= 2626 loss=  tensor(0.0004)\n",
      "epcho= 2627 loss=  tensor(0.0004)\n",
      "epcho= 2628 loss=  tensor(0.0004)\n",
      "epcho= 2629 loss=  tensor(0.0004)\n",
      "epcho= 2630 loss=  tensor(0.0004)\n",
      "epcho= 2631 loss=  tensor(0.0004)\n",
      "epcho= 2632 loss=  tensor(0.0004)\n",
      "epcho= 2633 loss=  tensor(0.0004)\n",
      "epcho= 2634 loss=  tensor(0.0004)\n",
      "epcho= 2635 loss=  tensor(0.0004)\n",
      "epcho= 2636 loss=  tensor(0.0004)\n",
      "epcho= 2637 loss=  tensor(0.0004)\n",
      "epcho= 2638 loss=  tensor(0.0004)\n",
      "epcho= 2639 loss=  tensor(0.0004)\n",
      "epcho= 2640 loss=  tensor(0.0004)\n",
      "epcho= 2641 loss=  tensor(0.0004)\n",
      "epcho= 2642 loss=  tensor(0.0004)\n",
      "epcho= 2643 loss=  tensor(0.0004)\n",
      "epcho= 2644 loss=  tensor(0.0004)\n",
      "epcho= 2645 loss=  tensor(0.0004)\n",
      "epcho= 2646 loss=  tensor(0.0004)\n",
      "epcho= 2647 loss=  tensor(0.0004)\n",
      "epcho= 2648 loss=  tensor(0.0004)\n",
      "epcho= 2649 loss=  tensor(0.0004)\n",
      "epcho= 2650 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1916, grad_fn=<DivBackward0>)\n",
      "epcho= 2651 loss=  tensor(0.0004)\n",
      "epcho= 2652 loss=  tensor(0.0004)\n",
      "epcho= 2653 loss=  tensor(0.0004)\n",
      "epcho= 2654 loss=  tensor(0.0004)\n",
      "epcho= 2655 loss=  tensor(0.0004)\n",
      "epcho= 2656 loss=  tensor(0.0004)\n",
      "epcho= 2657 loss=  tensor(0.0004)\n",
      "epcho= 2658 loss=  tensor(0.0004)\n",
      "epcho= 2659 loss=  tensor(0.0004)\n",
      "epcho= 2660 loss=  tensor(0.0004)\n",
      "epcho= 2661 loss=  tensor(0.0004)\n",
      "epcho= 2662 loss=  tensor(0.0004)\n",
      "epcho= 2663 loss=  tensor(0.0004)\n",
      "epcho= 2664 loss=  tensor(0.0004)\n",
      "epcho= 2665 loss=  tensor(0.0004)\n",
      "epcho= 2666 loss=  tensor(0.0004)\n",
      "epcho= 2667 loss=  tensor(0.0004)\n",
      "epcho= 2668 loss=  tensor(0.0004)\n",
      "epcho= 2669 loss=  tensor(0.0004)\n",
      "epcho= 2670 loss=  tensor(0.0004)\n",
      "epcho= 2671 loss=  tensor(0.0004)\n",
      "epcho= 2672 loss=  tensor(0.0004)\n",
      "epcho= 2673 loss=  tensor(0.0004)\n",
      "epcho= 2674 loss=  tensor(0.0004)\n",
      "epcho= 2675 loss=  tensor(0.0004)\n",
      "epcho= 2676 loss=  tensor(0.0004)\n",
      "epcho= 2677 loss=  tensor(0.0004)\n",
      "epcho= 2678 loss=  tensor(0.0004)\n",
      "epcho= 2679 loss=  tensor(0.0004)\n",
      "epcho= 2680 loss=  tensor(0.0004)\n",
      "epcho= 2681 loss=  tensor(0.0004)\n",
      "epcho= 2682 loss=  tensor(0.0004)\n",
      "epcho= 2683 loss=  tensor(0.0004)\n",
      "epcho= 2684 loss=  tensor(0.0004)\n",
      "epcho= 2685 loss=  tensor(0.0004)\n",
      "epcho= 2686 loss=  tensor(0.0004)\n",
      "epcho= 2687 loss=  tensor(0.0004)\n",
      "epcho= 2688 loss=  tensor(0.0004)\n",
      "epcho= 2689 loss=  tensor(0.0004)\n",
      "epcho= 2690 loss=  tensor(0.0004)\n",
      "epcho= 2691 loss=  tensor(0.0004)\n",
      "epcho= 2692 loss=  tensor(0.0004)\n",
      "epcho= 2693 loss=  tensor(0.0004)\n",
      "epcho= 2694 loss=  tensor(0.0004)\n",
      "epcho= 2695 loss=  tensor(0.0004)\n",
      "epcho= 2696 loss=  tensor(0.0004)\n",
      "epcho= 2697 loss=  tensor(0.0004)\n",
      "epcho= 2698 loss=  tensor(0.0004)\n",
      "epcho= 2699 loss=  tensor(0.0004)\n",
      "epcho= 2700 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1905, grad_fn=<DivBackward0>)\n",
      "epcho= 2701 loss=  tensor(0.0004)\n",
      "epcho= 2702 loss=  tensor(0.0004)\n",
      "epcho= 2703 loss=  tensor(0.0004)\n",
      "epcho= 2704 loss=  tensor(0.0004)\n",
      "epcho= 2705 loss=  tensor(0.0004)\n",
      "epcho= 2706 loss=  tensor(0.0004)\n",
      "epcho= 2707 loss=  tensor(0.0004)\n",
      "epcho= 2708 loss=  tensor(0.0004)\n",
      "epcho= 2709 loss=  tensor(0.0004)\n",
      "epcho= 2710 loss=  tensor(0.0004)\n",
      "epcho= 2711 loss=  tensor(0.0004)\n",
      "epcho= 2712 loss=  tensor(0.0004)\n",
      "epcho= 2713 loss=  tensor(0.0004)\n",
      "epcho= 2714 loss=  tensor(0.0004)\n",
      "epcho= 2715 loss=  tensor(0.0004)\n",
      "epcho= 2716 loss=  tensor(0.0004)\n",
      "epcho= 2717 loss=  tensor(0.0004)\n",
      "epcho= 2718 loss=  tensor(0.0004)\n",
      "epcho= 2719 loss=  tensor(0.0004)\n",
      "epcho= 2720 loss=  tensor(0.0004)\n",
      "epcho= 2721 loss=  tensor(0.0004)\n",
      "epcho= 2722 loss=  tensor(0.0004)\n",
      "epcho= 2723 loss=  tensor(0.0004)\n",
      "epcho= 2724 loss=  tensor(0.0004)\n",
      "epcho= 2725 loss=  tensor(0.0004)\n",
      "epcho= 2726 loss=  tensor(0.0004)\n",
      "epcho= 2727 loss=  tensor(0.0004)\n",
      "epcho= 2728 loss=  tensor(0.0004)\n",
      "epcho= 2729 loss=  tensor(0.0004)\n",
      "epcho= 2730 loss=  tensor(0.0004)\n",
      "epcho= 2731 loss=  tensor(0.0004)\n",
      "epcho= 2732 loss=  tensor(0.0004)\n",
      "epcho= 2733 loss=  tensor(0.0004)\n",
      "epcho= 2734 loss=  tensor(0.0004)\n",
      "epcho= 2735 loss=  tensor(0.0004)\n",
      "epcho= 2736 loss=  tensor(0.0004)\n",
      "epcho= 2737 loss=  tensor(0.0004)\n",
      "epcho= 2738 loss=  tensor(0.0004)\n",
      "epcho= 2739 loss=  tensor(0.0004)\n",
      "epcho= 2740 loss=  tensor(0.0004)\n",
      "epcho= 2741 loss=  tensor(0.0004)\n",
      "epcho= 2742 loss=  tensor(0.0004)\n",
      "epcho= 2743 loss=  tensor(0.0004)\n",
      "epcho= 2744 loss=  tensor(0.0004)\n",
      "epcho= 2745 loss=  tensor(0.0004)\n",
      "epcho= 2746 loss=  tensor(0.0004)\n",
      "epcho= 2747 loss=  tensor(0.0004)\n",
      "epcho= 2748 loss=  tensor(0.0004)\n",
      "epcho= 2749 loss=  tensor(0.0004)\n",
      "epcho= 2750 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1894, grad_fn=<DivBackward0>)\n",
      "epcho= 2751 loss=  tensor(0.0004)\n",
      "epcho= 2752 loss=  tensor(0.0004)\n",
      "epcho= 2753 loss=  tensor(0.0004)\n",
      "epcho= 2754 loss=  tensor(0.0004)\n",
      "epcho= 2755 loss=  tensor(0.0004)\n",
      "epcho= 2756 loss=  tensor(0.0004)\n",
      "epcho= 2757 loss=  tensor(0.0004)\n",
      "epcho= 2758 loss=  tensor(0.0004)\n",
      "epcho= 2759 loss=  tensor(0.0004)\n",
      "epcho= 2760 loss=  tensor(0.0004)\n",
      "epcho= 2761 loss=  tensor(0.0004)\n",
      "epcho= 2762 loss=  tensor(0.0004)\n",
      "epcho= 2763 loss=  tensor(0.0004)\n",
      "epcho= 2764 loss=  tensor(0.0004)\n",
      "epcho= 2765 loss=  tensor(0.0004)\n",
      "epcho= 2766 loss=  tensor(0.0004)\n",
      "epcho= 2767 loss=  tensor(0.0004)\n",
      "epcho= 2768 loss=  tensor(0.0004)\n",
      "epcho= 2769 loss=  tensor(0.0004)\n",
      "epcho= 2770 loss=  tensor(0.0004)\n",
      "epcho= 2771 loss=  tensor(0.0004)\n",
      "epcho= 2772 loss=  tensor(0.0004)\n",
      "epcho= 2773 loss=  tensor(0.0004)\n",
      "epcho= 2774 loss=  tensor(0.0004)\n",
      "epcho= 2775 loss=  tensor(0.0004)\n",
      "epcho= 2776 loss=  tensor(0.0004)\n",
      "epcho= 2777 loss=  tensor(0.0004)\n",
      "epcho= 2778 loss=  tensor(0.0004)\n",
      "epcho= 2779 loss=  tensor(0.0004)\n",
      "epcho= 2780 loss=  tensor(0.0004)\n",
      "epcho= 2781 loss=  tensor(0.0004)\n",
      "epcho= 2782 loss=  tensor(0.0004)\n",
      "epcho= 2783 loss=  tensor(0.0004)\n",
      "epcho= 2784 loss=  tensor(0.0004)\n",
      "epcho= 2785 loss=  tensor(0.0004)\n",
      "epcho= 2786 loss=  tensor(0.0004)\n",
      "epcho= 2787 loss=  tensor(0.0004)\n",
      "epcho= 2788 loss=  tensor(0.0004)\n",
      "epcho= 2789 loss=  tensor(0.0004)\n",
      "epcho= 2790 loss=  tensor(0.0004)\n",
      "epcho= 2791 loss=  tensor(0.0004)\n",
      "epcho= 2792 loss=  tensor(0.0004)\n",
      "epcho= 2793 loss=  tensor(0.0004)\n",
      "epcho= 2794 loss=  tensor(0.0004)\n",
      "epcho= 2795 loss=  tensor(0.0004)\n",
      "epcho= 2796 loss=  tensor(0.0004)\n",
      "epcho= 2797 loss=  tensor(0.0004)\n",
      "epcho= 2798 loss=  tensor(0.0004)\n",
      "epcho= 2799 loss=  tensor(0.0004)\n",
      "epcho= 2800 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1884, grad_fn=<DivBackward0>)\n",
      "epcho= 2801 loss=  tensor(0.0004)\n",
      "epcho= 2802 loss=  tensor(0.0004)\n",
      "epcho= 2803 loss=  tensor(0.0004)\n",
      "epcho= 2804 loss=  tensor(0.0004)\n",
      "epcho= 2805 loss=  tensor(0.0004)\n",
      "epcho= 2806 loss=  tensor(0.0004)\n",
      "epcho= 2807 loss=  tensor(0.0004)\n",
      "epcho= 2808 loss=  tensor(0.0004)\n",
      "epcho= 2809 loss=  tensor(0.0004)\n",
      "epcho= 2810 loss=  tensor(0.0004)\n",
      "epcho= 2811 loss=  tensor(0.0004)\n",
      "epcho= 2812 loss=  tensor(0.0004)\n",
      "epcho= 2813 loss=  tensor(0.0004)\n",
      "epcho= 2814 loss=  tensor(0.0004)\n",
      "epcho= 2815 loss=  tensor(0.0004)\n",
      "epcho= 2816 loss=  tensor(0.0004)\n",
      "epcho= 2817 loss=  tensor(0.0004)\n",
      "epcho= 2818 loss=  tensor(0.0004)\n",
      "epcho= 2819 loss=  tensor(0.0004)\n",
      "epcho= 2820 loss=  tensor(0.0004)\n",
      "epcho= 2821 loss=  tensor(0.0004)\n",
      "epcho= 2822 loss=  tensor(0.0004)\n",
      "epcho= 2823 loss=  tensor(0.0004)\n",
      "epcho= 2824 loss=  tensor(0.0004)\n",
      "epcho= 2825 loss=  tensor(0.0004)\n",
      "epcho= 2826 loss=  tensor(0.0004)\n",
      "epcho= 2827 loss=  tensor(0.0004)\n",
      "epcho= 2828 loss=  tensor(0.0004)\n",
      "epcho= 2829 loss=  tensor(0.0004)\n",
      "epcho= 2830 loss=  tensor(0.0004)\n",
      "epcho= 2831 loss=  tensor(0.0004)\n",
      "epcho= 2832 loss=  tensor(0.0004)\n",
      "epcho= 2833 loss=  tensor(0.0004)\n",
      "epcho= 2834 loss=  tensor(0.0004)\n",
      "epcho= 2835 loss=  tensor(0.0004)\n",
      "epcho= 2836 loss=  tensor(0.0004)\n",
      "epcho= 2837 loss=  tensor(0.0004)\n",
      "epcho= 2838 loss=  tensor(0.0004)\n",
      "epcho= 2839 loss=  tensor(0.0004)\n",
      "epcho= 2840 loss=  tensor(0.0004)\n",
      "epcho= 2841 loss=  tensor(0.0004)\n",
      "epcho= 2842 loss=  tensor(0.0004)\n",
      "epcho= 2843 loss=  tensor(0.0004)\n",
      "epcho= 2844 loss=  tensor(0.0004)\n",
      "epcho= 2845 loss=  tensor(0.0004)\n",
      "epcho= 2846 loss=  tensor(0.0004)\n",
      "epcho= 2847 loss=  tensor(0.0004)\n",
      "epcho= 2848 loss=  tensor(0.0004)\n",
      "epcho= 2849 loss=  tensor(0.0004)\n",
      "epcho= 2850 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1875, grad_fn=<DivBackward0>)\n",
      "epcho= 2851 loss=  tensor(0.0004)\n",
      "epcho= 2852 loss=  tensor(0.0004)\n",
      "epcho= 2853 loss=  tensor(0.0004)\n",
      "epcho= 2854 loss=  tensor(0.0004)\n",
      "epcho= 2855 loss=  tensor(0.0004)\n",
      "epcho= 2856 loss=  tensor(0.0004)\n",
      "epcho= 2857 loss=  tensor(0.0004)\n",
      "epcho= 2858 loss=  tensor(0.0004)\n",
      "epcho= 2859 loss=  tensor(0.0004)\n",
      "epcho= 2860 loss=  tensor(0.0004)\n",
      "epcho= 2861 loss=  tensor(0.0004)\n",
      "epcho= 2862 loss=  tensor(0.0004)\n",
      "epcho= 2863 loss=  tensor(0.0004)\n",
      "epcho= 2864 loss=  tensor(0.0004)\n",
      "epcho= 2865 loss=  tensor(0.0004)\n",
      "epcho= 2866 loss=  tensor(0.0004)\n",
      "epcho= 2867 loss=  tensor(0.0004)\n",
      "epcho= 2868 loss=  tensor(0.0004)\n",
      "epcho= 2869 loss=  tensor(0.0004)\n",
      "epcho= 2870 loss=  tensor(0.0004)\n",
      "epcho= 2871 loss=  tensor(0.0004)\n",
      "epcho= 2872 loss=  tensor(0.0004)\n",
      "epcho= 2873 loss=  tensor(0.0004)\n",
      "epcho= 2874 loss=  tensor(0.0004)\n",
      "epcho= 2875 loss=  tensor(0.0004)\n",
      "epcho= 2876 loss=  tensor(0.0004)\n",
      "epcho= 2877 loss=  tensor(0.0004)\n",
      "epcho= 2878 loss=  tensor(0.0004)\n",
      "epcho= 2879 loss=  tensor(0.0004)\n",
      "epcho= 2880 loss=  tensor(0.0004)\n",
      "epcho= 2881 loss=  tensor(0.0004)\n",
      "epcho= 2882 loss=  tensor(0.0004)\n",
      "epcho= 2883 loss=  tensor(0.0004)\n",
      "epcho= 2884 loss=  tensor(0.0004)\n",
      "epcho= 2885 loss=  tensor(0.0004)\n",
      "epcho= 2886 loss=  tensor(0.0004)\n",
      "epcho= 2887 loss=  tensor(0.0004)\n",
      "epcho= 2888 loss=  tensor(0.0004)\n",
      "epcho= 2889 loss=  tensor(0.0004)\n",
      "epcho= 2890 loss=  tensor(0.0004)\n",
      "epcho= 2891 loss=  tensor(0.0004)\n",
      "epcho= 2892 loss=  tensor(0.0004)\n",
      "epcho= 2893 loss=  tensor(0.0004)\n",
      "epcho= 2894 loss=  tensor(0.0004)\n",
      "epcho= 2895 loss=  tensor(0.0004)\n",
      "epcho= 2896 loss=  tensor(0.0004)\n",
      "epcho= 2897 loss=  tensor(0.0004)\n",
      "epcho= 2898 loss=  tensor(0.0004)\n",
      "epcho= 2899 loss=  tensor(0.0004)\n",
      "epcho= 2900 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1866, grad_fn=<DivBackward0>)\n",
      "epcho= 2901 loss=  tensor(0.0004)\n",
      "epcho= 2902 loss=  tensor(0.0004)\n",
      "epcho= 2903 loss=  tensor(0.0004)\n",
      "epcho= 2904 loss=  tensor(0.0004)\n",
      "epcho= 2905 loss=  tensor(0.0004)\n",
      "epcho= 2906 loss=  tensor(0.0004)\n",
      "epcho= 2907 loss=  tensor(0.0004)\n",
      "epcho= 2908 loss=  tensor(0.0004)\n",
      "epcho= 2909 loss=  tensor(0.0004)\n",
      "epcho= 2910 loss=  tensor(0.0004)\n",
      "epcho= 2911 loss=  tensor(0.0004)\n",
      "epcho= 2912 loss=  tensor(0.0004)\n",
      "epcho= 2913 loss=  tensor(0.0004)\n",
      "epcho= 2914 loss=  tensor(0.0004)\n",
      "epcho= 2915 loss=  tensor(0.0004)\n",
      "epcho= 2916 loss=  tensor(0.0004)\n",
      "epcho= 2917 loss=  tensor(0.0004)\n",
      "epcho= 2918 loss=  tensor(0.0004)\n",
      "epcho= 2919 loss=  tensor(0.0004)\n",
      "epcho= 2920 loss=  tensor(0.0004)\n",
      "epcho= 2921 loss=  tensor(0.0004)\n",
      "epcho= 2922 loss=  tensor(0.0004)\n",
      "epcho= 2923 loss=  tensor(0.0004)\n",
      "epcho= 2924 loss=  tensor(0.0004)\n",
      "epcho= 2925 loss=  tensor(0.0004)\n",
      "epcho= 2926 loss=  tensor(0.0004)\n",
      "epcho= 2927 loss=  tensor(0.0004)\n",
      "epcho= 2928 loss=  tensor(0.0004)\n",
      "epcho= 2929 loss=  tensor(0.0004)\n",
      "epcho= 2930 loss=  tensor(0.0004)\n",
      "epcho= 2931 loss=  tensor(0.0004)\n",
      "epcho= 2932 loss=  tensor(0.0004)\n",
      "epcho= 2933 loss=  tensor(0.0004)\n",
      "epcho= 2934 loss=  tensor(0.0004)\n",
      "epcho= 2935 loss=  tensor(0.0004)\n",
      "epcho= 2936 loss=  tensor(0.0004)\n",
      "epcho= 2937 loss=  tensor(0.0004)\n",
      "epcho= 2938 loss=  tensor(0.0004)\n",
      "epcho= 2939 loss=  tensor(0.0004)\n",
      "epcho= 2940 loss=  tensor(0.0004)\n",
      "epcho= 2941 loss=  tensor(0.0004)\n",
      "epcho= 2942 loss=  tensor(0.0004)\n",
      "epcho= 2943 loss=  tensor(0.0004)\n",
      "epcho= 2944 loss=  tensor(0.0004)\n",
      "epcho= 2945 loss=  tensor(0.0004)\n",
      "epcho= 2946 loss=  tensor(0.0004)\n",
      "epcho= 2947 loss=  tensor(0.0004)\n",
      "epcho= 2948 loss=  tensor(0.0004)\n",
      "epcho= 2949 loss=  tensor(0.0004)\n",
      "epcho= 2950 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1857, grad_fn=<DivBackward0>)\n",
      "epcho= 2951 loss=  tensor(0.0004)\n",
      "epcho= 2952 loss=  tensor(0.0004)\n",
      "epcho= 2953 loss=  tensor(0.0004)\n",
      "epcho= 2954 loss=  tensor(0.0004)\n",
      "epcho= 2955 loss=  tensor(0.0004)\n",
      "epcho= 2956 loss=  tensor(0.0004)\n",
      "epcho= 2957 loss=  tensor(0.0004)\n",
      "epcho= 2958 loss=  tensor(0.0004)\n",
      "epcho= 2959 loss=  tensor(0.0004)\n",
      "epcho= 2960 loss=  tensor(0.0004)\n",
      "epcho= 2961 loss=  tensor(0.0004)\n",
      "epcho= 2962 loss=  tensor(0.0004)\n",
      "epcho= 2963 loss=  tensor(0.0004)\n",
      "epcho= 2964 loss=  tensor(0.0004)\n",
      "epcho= 2965 loss=  tensor(0.0004)\n",
      "epcho= 2966 loss=  tensor(0.0004)\n",
      "epcho= 2967 loss=  tensor(0.0004)\n",
      "epcho= 2968 loss=  tensor(0.0004)\n",
      "epcho= 2969 loss=  tensor(0.0004)\n",
      "epcho= 2970 loss=  tensor(0.0004)\n",
      "epcho= 2971 loss=  tensor(0.0004)\n",
      "epcho= 2972 loss=  tensor(0.0004)\n",
      "epcho= 2973 loss=  tensor(0.0004)\n",
      "epcho= 2974 loss=  tensor(0.0004)\n",
      "epcho= 2975 loss=  tensor(0.0004)\n",
      "epcho= 2976 loss=  tensor(0.0004)\n",
      "epcho= 2977 loss=  tensor(0.0004)\n",
      "epcho= 2978 loss=  tensor(0.0004)\n",
      "epcho= 2979 loss=  tensor(0.0004)\n",
      "epcho= 2980 loss=  tensor(0.0004)\n",
      "epcho= 2981 loss=  tensor(0.0004)\n",
      "epcho= 2982 loss=  tensor(0.0004)\n",
      "epcho= 2983 loss=  tensor(0.0004)\n",
      "epcho= 2984 loss=  tensor(0.0004)\n",
      "epcho= 2985 loss=  tensor(0.0004)\n",
      "epcho= 2986 loss=  tensor(0.0004)\n",
      "epcho= 2987 loss=  tensor(0.0004)\n",
      "epcho= 2988 loss=  tensor(0.0004)\n",
      "epcho= 2989 loss=  tensor(0.0004)\n",
      "epcho= 2990 loss=  tensor(0.0004)\n",
      "epcho= 2991 loss=  tensor(0.0004)\n",
      "epcho= 2992 loss=  tensor(0.0004)\n",
      "epcho= 2993 loss=  tensor(0.0004)\n",
      "epcho= 2994 loss=  tensor(0.0004)\n",
      "epcho= 2995 loss=  tensor(0.0004)\n",
      "epcho= 2996 loss=  tensor(0.0004)\n",
      "epcho= 2997 loss=  tensor(0.0004)\n",
      "epcho= 2998 loss=  tensor(0.0004)\n",
      "epcho= 2999 loss=  tensor(0.0004)\n",
      "epcho= 3000 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1848, grad_fn=<DivBackward0>)\n",
      "epcho= 3001 loss=  tensor(0.0004)\n",
      "epcho= 3002 loss=  tensor(0.0004)\n",
      "epcho= 3003 loss=  tensor(0.0004)\n",
      "epcho= 3004 loss=  tensor(0.0004)\n",
      "epcho= 3005 loss=  tensor(0.0004)\n",
      "epcho= 3006 loss=  tensor(0.0004)\n",
      "epcho= 3007 loss=  tensor(0.0004)\n",
      "epcho= 3008 loss=  tensor(0.0004)\n",
      "epcho= 3009 loss=  tensor(0.0004)\n",
      "epcho= 3010 loss=  tensor(0.0004)\n",
      "epcho= 3011 loss=  tensor(0.0004)\n",
      "epcho= 3012 loss=  tensor(0.0004)\n",
      "epcho= 3013 loss=  tensor(0.0004)\n",
      "epcho= 3014 loss=  tensor(0.0004)\n",
      "epcho= 3015 loss=  tensor(0.0004)\n",
      "epcho= 3016 loss=  tensor(0.0004)\n",
      "epcho= 3017 loss=  tensor(0.0004)\n",
      "epcho= 3018 loss=  tensor(0.0004)\n",
      "epcho= 3019 loss=  tensor(0.0004)\n",
      "epcho= 3020 loss=  tensor(0.0004)\n",
      "epcho= 3021 loss=  tensor(0.0004)\n",
      "epcho= 3022 loss=  tensor(0.0004)\n",
      "epcho= 3023 loss=  tensor(0.0004)\n",
      "epcho= 3024 loss=  tensor(0.0004)\n",
      "epcho= 3025 loss=  tensor(0.0004)\n",
      "epcho= 3026 loss=  tensor(0.0004)\n",
      "epcho= 3027 loss=  tensor(0.0004)\n",
      "epcho= 3028 loss=  tensor(0.0004)\n",
      "epcho= 3029 loss=  tensor(0.0004)\n",
      "epcho= 3030 loss=  tensor(0.0004)\n",
      "epcho= 3031 loss=  tensor(0.0004)\n",
      "epcho= 3032 loss=  tensor(0.0004)\n",
      "epcho= 3033 loss=  tensor(0.0004)\n",
      "epcho= 3034 loss=  tensor(0.0004)\n",
      "epcho= 3035 loss=  tensor(0.0004)\n",
      "epcho= 3036 loss=  tensor(0.0004)\n",
      "epcho= 3037 loss=  tensor(0.0004)\n",
      "epcho= 3038 loss=  tensor(0.0004)\n",
      "epcho= 3039 loss=  tensor(0.0004)\n",
      "epcho= 3040 loss=  tensor(0.0004)\n",
      "epcho= 3041 loss=  tensor(0.0004)\n",
      "epcho= 3042 loss=  tensor(0.0004)\n",
      "epcho= 3043 loss=  tensor(0.0004)\n",
      "epcho= 3044 loss=  tensor(0.0004)\n",
      "epcho= 3045 loss=  tensor(0.0004)\n",
      "epcho= 3046 loss=  tensor(0.0004)\n",
      "epcho= 3047 loss=  tensor(0.0004)\n",
      "epcho= 3048 loss=  tensor(0.0004)\n",
      "epcho= 3049 loss=  tensor(0.0004)\n",
      "epcho= 3050 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1839, grad_fn=<DivBackward0>)\n",
      "epcho= 3051 loss=  tensor(0.0004)\n",
      "epcho= 3052 loss=  tensor(0.0004)\n",
      "epcho= 3053 loss=  tensor(0.0004)\n",
      "epcho= 3054 loss=  tensor(0.0004)\n",
      "epcho= 3055 loss=  tensor(0.0004)\n",
      "epcho= 3056 loss=  tensor(0.0004)\n",
      "epcho= 3057 loss=  tensor(0.0004)\n",
      "epcho= 3058 loss=  tensor(0.0004)\n",
      "epcho= 3059 loss=  tensor(0.0004)\n",
      "epcho= 3060 loss=  tensor(0.0004)\n",
      "epcho= 3061 loss=  tensor(0.0004)\n",
      "epcho= 3062 loss=  tensor(0.0004)\n",
      "epcho= 3063 loss=  tensor(0.0004)\n",
      "epcho= 3064 loss=  tensor(0.0004)\n",
      "epcho= 3065 loss=  tensor(0.0004)\n",
      "epcho= 3066 loss=  tensor(0.0004)\n",
      "epcho= 3067 loss=  tensor(0.0004)\n",
      "epcho= 3068 loss=  tensor(0.0004)\n",
      "epcho= 3069 loss=  tensor(0.0004)\n",
      "epcho= 3070 loss=  tensor(0.0004)\n",
      "epcho= 3071 loss=  tensor(0.0004)\n",
      "epcho= 3072 loss=  tensor(0.0004)\n",
      "epcho= 3073 loss=  tensor(0.0004)\n",
      "epcho= 3074 loss=  tensor(0.0004)\n",
      "epcho= 3075 loss=  tensor(0.0004)\n",
      "epcho= 3076 loss=  tensor(0.0004)\n",
      "epcho= 3077 loss=  tensor(0.0004)\n",
      "epcho= 3078 loss=  tensor(0.0004)\n",
      "epcho= 3079 loss=  tensor(0.0004)\n",
      "epcho= 3080 loss=  tensor(0.0004)\n",
      "epcho= 3081 loss=  tensor(0.0004)\n",
      "epcho= 3082 loss=  tensor(0.0004)\n",
      "epcho= 3083 loss=  tensor(0.0004)\n",
      "epcho= 3084 loss=  tensor(0.0004)\n",
      "epcho= 3085 loss=  tensor(0.0004)\n",
      "epcho= 3086 loss=  tensor(0.0004)\n",
      "epcho= 3087 loss=  tensor(0.0004)\n",
      "epcho= 3088 loss=  tensor(0.0004)\n",
      "epcho= 3089 loss=  tensor(0.0004)\n",
      "epcho= 3090 loss=  tensor(0.0004)\n",
      "epcho= 3091 loss=  tensor(0.0003)\n",
      "epcho= 3092 loss=  tensor(0.0003)\n",
      "epcho= 3093 loss=  tensor(0.0003)\n",
      "epcho= 3094 loss=  tensor(0.0003)\n",
      "epcho= 3095 loss=  tensor(0.0003)\n",
      "epcho= 3096 loss=  tensor(0.0003)\n",
      "epcho= 3097 loss=  tensor(0.0003)\n",
      "epcho= 3098 loss=  tensor(0.0003)\n",
      "epcho= 3099 loss=  tensor(0.0003)\n",
      "epcho= 3100 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1831, grad_fn=<DivBackward0>)\n",
      "epcho= 3101 loss=  tensor(0.0003)\n",
      "epcho= 3102 loss=  tensor(0.0003)\n",
      "epcho= 3103 loss=  tensor(0.0003)\n",
      "epcho= 3104 loss=  tensor(0.0003)\n",
      "epcho= 3105 loss=  tensor(0.0003)\n",
      "epcho= 3106 loss=  tensor(0.0003)\n",
      "epcho= 3107 loss=  tensor(0.0003)\n",
      "epcho= 3108 loss=  tensor(0.0003)\n",
      "epcho= 3109 loss=  tensor(0.0003)\n",
      "epcho= 3110 loss=  tensor(0.0003)\n",
      "epcho= 3111 loss=  tensor(0.0003)\n",
      "epcho= 3112 loss=  tensor(0.0003)\n",
      "epcho= 3113 loss=  tensor(0.0003)\n",
      "epcho= 3114 loss=  tensor(0.0003)\n",
      "epcho= 3115 loss=  tensor(0.0003)\n",
      "epcho= 3116 loss=  tensor(0.0003)\n",
      "epcho= 3117 loss=  tensor(0.0003)\n",
      "epcho= 3118 loss=  tensor(0.0003)\n",
      "epcho= 3119 loss=  tensor(0.0003)\n",
      "epcho= 3120 loss=  tensor(0.0003)\n",
      "epcho= 3121 loss=  tensor(0.0003)\n",
      "epcho= 3122 loss=  tensor(0.0003)\n",
      "epcho= 3123 loss=  tensor(0.0003)\n",
      "epcho= 3124 loss=  tensor(0.0003)\n",
      "epcho= 3125 loss=  tensor(0.0003)\n",
      "epcho= 3126 loss=  tensor(0.0003)\n",
      "epcho= 3127 loss=  tensor(0.0003)\n",
      "epcho= 3128 loss=  tensor(0.0003)\n",
      "epcho= 3129 loss=  tensor(0.0003)\n",
      "epcho= 3130 loss=  tensor(0.0003)\n",
      "epcho= 3131 loss=  tensor(0.0003)\n",
      "epcho= 3132 loss=  tensor(0.0003)\n",
      "epcho= 3133 loss=  tensor(0.0003)\n",
      "epcho= 3134 loss=  tensor(0.0003)\n",
      "epcho= 3135 loss=  tensor(0.0003)\n",
      "epcho= 3136 loss=  tensor(0.0003)\n",
      "epcho= 3137 loss=  tensor(0.0003)\n",
      "epcho= 3138 loss=  tensor(0.0003)\n",
      "epcho= 3139 loss=  tensor(0.0003)\n",
      "epcho= 3140 loss=  tensor(0.0003)\n",
      "epcho= 3141 loss=  tensor(0.0003)\n",
      "epcho= 3142 loss=  tensor(0.0003)\n",
      "epcho= 3143 loss=  tensor(0.0003)\n",
      "epcho= 3144 loss=  tensor(0.0003)\n",
      "epcho= 3145 loss=  tensor(0.0003)\n",
      "epcho= 3146 loss=  tensor(0.0003)\n",
      "epcho= 3147 loss=  tensor(0.0003)\n",
      "epcho= 3148 loss=  tensor(0.0003)\n",
      "epcho= 3149 loss=  tensor(0.0003)\n",
      "epcho= 3150 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1822, grad_fn=<DivBackward0>)\n",
      "epcho= 3151 loss=  tensor(0.0003)\n",
      "epcho= 3152 loss=  tensor(0.0003)\n",
      "epcho= 3153 loss=  tensor(0.0003)\n",
      "epcho= 3154 loss=  tensor(0.0003)\n",
      "epcho= 3155 loss=  tensor(0.0003)\n",
      "epcho= 3156 loss=  tensor(0.0003)\n",
      "epcho= 3157 loss=  tensor(0.0003)\n",
      "epcho= 3158 loss=  tensor(0.0003)\n",
      "epcho= 3159 loss=  tensor(0.0003)\n",
      "epcho= 3160 loss=  tensor(0.0003)\n",
      "epcho= 3161 loss=  tensor(0.0003)\n",
      "epcho= 3162 loss=  tensor(0.0003)\n",
      "epcho= 3163 loss=  tensor(0.0003)\n",
      "epcho= 3164 loss=  tensor(0.0003)\n",
      "epcho= 3165 loss=  tensor(0.0003)\n",
      "epcho= 3166 loss=  tensor(0.0003)\n",
      "epcho= 3167 loss=  tensor(0.0003)\n",
      "epcho= 3168 loss=  tensor(0.0003)\n",
      "epcho= 3169 loss=  tensor(0.0003)\n",
      "epcho= 3170 loss=  tensor(0.0003)\n",
      "epcho= 3171 loss=  tensor(0.0003)\n",
      "epcho= 3172 loss=  tensor(0.0003)\n",
      "epcho= 3173 loss=  tensor(0.0003)\n",
      "epcho= 3174 loss=  tensor(0.0003)\n",
      "epcho= 3175 loss=  tensor(0.0003)\n",
      "epcho= 3176 loss=  tensor(0.0003)\n",
      "epcho= 3177 loss=  tensor(0.0003)\n",
      "epcho= 3178 loss=  tensor(0.0003)\n",
      "epcho= 3179 loss=  tensor(0.0003)\n",
      "epcho= 3180 loss=  tensor(0.0003)\n",
      "epcho= 3181 loss=  tensor(0.0003)\n",
      "epcho= 3182 loss=  tensor(0.0003)\n",
      "epcho= 3183 loss=  tensor(0.0003)\n",
      "epcho= 3184 loss=  tensor(0.0003)\n",
      "epcho= 3185 loss=  tensor(0.0003)\n",
      "epcho= 3186 loss=  tensor(0.0003)\n",
      "epcho= 3187 loss=  tensor(0.0003)\n",
      "epcho= 3188 loss=  tensor(0.0003)\n",
      "epcho= 3189 loss=  tensor(0.0003)\n",
      "epcho= 3190 loss=  tensor(0.0003)\n",
      "epcho= 3191 loss=  tensor(0.0004)\n",
      "epcho= 3192 loss=  tensor(0.0004)\n",
      "epcho= 3193 loss=  tensor(0.0004)\n",
      "epcho= 3194 loss=  tensor(0.0004)\n",
      "epcho= 3195 loss=  tensor(0.0004)\n",
      "epcho= 3196 loss=  tensor(0.0005)\n",
      "epcho= 3197 loss=  tensor(0.0006)\n",
      "epcho= 3198 loss=  tensor(0.0009)\n",
      "epcho= 3199 loss=  tensor(0.0012)\n",
      "epcho= 3200 loss=  tensor(0.0018)\n",
      "tensor(0.0018, grad_fn=<AddBackward0>) tensor(0.2144, grad_fn=<DivBackward0>)\n",
      "epcho= 3201 loss=  tensor(0.0025)\n",
      "epcho= 3202 loss=  tensor(0.0031)\n",
      "epcho= 3203 loss=  tensor(0.0034)\n",
      "epcho= 3204 loss=  tensor(0.0032)\n",
      "epcho= 3205 loss=  tensor(0.0024)\n",
      "epcho= 3206 loss=  tensor(0.0014)\n",
      "epcho= 3207 loss=  tensor(0.0006)\n",
      "epcho= 3208 loss=  tensor(0.0003)\n",
      "epcho= 3209 loss=  tensor(0.0007)\n",
      "epcho= 3210 loss=  tensor(0.0012)\n",
      "epcho= 3211 loss=  tensor(0.0015)\n",
      "epcho= 3212 loss=  tensor(0.0013)\n",
      "epcho= 3213 loss=  tensor(0.0009)\n",
      "epcho= 3214 loss=  tensor(0.0005)\n",
      "epcho= 3215 loss=  tensor(0.0003)\n",
      "epcho= 3216 loss=  tensor(0.0005)\n",
      "epcho= 3217 loss=  tensor(0.0008)\n",
      "epcho= 3218 loss=  tensor(0.0009)\n",
      "epcho= 3219 loss=  tensor(0.0007)\n",
      "epcho= 3220 loss=  tensor(0.0005)\n",
      "epcho= 3221 loss=  tensor(0.0003)\n",
      "epcho= 3222 loss=  tensor(0.0004)\n",
      "epcho= 3223 loss=  tensor(0.0005)\n",
      "epcho= 3224 loss=  tensor(0.0006)\n",
      "epcho= 3225 loss=  tensor(0.0006)\n",
      "epcho= 3226 loss=  tensor(0.0005)\n",
      "epcho= 3227 loss=  tensor(0.0004)\n",
      "epcho= 3228 loss=  tensor(0.0003)\n",
      "epcho= 3229 loss=  tensor(0.0004)\n",
      "epcho= 3230 loss=  tensor(0.0005)\n",
      "epcho= 3231 loss=  tensor(0.0005)\n",
      "epcho= 3232 loss=  tensor(0.0004)\n",
      "epcho= 3233 loss=  tensor(0.0004)\n",
      "epcho= 3234 loss=  tensor(0.0003)\n",
      "epcho= 3235 loss=  tensor(0.0003)\n",
      "epcho= 3236 loss=  tensor(0.0004)\n",
      "epcho= 3237 loss=  tensor(0.0004)\n",
      "epcho= 3238 loss=  tensor(0.0004)\n",
      "epcho= 3239 loss=  tensor(0.0004)\n",
      "epcho= 3240 loss=  tensor(0.0003)\n",
      "epcho= 3241 loss=  tensor(0.0003)\n",
      "epcho= 3242 loss=  tensor(0.0003)\n",
      "epcho= 3243 loss=  tensor(0.0004)\n",
      "epcho= 3244 loss=  tensor(0.0004)\n",
      "epcho= 3245 loss=  tensor(0.0004)\n",
      "epcho= 3246 loss=  tensor(0.0003)\n",
      "epcho= 3247 loss=  tensor(0.0003)\n",
      "epcho= 3248 loss=  tensor(0.0003)\n",
      "epcho= 3249 loss=  tensor(0.0003)\n",
      "epcho= 3250 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1789, grad_fn=<DivBackward0>)\n",
      "epcho= 3251 loss=  tensor(0.0003)\n",
      "epcho= 3252 loss=  tensor(0.0003)\n",
      "epcho= 3253 loss=  tensor(0.0003)\n",
      "epcho= 3254 loss=  tensor(0.0003)\n",
      "epcho= 3255 loss=  tensor(0.0003)\n",
      "epcho= 3256 loss=  tensor(0.0003)\n",
      "epcho= 3257 loss=  tensor(0.0003)\n",
      "epcho= 3258 loss=  tensor(0.0003)\n",
      "epcho= 3259 loss=  tensor(0.0003)\n",
      "epcho= 3260 loss=  tensor(0.0003)\n",
      "epcho= 3261 loss=  tensor(0.0003)\n",
      "epcho= 3262 loss=  tensor(0.0003)\n",
      "epcho= 3263 loss=  tensor(0.0003)\n",
      "epcho= 3264 loss=  tensor(0.0003)\n",
      "epcho= 3265 loss=  tensor(0.0003)\n",
      "epcho= 3266 loss=  tensor(0.0003)\n",
      "epcho= 3267 loss=  tensor(0.0003)\n",
      "epcho= 3268 loss=  tensor(0.0003)\n",
      "epcho= 3269 loss=  tensor(0.0003)\n",
      "epcho= 3270 loss=  tensor(0.0003)\n",
      "epcho= 3271 loss=  tensor(0.0003)\n",
      "epcho= 3272 loss=  tensor(0.0003)\n",
      "epcho= 3273 loss=  tensor(0.0003)\n",
      "epcho= 3274 loss=  tensor(0.0003)\n",
      "epcho= 3275 loss=  tensor(0.0003)\n",
      "epcho= 3276 loss=  tensor(0.0003)\n",
      "epcho= 3277 loss=  tensor(0.0003)\n",
      "epcho= 3278 loss=  tensor(0.0003)\n",
      "epcho= 3279 loss=  tensor(0.0003)\n",
      "epcho= 3280 loss=  tensor(0.0003)\n",
      "epcho= 3281 loss=  tensor(0.0003)\n",
      "epcho= 3282 loss=  tensor(0.0003)\n",
      "epcho= 3283 loss=  tensor(0.0003)\n",
      "epcho= 3284 loss=  tensor(0.0003)\n",
      "epcho= 3285 loss=  tensor(0.0003)\n",
      "epcho= 3286 loss=  tensor(0.0003)\n",
      "epcho= 3287 loss=  tensor(0.0003)\n",
      "epcho= 3288 loss=  tensor(0.0003)\n",
      "epcho= 3289 loss=  tensor(0.0003)\n",
      "epcho= 3290 loss=  tensor(0.0003)\n",
      "epcho= 3291 loss=  tensor(0.0003)\n",
      "epcho= 3292 loss=  tensor(0.0003)\n",
      "epcho= 3293 loss=  tensor(0.0003)\n",
      "epcho= 3294 loss=  tensor(0.0003)\n",
      "epcho= 3295 loss=  tensor(0.0003)\n",
      "epcho= 3296 loss=  tensor(0.0003)\n",
      "epcho= 3297 loss=  tensor(0.0003)\n",
      "epcho= 3298 loss=  tensor(0.0003)\n",
      "epcho= 3299 loss=  tensor(0.0003)\n",
      "epcho= 3300 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1807, grad_fn=<DivBackward0>)\n",
      "epcho= 3301 loss=  tensor(0.0003)\n",
      "epcho= 3302 loss=  tensor(0.0003)\n",
      "epcho= 3303 loss=  tensor(0.0003)\n",
      "epcho= 3304 loss=  tensor(0.0003)\n",
      "epcho= 3305 loss=  tensor(0.0003)\n",
      "epcho= 3306 loss=  tensor(0.0003)\n",
      "epcho= 3307 loss=  tensor(0.0003)\n",
      "epcho= 3308 loss=  tensor(0.0003)\n",
      "epcho= 3309 loss=  tensor(0.0003)\n",
      "epcho= 3310 loss=  tensor(0.0003)\n",
      "epcho= 3311 loss=  tensor(0.0003)\n",
      "epcho= 3312 loss=  tensor(0.0003)\n",
      "epcho= 3313 loss=  tensor(0.0003)\n",
      "epcho= 3314 loss=  tensor(0.0003)\n",
      "epcho= 3315 loss=  tensor(0.0003)\n",
      "epcho= 3316 loss=  tensor(0.0003)\n",
      "epcho= 3317 loss=  tensor(0.0003)\n",
      "epcho= 3318 loss=  tensor(0.0003)\n",
      "epcho= 3319 loss=  tensor(0.0003)\n",
      "epcho= 3320 loss=  tensor(0.0003)\n",
      "epcho= 3321 loss=  tensor(0.0003)\n",
      "epcho= 3322 loss=  tensor(0.0003)\n",
      "epcho= 3323 loss=  tensor(0.0003)\n",
      "epcho= 3324 loss=  tensor(0.0003)\n",
      "epcho= 3325 loss=  tensor(0.0003)\n",
      "epcho= 3326 loss=  tensor(0.0003)\n",
      "epcho= 3327 loss=  tensor(0.0003)\n",
      "epcho= 3328 loss=  tensor(0.0003)\n",
      "epcho= 3329 loss=  tensor(0.0003)\n",
      "epcho= 3330 loss=  tensor(0.0003)\n",
      "epcho= 3331 loss=  tensor(0.0003)\n",
      "epcho= 3332 loss=  tensor(0.0003)\n",
      "epcho= 3333 loss=  tensor(0.0003)\n",
      "epcho= 3334 loss=  tensor(0.0003)\n",
      "epcho= 3335 loss=  tensor(0.0003)\n",
      "epcho= 3336 loss=  tensor(0.0003)\n",
      "epcho= 3337 loss=  tensor(0.0003)\n",
      "epcho= 3338 loss=  tensor(0.0003)\n",
      "epcho= 3339 loss=  tensor(0.0003)\n",
      "epcho= 3340 loss=  tensor(0.0003)\n",
      "epcho= 3341 loss=  tensor(0.0003)\n",
      "epcho= 3342 loss=  tensor(0.0003)\n",
      "epcho= 3343 loss=  tensor(0.0003)\n",
      "epcho= 3344 loss=  tensor(0.0003)\n",
      "epcho= 3345 loss=  tensor(0.0003)\n",
      "epcho= 3346 loss=  tensor(0.0003)\n",
      "epcho= 3347 loss=  tensor(0.0003)\n",
      "epcho= 3348 loss=  tensor(0.0003)\n",
      "epcho= 3349 loss=  tensor(0.0003)\n",
      "epcho= 3350 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1797, grad_fn=<DivBackward0>)\n",
      "epcho= 3351 loss=  tensor(0.0003)\n",
      "epcho= 3352 loss=  tensor(0.0003)\n",
      "epcho= 3353 loss=  tensor(0.0003)\n",
      "epcho= 3354 loss=  tensor(0.0003)\n",
      "epcho= 3355 loss=  tensor(0.0003)\n",
      "epcho= 3356 loss=  tensor(0.0003)\n",
      "epcho= 3357 loss=  tensor(0.0003)\n",
      "epcho= 3358 loss=  tensor(0.0003)\n",
      "epcho= 3359 loss=  tensor(0.0003)\n",
      "epcho= 3360 loss=  tensor(0.0003)\n",
      "epcho= 3361 loss=  tensor(0.0003)\n",
      "epcho= 3362 loss=  tensor(0.0003)\n",
      "epcho= 3363 loss=  tensor(0.0003)\n",
      "epcho= 3364 loss=  tensor(0.0003)\n",
      "epcho= 3365 loss=  tensor(0.0003)\n",
      "epcho= 3366 loss=  tensor(0.0003)\n",
      "epcho= 3367 loss=  tensor(0.0003)\n",
      "epcho= 3368 loss=  tensor(0.0003)\n",
      "epcho= 3369 loss=  tensor(0.0003)\n",
      "epcho= 3370 loss=  tensor(0.0003)\n",
      "epcho= 3371 loss=  tensor(0.0003)\n",
      "epcho= 3372 loss=  tensor(0.0003)\n",
      "epcho= 3373 loss=  tensor(0.0003)\n",
      "epcho= 3374 loss=  tensor(0.0003)\n",
      "epcho= 3375 loss=  tensor(0.0003)\n",
      "epcho= 3376 loss=  tensor(0.0003)\n",
      "epcho= 3377 loss=  tensor(0.0003)\n",
      "epcho= 3378 loss=  tensor(0.0003)\n",
      "epcho= 3379 loss=  tensor(0.0003)\n",
      "epcho= 3380 loss=  tensor(0.0003)\n",
      "epcho= 3381 loss=  tensor(0.0003)\n",
      "epcho= 3382 loss=  tensor(0.0003)\n",
      "epcho= 3383 loss=  tensor(0.0003)\n",
      "epcho= 3384 loss=  tensor(0.0003)\n",
      "epcho= 3385 loss=  tensor(0.0003)\n",
      "epcho= 3386 loss=  tensor(0.0003)\n",
      "epcho= 3387 loss=  tensor(0.0003)\n",
      "epcho= 3388 loss=  tensor(0.0003)\n",
      "epcho= 3389 loss=  tensor(0.0003)\n",
      "epcho= 3390 loss=  tensor(0.0003)\n",
      "epcho= 3391 loss=  tensor(0.0003)\n",
      "epcho= 3392 loss=  tensor(0.0003)\n",
      "epcho= 3393 loss=  tensor(0.0003)\n",
      "epcho= 3394 loss=  tensor(0.0003)\n",
      "epcho= 3395 loss=  tensor(0.0003)\n",
      "epcho= 3396 loss=  tensor(0.0003)\n",
      "epcho= 3397 loss=  tensor(0.0003)\n",
      "epcho= 3398 loss=  tensor(0.0003)\n",
      "epcho= 3399 loss=  tensor(0.0003)\n",
      "epcho= 3400 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1787, grad_fn=<DivBackward0>)\n",
      "epcho= 3401 loss=  tensor(0.0003)\n",
      "epcho= 3402 loss=  tensor(0.0003)\n",
      "epcho= 3403 loss=  tensor(0.0003)\n",
      "epcho= 3404 loss=  tensor(0.0003)\n",
      "epcho= 3405 loss=  tensor(0.0003)\n",
      "epcho= 3406 loss=  tensor(0.0003)\n",
      "epcho= 3407 loss=  tensor(0.0003)\n",
      "epcho= 3408 loss=  tensor(0.0003)\n",
      "epcho= 3409 loss=  tensor(0.0003)\n",
      "epcho= 3410 loss=  tensor(0.0003)\n",
      "epcho= 3411 loss=  tensor(0.0003)\n",
      "epcho= 3412 loss=  tensor(0.0003)\n",
      "epcho= 3413 loss=  tensor(0.0003)\n",
      "epcho= 3414 loss=  tensor(0.0003)\n",
      "epcho= 3415 loss=  tensor(0.0003)\n",
      "epcho= 3416 loss=  tensor(0.0003)\n",
      "epcho= 3417 loss=  tensor(0.0003)\n",
      "epcho= 3418 loss=  tensor(0.0003)\n",
      "epcho= 3419 loss=  tensor(0.0003)\n",
      "epcho= 3420 loss=  tensor(0.0003)\n",
      "epcho= 3421 loss=  tensor(0.0003)\n",
      "epcho= 3422 loss=  tensor(0.0003)\n",
      "epcho= 3423 loss=  tensor(0.0003)\n",
      "epcho= 3424 loss=  tensor(0.0003)\n",
      "epcho= 3425 loss=  tensor(0.0003)\n",
      "epcho= 3426 loss=  tensor(0.0003)\n",
      "epcho= 3427 loss=  tensor(0.0003)\n",
      "epcho= 3428 loss=  tensor(0.0003)\n",
      "epcho= 3429 loss=  tensor(0.0003)\n",
      "epcho= 3430 loss=  tensor(0.0003)\n",
      "epcho= 3431 loss=  tensor(0.0003)\n",
      "epcho= 3432 loss=  tensor(0.0003)\n",
      "epcho= 3433 loss=  tensor(0.0003)\n",
      "epcho= 3434 loss=  tensor(0.0003)\n",
      "epcho= 3435 loss=  tensor(0.0003)\n",
      "epcho= 3436 loss=  tensor(0.0003)\n",
      "epcho= 3437 loss=  tensor(0.0003)\n",
      "epcho= 3438 loss=  tensor(0.0003)\n",
      "epcho= 3439 loss=  tensor(0.0003)\n",
      "epcho= 3440 loss=  tensor(0.0003)\n",
      "epcho= 3441 loss=  tensor(0.0003)\n",
      "epcho= 3442 loss=  tensor(0.0003)\n",
      "epcho= 3443 loss=  tensor(0.0003)\n",
      "epcho= 3444 loss=  tensor(0.0003)\n",
      "epcho= 3445 loss=  tensor(0.0003)\n",
      "epcho= 3446 loss=  tensor(0.0003)\n",
      "epcho= 3447 loss=  tensor(0.0003)\n",
      "epcho= 3448 loss=  tensor(0.0003)\n",
      "epcho= 3449 loss=  tensor(0.0003)\n",
      "epcho= 3450 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1778, grad_fn=<DivBackward0>)\n",
      "epcho= 3451 loss=  tensor(0.0003)\n",
      "epcho= 3452 loss=  tensor(0.0003)\n",
      "epcho= 3453 loss=  tensor(0.0003)\n",
      "epcho= 3454 loss=  tensor(0.0003)\n",
      "epcho= 3455 loss=  tensor(0.0003)\n",
      "epcho= 3456 loss=  tensor(0.0003)\n",
      "epcho= 3457 loss=  tensor(0.0003)\n",
      "epcho= 3458 loss=  tensor(0.0003)\n",
      "epcho= 3459 loss=  tensor(0.0003)\n",
      "epcho= 3460 loss=  tensor(0.0003)\n",
      "epcho= 3461 loss=  tensor(0.0003)\n",
      "epcho= 3462 loss=  tensor(0.0003)\n",
      "epcho= 3463 loss=  tensor(0.0003)\n",
      "epcho= 3464 loss=  tensor(0.0003)\n",
      "epcho= 3465 loss=  tensor(0.0003)\n",
      "epcho= 3466 loss=  tensor(0.0003)\n",
      "epcho= 3467 loss=  tensor(0.0003)\n",
      "epcho= 3468 loss=  tensor(0.0003)\n",
      "epcho= 3469 loss=  tensor(0.0003)\n",
      "epcho= 3470 loss=  tensor(0.0003)\n",
      "epcho= 3471 loss=  tensor(0.0003)\n",
      "epcho= 3472 loss=  tensor(0.0003)\n",
      "epcho= 3473 loss=  tensor(0.0003)\n",
      "epcho= 3474 loss=  tensor(0.0003)\n",
      "epcho= 3475 loss=  tensor(0.0003)\n",
      "epcho= 3476 loss=  tensor(0.0003)\n",
      "epcho= 3477 loss=  tensor(0.0003)\n",
      "epcho= 3478 loss=  tensor(0.0003)\n",
      "epcho= 3479 loss=  tensor(0.0003)\n",
      "epcho= 3480 loss=  tensor(0.0003)\n",
      "epcho= 3481 loss=  tensor(0.0003)\n",
      "epcho= 3482 loss=  tensor(0.0003)\n",
      "epcho= 3483 loss=  tensor(0.0003)\n",
      "epcho= 3484 loss=  tensor(0.0003)\n",
      "epcho= 3485 loss=  tensor(0.0003)\n",
      "epcho= 3486 loss=  tensor(0.0003)\n",
      "epcho= 3487 loss=  tensor(0.0003)\n",
      "epcho= 3488 loss=  tensor(0.0003)\n",
      "epcho= 3489 loss=  tensor(0.0003)\n",
      "epcho= 3490 loss=  tensor(0.0003)\n",
      "epcho= 3491 loss=  tensor(0.0003)\n",
      "epcho= 3492 loss=  tensor(0.0003)\n",
      "epcho= 3493 loss=  tensor(0.0003)\n",
      "epcho= 3494 loss=  tensor(0.0003)\n",
      "epcho= 3495 loss=  tensor(0.0003)\n",
      "epcho= 3496 loss=  tensor(0.0003)\n",
      "epcho= 3497 loss=  tensor(0.0003)\n",
      "epcho= 3498 loss=  tensor(0.0003)\n",
      "epcho= 3499 loss=  tensor(0.0003)\n",
      "epcho= 3500 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1768, grad_fn=<DivBackward0>)\n",
      "epcho= 3501 loss=  tensor(0.0003)\n",
      "epcho= 3502 loss=  tensor(0.0003)\n",
      "epcho= 3503 loss=  tensor(0.0003)\n",
      "epcho= 3504 loss=  tensor(0.0003)\n",
      "epcho= 3505 loss=  tensor(0.0003)\n",
      "epcho= 3506 loss=  tensor(0.0003)\n",
      "epcho= 3507 loss=  tensor(0.0003)\n",
      "epcho= 3508 loss=  tensor(0.0003)\n",
      "epcho= 3509 loss=  tensor(0.0003)\n",
      "epcho= 3510 loss=  tensor(0.0003)\n",
      "epcho= 3511 loss=  tensor(0.0003)\n",
      "epcho= 3512 loss=  tensor(0.0003)\n",
      "epcho= 3513 loss=  tensor(0.0003)\n",
      "epcho= 3514 loss=  tensor(0.0003)\n",
      "epcho= 3515 loss=  tensor(0.0003)\n",
      "epcho= 3516 loss=  tensor(0.0003)\n",
      "epcho= 3517 loss=  tensor(0.0003)\n",
      "epcho= 3518 loss=  tensor(0.0003)\n",
      "epcho= 3519 loss=  tensor(0.0003)\n",
      "epcho= 3520 loss=  tensor(0.0003)\n",
      "epcho= 3521 loss=  tensor(0.0003)\n",
      "epcho= 3522 loss=  tensor(0.0003)\n",
      "epcho= 3523 loss=  tensor(0.0003)\n",
      "epcho= 3524 loss=  tensor(0.0003)\n",
      "epcho= 3525 loss=  tensor(0.0003)\n",
      "epcho= 3526 loss=  tensor(0.0003)\n",
      "epcho= 3527 loss=  tensor(0.0003)\n",
      "epcho= 3528 loss=  tensor(0.0003)\n",
      "epcho= 3529 loss=  tensor(0.0003)\n",
      "epcho= 3530 loss=  tensor(0.0003)\n",
      "epcho= 3531 loss=  tensor(0.0003)\n",
      "epcho= 3532 loss=  tensor(0.0003)\n",
      "epcho= 3533 loss=  tensor(0.0003)\n",
      "epcho= 3534 loss=  tensor(0.0003)\n",
      "epcho= 3535 loss=  tensor(0.0003)\n",
      "epcho= 3536 loss=  tensor(0.0003)\n",
      "epcho= 3537 loss=  tensor(0.0003)\n",
      "epcho= 3538 loss=  tensor(0.0003)\n",
      "epcho= 3539 loss=  tensor(0.0003)\n",
      "epcho= 3540 loss=  tensor(0.0003)\n",
      "epcho= 3541 loss=  tensor(0.0003)\n",
      "epcho= 3542 loss=  tensor(0.0003)\n",
      "epcho= 3543 loss=  tensor(0.0003)\n",
      "epcho= 3544 loss=  tensor(0.0003)\n",
      "epcho= 3545 loss=  tensor(0.0003)\n",
      "epcho= 3546 loss=  tensor(0.0003)\n",
      "epcho= 3547 loss=  tensor(0.0003)\n",
      "epcho= 3548 loss=  tensor(0.0003)\n",
      "epcho= 3549 loss=  tensor(0.0003)\n",
      "epcho= 3550 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1767, grad_fn=<DivBackward0>)\n",
      "epcho= 3551 loss=  tensor(0.0003)\n",
      "epcho= 3552 loss=  tensor(0.0003)\n",
      "epcho= 3553 loss=  tensor(0.0003)\n",
      "epcho= 3554 loss=  tensor(0.0003)\n",
      "epcho= 3555 loss=  tensor(0.0003)\n",
      "epcho= 3556 loss=  tensor(0.0003)\n",
      "epcho= 3557 loss=  tensor(0.0003)\n",
      "epcho= 3558 loss=  tensor(0.0003)\n",
      "epcho= 3559 loss=  tensor(0.0003)\n",
      "epcho= 3560 loss=  tensor(0.0003)\n",
      "epcho= 3561 loss=  tensor(0.0003)\n",
      "epcho= 3562 loss=  tensor(0.0003)\n",
      "epcho= 3563 loss=  tensor(0.0003)\n",
      "epcho= 3564 loss=  tensor(0.0003)\n",
      "epcho= 3565 loss=  tensor(0.0003)\n",
      "epcho= 3566 loss=  tensor(0.0004)\n",
      "epcho= 3567 loss=  tensor(0.0004)\n",
      "epcho= 3568 loss=  tensor(0.0004)\n",
      "epcho= 3569 loss=  tensor(0.0005)\n",
      "epcho= 3570 loss=  tensor(0.0006)\n",
      "epcho= 3571 loss=  tensor(0.0007)\n",
      "epcho= 3572 loss=  tensor(0.0009)\n",
      "epcho= 3573 loss=  tensor(0.0011)\n",
      "epcho= 3574 loss=  tensor(0.0014)\n",
      "epcho= 3575 loss=  tensor(0.0018)\n",
      "epcho= 3576 loss=  tensor(0.0022)\n",
      "epcho= 3577 loss=  tensor(0.0025)\n",
      "epcho= 3578 loss=  tensor(0.0025)\n",
      "epcho= 3579 loss=  tensor(0.0023)\n",
      "epcho= 3580 loss=  tensor(0.0017)\n",
      "epcho= 3581 loss=  tensor(0.0010)\n",
      "epcho= 3582 loss=  tensor(0.0005)\n",
      "epcho= 3583 loss=  tensor(0.0003)\n",
      "epcho= 3584 loss=  tensor(0.0004)\n",
      "epcho= 3585 loss=  tensor(0.0008)\n",
      "epcho= 3586 loss=  tensor(0.0010)\n",
      "epcho= 3587 loss=  tensor(0.0010)\n",
      "epcho= 3588 loss=  tensor(0.0008)\n",
      "epcho= 3589 loss=  tensor(0.0005)\n",
      "epcho= 3590 loss=  tensor(0.0003)\n",
      "epcho= 3591 loss=  tensor(0.0003)\n",
      "epcho= 3592 loss=  tensor(0.0004)\n",
      "epcho= 3593 loss=  tensor(0.0006)\n",
      "epcho= 3594 loss=  tensor(0.0006)\n",
      "epcho= 3595 loss=  tensor(0.0006)\n",
      "epcho= 3596 loss=  tensor(0.0004)\n",
      "epcho= 3597 loss=  tensor(0.0003)\n",
      "epcho= 3598 loss=  tensor(0.0003)\n",
      "epcho= 3599 loss=  tensor(0.0004)\n",
      "epcho= 3600 loss=  tensor(0.0004)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) tensor(0.1681, grad_fn=<DivBackward0>)\n",
      "epcho= 3601 loss=  tensor(0.0004)\n",
      "epcho= 3602 loss=  tensor(0.0004)\n",
      "epcho= 3603 loss=  tensor(0.0003)\n",
      "epcho= 3604 loss=  tensor(0.0003)\n",
      "epcho= 3605 loss=  tensor(0.0003)\n",
      "epcho= 3606 loss=  tensor(0.0004)\n",
      "epcho= 3607 loss=  tensor(0.0004)\n",
      "epcho= 3608 loss=  tensor(0.0003)\n",
      "epcho= 3609 loss=  tensor(0.0003)\n",
      "epcho= 3610 loss=  tensor(0.0003)\n",
      "epcho= 3611 loss=  tensor(0.0003)\n",
      "epcho= 3612 loss=  tensor(0.0003)\n",
      "epcho= 3613 loss=  tensor(0.0003)\n",
      "epcho= 3614 loss=  tensor(0.0003)\n",
      "epcho= 3615 loss=  tensor(0.0003)\n",
      "epcho= 3616 loss=  tensor(0.0003)\n",
      "epcho= 3617 loss=  tensor(0.0003)\n",
      "epcho= 3618 loss=  tensor(0.0003)\n",
      "epcho= 3619 loss=  tensor(0.0003)\n",
      "epcho= 3620 loss=  tensor(0.0003)\n",
      "epcho= 3621 loss=  tensor(0.0003)\n",
      "epcho= 3622 loss=  tensor(0.0003)\n",
      "epcho= 3623 loss=  tensor(0.0003)\n",
      "epcho= 3624 loss=  tensor(0.0003)\n",
      "epcho= 3625 loss=  tensor(0.0003)\n",
      "epcho= 3626 loss=  tensor(0.0003)\n",
      "epcho= 3627 loss=  tensor(0.0003)\n",
      "epcho= 3628 loss=  tensor(0.0003)\n",
      "epcho= 3629 loss=  tensor(0.0003)\n",
      "epcho= 3630 loss=  tensor(0.0003)\n",
      "epcho= 3631 loss=  tensor(0.0003)\n",
      "epcho= 3632 loss=  tensor(0.0003)\n",
      "epcho= 3633 loss=  tensor(0.0003)\n",
      "epcho= 3634 loss=  tensor(0.0003)\n",
      "epcho= 3635 loss=  tensor(0.0003)\n",
      "epcho= 3636 loss=  tensor(0.0003)\n",
      "epcho= 3637 loss=  tensor(0.0003)\n",
      "epcho= 3638 loss=  tensor(0.0003)\n",
      "epcho= 3639 loss=  tensor(0.0003)\n",
      "epcho= 3640 loss=  tensor(0.0003)\n",
      "epcho= 3641 loss=  tensor(0.0003)\n",
      "epcho= 3642 loss=  tensor(0.0003)\n",
      "epcho= 3643 loss=  tensor(0.0003)\n",
      "epcho= 3644 loss=  tensor(0.0003)\n",
      "epcho= 3645 loss=  tensor(0.0003)\n",
      "epcho= 3646 loss=  tensor(0.0003)\n",
      "epcho= 3647 loss=  tensor(0.0003)\n",
      "epcho= 3648 loss=  tensor(0.0003)\n",
      "epcho= 3649 loss=  tensor(0.0003)\n",
      "epcho= 3650 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1749, grad_fn=<DivBackward0>)\n",
      "epcho= 3651 loss=  tensor(0.0003)\n",
      "epcho= 3652 loss=  tensor(0.0003)\n",
      "epcho= 3653 loss=  tensor(0.0003)\n",
      "epcho= 3654 loss=  tensor(0.0003)\n",
      "epcho= 3655 loss=  tensor(0.0003)\n",
      "epcho= 3656 loss=  tensor(0.0003)\n",
      "epcho= 3657 loss=  tensor(0.0003)\n",
      "epcho= 3658 loss=  tensor(0.0003)\n",
      "epcho= 3659 loss=  tensor(0.0003)\n",
      "epcho= 3660 loss=  tensor(0.0003)\n",
      "epcho= 3661 loss=  tensor(0.0003)\n",
      "epcho= 3662 loss=  tensor(0.0003)\n",
      "epcho= 3663 loss=  tensor(0.0003)\n",
      "epcho= 3664 loss=  tensor(0.0003)\n",
      "epcho= 3665 loss=  tensor(0.0003)\n",
      "epcho= 3666 loss=  tensor(0.0003)\n",
      "epcho= 3667 loss=  tensor(0.0003)\n",
      "epcho= 3668 loss=  tensor(0.0003)\n",
      "epcho= 3669 loss=  tensor(0.0003)\n",
      "epcho= 3670 loss=  tensor(0.0003)\n",
      "epcho= 3671 loss=  tensor(0.0003)\n",
      "epcho= 3672 loss=  tensor(0.0003)\n",
      "epcho= 3673 loss=  tensor(0.0003)\n",
      "epcho= 3674 loss=  tensor(0.0003)\n",
      "epcho= 3675 loss=  tensor(0.0003)\n",
      "epcho= 3676 loss=  tensor(0.0003)\n",
      "epcho= 3677 loss=  tensor(0.0003)\n",
      "epcho= 3678 loss=  tensor(0.0003)\n",
      "epcho= 3679 loss=  tensor(0.0003)\n",
      "epcho= 3680 loss=  tensor(0.0003)\n",
      "epcho= 3681 loss=  tensor(0.0003)\n",
      "epcho= 3682 loss=  tensor(0.0003)\n",
      "epcho= 3683 loss=  tensor(0.0003)\n",
      "epcho= 3684 loss=  tensor(0.0003)\n",
      "epcho= 3685 loss=  tensor(0.0003)\n",
      "epcho= 3686 loss=  tensor(0.0003)\n",
      "epcho= 3687 loss=  tensor(0.0003)\n",
      "epcho= 3688 loss=  tensor(0.0003)\n",
      "epcho= 3689 loss=  tensor(0.0003)\n",
      "epcho= 3690 loss=  tensor(0.0003)\n",
      "epcho= 3691 loss=  tensor(0.0003)\n",
      "epcho= 3692 loss=  tensor(0.0003)\n",
      "epcho= 3693 loss=  tensor(0.0003)\n",
      "epcho= 3694 loss=  tensor(0.0003)\n",
      "epcho= 3695 loss=  tensor(0.0003)\n",
      "epcho= 3696 loss=  tensor(0.0003)\n",
      "epcho= 3697 loss=  tensor(0.0003)\n",
      "epcho= 3698 loss=  tensor(0.0003)\n",
      "epcho= 3699 loss=  tensor(0.0003)\n",
      "epcho= 3700 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1743, grad_fn=<DivBackward0>)\n",
      "epcho= 3701 loss=  tensor(0.0003)\n",
      "epcho= 3702 loss=  tensor(0.0003)\n",
      "epcho= 3703 loss=  tensor(0.0003)\n",
      "epcho= 3704 loss=  tensor(0.0003)\n",
      "epcho= 3705 loss=  tensor(0.0003)\n",
      "epcho= 3706 loss=  tensor(0.0003)\n",
      "epcho= 3707 loss=  tensor(0.0003)\n",
      "epcho= 3708 loss=  tensor(0.0003)\n",
      "epcho= 3709 loss=  tensor(0.0003)\n",
      "epcho= 3710 loss=  tensor(0.0003)\n",
      "epcho= 3711 loss=  tensor(0.0003)\n",
      "epcho= 3712 loss=  tensor(0.0003)\n",
      "epcho= 3713 loss=  tensor(0.0003)\n",
      "epcho= 3714 loss=  tensor(0.0003)\n",
      "epcho= 3715 loss=  tensor(0.0003)\n",
      "epcho= 3716 loss=  tensor(0.0003)\n",
      "epcho= 3717 loss=  tensor(0.0003)\n",
      "epcho= 3718 loss=  tensor(0.0003)\n",
      "epcho= 3719 loss=  tensor(0.0003)\n",
      "epcho= 3720 loss=  tensor(0.0003)\n",
      "epcho= 3721 loss=  tensor(0.0003)\n",
      "epcho= 3722 loss=  tensor(0.0003)\n",
      "epcho= 3723 loss=  tensor(0.0003)\n",
      "epcho= 3724 loss=  tensor(0.0003)\n",
      "epcho= 3725 loss=  tensor(0.0003)\n",
      "epcho= 3726 loss=  tensor(0.0003)\n",
      "epcho= 3727 loss=  tensor(0.0003)\n",
      "epcho= 3728 loss=  tensor(0.0003)\n",
      "epcho= 3729 loss=  tensor(0.0003)\n",
      "epcho= 3730 loss=  tensor(0.0003)\n",
      "epcho= 3731 loss=  tensor(0.0003)\n",
      "epcho= 3732 loss=  tensor(0.0003)\n",
      "epcho= 3733 loss=  tensor(0.0003)\n",
      "epcho= 3734 loss=  tensor(0.0003)\n",
      "epcho= 3735 loss=  tensor(0.0003)\n",
      "epcho= 3736 loss=  tensor(0.0003)\n",
      "epcho= 3737 loss=  tensor(0.0003)\n",
      "epcho= 3738 loss=  tensor(0.0003)\n",
      "epcho= 3739 loss=  tensor(0.0003)\n",
      "epcho= 3740 loss=  tensor(0.0003)\n",
      "epcho= 3741 loss=  tensor(0.0003)\n",
      "epcho= 3742 loss=  tensor(0.0003)\n",
      "epcho= 3743 loss=  tensor(0.0003)\n",
      "epcho= 3744 loss=  tensor(0.0003)\n",
      "epcho= 3745 loss=  tensor(0.0003)\n",
      "epcho= 3746 loss=  tensor(0.0003)\n",
      "epcho= 3747 loss=  tensor(0.0003)\n",
      "epcho= 3748 loss=  tensor(0.0003)\n",
      "epcho= 3749 loss=  tensor(0.0003)\n",
      "epcho= 3750 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1732, grad_fn=<DivBackward0>)\n",
      "epcho= 3751 loss=  tensor(0.0003)\n",
      "epcho= 3752 loss=  tensor(0.0003)\n",
      "epcho= 3753 loss=  tensor(0.0003)\n",
      "epcho= 3754 loss=  tensor(0.0003)\n",
      "epcho= 3755 loss=  tensor(0.0003)\n",
      "epcho= 3756 loss=  tensor(0.0003)\n",
      "epcho= 3757 loss=  tensor(0.0003)\n",
      "epcho= 3758 loss=  tensor(0.0003)\n",
      "epcho= 3759 loss=  tensor(0.0003)\n",
      "epcho= 3760 loss=  tensor(0.0003)\n",
      "epcho= 3761 loss=  tensor(0.0003)\n",
      "epcho= 3762 loss=  tensor(0.0003)\n",
      "epcho= 3763 loss=  tensor(0.0003)\n",
      "epcho= 3764 loss=  tensor(0.0003)\n",
      "epcho= 3765 loss=  tensor(0.0003)\n",
      "epcho= 3766 loss=  tensor(0.0003)\n",
      "epcho= 3767 loss=  tensor(0.0003)\n",
      "epcho= 3768 loss=  tensor(0.0003)\n",
      "epcho= 3769 loss=  tensor(0.0003)\n",
      "epcho= 3770 loss=  tensor(0.0003)\n",
      "epcho= 3771 loss=  tensor(0.0003)\n",
      "epcho= 3772 loss=  tensor(0.0003)\n",
      "epcho= 3773 loss=  tensor(0.0003)\n",
      "epcho= 3774 loss=  tensor(0.0003)\n",
      "epcho= 3775 loss=  tensor(0.0003)\n",
      "epcho= 3776 loss=  tensor(0.0003)\n",
      "epcho= 3777 loss=  tensor(0.0003)\n",
      "epcho= 3778 loss=  tensor(0.0003)\n",
      "epcho= 3779 loss=  tensor(0.0003)\n",
      "epcho= 3780 loss=  tensor(0.0003)\n",
      "epcho= 3781 loss=  tensor(0.0003)\n",
      "epcho= 3782 loss=  tensor(0.0003)\n",
      "epcho= 3783 loss=  tensor(0.0003)\n",
      "epcho= 3784 loss=  tensor(0.0003)\n",
      "epcho= 3785 loss=  tensor(0.0003)\n",
      "epcho= 3786 loss=  tensor(0.0003)\n",
      "epcho= 3787 loss=  tensor(0.0003)\n",
      "epcho= 3788 loss=  tensor(0.0003)\n",
      "epcho= 3789 loss=  tensor(0.0003)\n",
      "epcho= 3790 loss=  tensor(0.0003)\n",
      "epcho= 3791 loss=  tensor(0.0003)\n",
      "epcho= 3792 loss=  tensor(0.0003)\n",
      "epcho= 3793 loss=  tensor(0.0003)\n",
      "epcho= 3794 loss=  tensor(0.0003)\n",
      "epcho= 3795 loss=  tensor(0.0003)\n",
      "epcho= 3796 loss=  tensor(0.0003)\n",
      "epcho= 3797 loss=  tensor(0.0003)\n",
      "epcho= 3798 loss=  tensor(0.0003)\n",
      "epcho= 3799 loss=  tensor(0.0003)\n",
      "epcho= 3800 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1722, grad_fn=<DivBackward0>)\n",
      "epcho= 3801 loss=  tensor(0.0003)\n",
      "epcho= 3802 loss=  tensor(0.0003)\n",
      "epcho= 3803 loss=  tensor(0.0003)\n",
      "epcho= 3804 loss=  tensor(0.0003)\n",
      "epcho= 3805 loss=  tensor(0.0003)\n",
      "epcho= 3806 loss=  tensor(0.0003)\n",
      "epcho= 3807 loss=  tensor(0.0003)\n",
      "epcho= 3808 loss=  tensor(0.0003)\n",
      "epcho= 3809 loss=  tensor(0.0003)\n",
      "epcho= 3810 loss=  tensor(0.0003)\n",
      "epcho= 3811 loss=  tensor(0.0003)\n",
      "epcho= 3812 loss=  tensor(0.0003)\n",
      "epcho= 3813 loss=  tensor(0.0003)\n",
      "epcho= 3814 loss=  tensor(0.0003)\n",
      "epcho= 3815 loss=  tensor(0.0003)\n",
      "epcho= 3816 loss=  tensor(0.0003)\n",
      "epcho= 3817 loss=  tensor(0.0003)\n",
      "epcho= 3818 loss=  tensor(0.0003)\n",
      "epcho= 3819 loss=  tensor(0.0003)\n",
      "epcho= 3820 loss=  tensor(0.0003)\n",
      "epcho= 3821 loss=  tensor(0.0003)\n",
      "epcho= 3822 loss=  tensor(0.0003)\n",
      "epcho= 3823 loss=  tensor(0.0003)\n",
      "epcho= 3824 loss=  tensor(0.0003)\n",
      "epcho= 3825 loss=  tensor(0.0003)\n",
      "epcho= 3826 loss=  tensor(0.0003)\n",
      "epcho= 3827 loss=  tensor(0.0003)\n",
      "epcho= 3828 loss=  tensor(0.0003)\n",
      "epcho= 3829 loss=  tensor(0.0003)\n",
      "epcho= 3830 loss=  tensor(0.0003)\n",
      "epcho= 3831 loss=  tensor(0.0003)\n",
      "epcho= 3832 loss=  tensor(0.0003)\n",
      "epcho= 3833 loss=  tensor(0.0003)\n",
      "epcho= 3834 loss=  tensor(0.0003)\n",
      "epcho= 3835 loss=  tensor(0.0003)\n",
      "epcho= 3836 loss=  tensor(0.0003)\n",
      "epcho= 3837 loss=  tensor(0.0003)\n",
      "epcho= 3838 loss=  tensor(0.0003)\n",
      "epcho= 3839 loss=  tensor(0.0003)\n",
      "epcho= 3840 loss=  tensor(0.0003)\n",
      "epcho= 3841 loss=  tensor(0.0003)\n",
      "epcho= 3842 loss=  tensor(0.0003)\n",
      "epcho= 3843 loss=  tensor(0.0003)\n",
      "epcho= 3844 loss=  tensor(0.0003)\n",
      "epcho= 3845 loss=  tensor(0.0003)\n",
      "epcho= 3846 loss=  tensor(0.0003)\n",
      "epcho= 3847 loss=  tensor(0.0003)\n",
      "epcho= 3848 loss=  tensor(0.0003)\n",
      "epcho= 3849 loss=  tensor(0.0003)\n",
      "epcho= 3850 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1712, grad_fn=<DivBackward0>)\n",
      "epcho= 3851 loss=  tensor(0.0003)\n",
      "epcho= 3852 loss=  tensor(0.0003)\n",
      "epcho= 3853 loss=  tensor(0.0003)\n",
      "epcho= 3854 loss=  tensor(0.0003)\n",
      "epcho= 3855 loss=  tensor(0.0003)\n",
      "epcho= 3856 loss=  tensor(0.0003)\n",
      "epcho= 3857 loss=  tensor(0.0003)\n",
      "epcho= 3858 loss=  tensor(0.0003)\n",
      "epcho= 3859 loss=  tensor(0.0003)\n",
      "epcho= 3860 loss=  tensor(0.0003)\n",
      "epcho= 3861 loss=  tensor(0.0003)\n",
      "epcho= 3862 loss=  tensor(0.0003)\n",
      "epcho= 3863 loss=  tensor(0.0003)\n",
      "epcho= 3864 loss=  tensor(0.0003)\n",
      "epcho= 3865 loss=  tensor(0.0003)\n",
      "epcho= 3866 loss=  tensor(0.0003)\n",
      "epcho= 3867 loss=  tensor(0.0003)\n",
      "epcho= 3868 loss=  tensor(0.0003)\n",
      "epcho= 3869 loss=  tensor(0.0003)\n",
      "epcho= 3870 loss=  tensor(0.0003)\n",
      "epcho= 3871 loss=  tensor(0.0003)\n",
      "epcho= 3872 loss=  tensor(0.0003)\n",
      "epcho= 3873 loss=  tensor(0.0003)\n",
      "epcho= 3874 loss=  tensor(0.0003)\n",
      "epcho= 3875 loss=  tensor(0.0003)\n",
      "epcho= 3876 loss=  tensor(0.0003)\n",
      "epcho= 3877 loss=  tensor(0.0003)\n",
      "epcho= 3878 loss=  tensor(0.0003)\n",
      "epcho= 3879 loss=  tensor(0.0003)\n",
      "epcho= 3880 loss=  tensor(0.0003)\n",
      "epcho= 3881 loss=  tensor(0.0003)\n",
      "epcho= 3882 loss=  tensor(0.0003)\n",
      "epcho= 3883 loss=  tensor(0.0003)\n",
      "epcho= 3884 loss=  tensor(0.0003)\n",
      "epcho= 3885 loss=  tensor(0.0003)\n",
      "epcho= 3886 loss=  tensor(0.0003)\n",
      "epcho= 3887 loss=  tensor(0.0003)\n",
      "epcho= 3888 loss=  tensor(0.0003)\n",
      "epcho= 3889 loss=  tensor(0.0003)\n",
      "epcho= 3890 loss=  tensor(0.0003)\n",
      "epcho= 3891 loss=  tensor(0.0003)\n",
      "epcho= 3892 loss=  tensor(0.0003)\n",
      "epcho= 3893 loss=  tensor(0.0003)\n",
      "epcho= 3894 loss=  tensor(0.0003)\n",
      "epcho= 3895 loss=  tensor(0.0003)\n",
      "epcho= 3896 loss=  tensor(0.0003)\n",
      "epcho= 3897 loss=  tensor(0.0003)\n",
      "epcho= 3898 loss=  tensor(0.0003)\n",
      "epcho= 3899 loss=  tensor(0.0003)\n",
      "epcho= 3900 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1702, grad_fn=<DivBackward0>)\n",
      "epcho= 3901 loss=  tensor(0.0003)\n",
      "epcho= 3902 loss=  tensor(0.0003)\n",
      "epcho= 3903 loss=  tensor(0.0003)\n",
      "epcho= 3904 loss=  tensor(0.0003)\n",
      "epcho= 3905 loss=  tensor(0.0003)\n",
      "epcho= 3906 loss=  tensor(0.0003)\n",
      "epcho= 3907 loss=  tensor(0.0003)\n",
      "epcho= 3908 loss=  tensor(0.0003)\n",
      "epcho= 3909 loss=  tensor(0.0003)\n",
      "epcho= 3910 loss=  tensor(0.0003)\n",
      "epcho= 3911 loss=  tensor(0.0003)\n",
      "epcho= 3912 loss=  tensor(0.0003)\n",
      "epcho= 3913 loss=  tensor(0.0003)\n",
      "epcho= 3914 loss=  tensor(0.0003)\n",
      "epcho= 3915 loss=  tensor(0.0003)\n",
      "epcho= 3916 loss=  tensor(0.0003)\n",
      "epcho= 3917 loss=  tensor(0.0003)\n",
      "epcho= 3918 loss=  tensor(0.0003)\n",
      "epcho= 3919 loss=  tensor(0.0003)\n",
      "epcho= 3920 loss=  tensor(0.0003)\n",
      "epcho= 3921 loss=  tensor(0.0003)\n",
      "epcho= 3922 loss=  tensor(0.0003)\n",
      "epcho= 3923 loss=  tensor(0.0003)\n",
      "epcho= 3924 loss=  tensor(0.0003)\n",
      "epcho= 3925 loss=  tensor(0.0003)\n",
      "epcho= 3926 loss=  tensor(0.0003)\n",
      "epcho= 3927 loss=  tensor(0.0003)\n",
      "epcho= 3928 loss=  tensor(0.0003)\n",
      "epcho= 3929 loss=  tensor(0.0003)\n",
      "epcho= 3930 loss=  tensor(0.0003)\n",
      "epcho= 3931 loss=  tensor(0.0003)\n",
      "epcho= 3932 loss=  tensor(0.0003)\n",
      "epcho= 3933 loss=  tensor(0.0003)\n",
      "epcho= 3934 loss=  tensor(0.0003)\n",
      "epcho= 3935 loss=  tensor(0.0003)\n",
      "epcho= 3936 loss=  tensor(0.0003)\n",
      "epcho= 3937 loss=  tensor(0.0003)\n",
      "epcho= 3938 loss=  tensor(0.0003)\n",
      "epcho= 3939 loss=  tensor(0.0003)\n",
      "epcho= 3940 loss=  tensor(0.0003)\n",
      "epcho= 3941 loss=  tensor(0.0003)\n",
      "epcho= 3942 loss=  tensor(0.0003)\n",
      "epcho= 3943 loss=  tensor(0.0003)\n",
      "epcho= 3944 loss=  tensor(0.0003)\n",
      "epcho= 3945 loss=  tensor(0.0003)\n",
      "epcho= 3946 loss=  tensor(0.0003)\n",
      "epcho= 3947 loss=  tensor(0.0003)\n",
      "epcho= 3948 loss=  tensor(0.0003)\n",
      "epcho= 3949 loss=  tensor(0.0004)\n",
      "epcho= 3950 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1816, grad_fn=<DivBackward0>)\n",
      "epcho= 3951 loss=  tensor(0.0006)\n",
      "epcho= 3952 loss=  tensor(0.0007)\n",
      "epcho= 3953 loss=  tensor(0.0009)\n",
      "epcho= 3954 loss=  tensor(0.0011)\n",
      "epcho= 3955 loss=  tensor(0.0012)\n",
      "epcho= 3956 loss=  tensor(0.0014)\n",
      "epcho= 3957 loss=  tensor(0.0014)\n",
      "epcho= 3958 loss=  tensor(0.0013)\n",
      "epcho= 3959 loss=  tensor(0.0010)\n",
      "epcho= 3960 loss=  tensor(0.0007)\n",
      "epcho= 3961 loss=  tensor(0.0005)\n",
      "epcho= 3962 loss=  tensor(0.0003)\n",
      "epcho= 3963 loss=  tensor(0.0003)\n",
      "epcho= 3964 loss=  tensor(0.0003)\n",
      "epcho= 3965 loss=  tensor(0.0004)\n",
      "epcho= 3966 loss=  tensor(0.0005)\n",
      "epcho= 3967 loss=  tensor(0.0006)\n",
      "epcho= 3968 loss=  tensor(0.0006)\n",
      "epcho= 3969 loss=  tensor(0.0005)\n",
      "epcho= 3970 loss=  tensor(0.0004)\n",
      "epcho= 3971 loss=  tensor(0.0003)\n",
      "epcho= 3972 loss=  tensor(0.0003)\n",
      "epcho= 3973 loss=  tensor(0.0003)\n",
      "epcho= 3974 loss=  tensor(0.0003)\n",
      "epcho= 3975 loss=  tensor(0.0004)\n",
      "epcho= 3976 loss=  tensor(0.0004)\n",
      "epcho= 3977 loss=  tensor(0.0004)\n",
      "epcho= 3978 loss=  tensor(0.0003)\n",
      "epcho= 3979 loss=  tensor(0.0003)\n",
      "epcho= 3980 loss=  tensor(0.0003)\n",
      "epcho= 3981 loss=  tensor(0.0002)\n",
      "epcho= 3982 loss=  tensor(0.0003)\n",
      "epcho= 3983 loss=  tensor(0.0003)\n",
      "epcho= 3984 loss=  tensor(0.0003)\n",
      "epcho= 3985 loss=  tensor(0.0003)\n",
      "epcho= 3986 loss=  tensor(0.0003)\n",
      "epcho= 3987 loss=  tensor(0.0003)\n",
      "epcho= 3988 loss=  tensor(0.0003)\n",
      "epcho= 3989 loss=  tensor(0.0003)\n",
      "epcho= 3990 loss=  tensor(0.0003)\n",
      "epcho= 3991 loss=  tensor(0.0002)\n",
      "epcho= 3992 loss=  tensor(0.0003)\n",
      "epcho= 3993 loss=  tensor(0.0003)\n",
      "epcho= 3994 loss=  tensor(0.0003)\n",
      "epcho= 3995 loss=  tensor(0.0003)\n",
      "epcho= 3996 loss=  tensor(0.0003)\n",
      "epcho= 3997 loss=  tensor(0.0003)\n",
      "epcho= 3998 loss=  tensor(0.0003)\n",
      "epcho= 3999 loss=  tensor(0.0003)\n",
      "epcho= 4000 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1689, grad_fn=<DivBackward0>)\n",
      "epcho= 4001 loss=  tensor(0.0002)\n",
      "epcho= 4002 loss=  tensor(0.0002)\n",
      "epcho= 4003 loss=  tensor(0.0003)\n",
      "epcho= 4004 loss=  tensor(0.0003)\n",
      "epcho= 4005 loss=  tensor(0.0003)\n",
      "epcho= 4006 loss=  tensor(0.0003)\n",
      "epcho= 4007 loss=  tensor(0.0002)\n",
      "epcho= 4008 loss=  tensor(0.0002)\n",
      "epcho= 4009 loss=  tensor(0.0002)\n",
      "epcho= 4010 loss=  tensor(0.0002)\n",
      "epcho= 4011 loss=  tensor(0.0002)\n",
      "epcho= 4012 loss=  tensor(0.0002)\n",
      "epcho= 4013 loss=  tensor(0.0002)\n",
      "epcho= 4014 loss=  tensor(0.0002)\n",
      "epcho= 4015 loss=  tensor(0.0002)\n",
      "epcho= 4016 loss=  tensor(0.0002)\n",
      "epcho= 4017 loss=  tensor(0.0002)\n",
      "epcho= 4018 loss=  tensor(0.0002)\n",
      "epcho= 4019 loss=  tensor(0.0002)\n",
      "epcho= 4020 loss=  tensor(0.0002)\n",
      "epcho= 4021 loss=  tensor(0.0002)\n",
      "epcho= 4022 loss=  tensor(0.0002)\n",
      "epcho= 4023 loss=  tensor(0.0002)\n",
      "epcho= 4024 loss=  tensor(0.0002)\n",
      "epcho= 4025 loss=  tensor(0.0002)\n",
      "epcho= 4026 loss=  tensor(0.0002)\n",
      "epcho= 4027 loss=  tensor(0.0002)\n",
      "epcho= 4028 loss=  tensor(0.0002)\n",
      "epcho= 4029 loss=  tensor(0.0002)\n",
      "epcho= 4030 loss=  tensor(0.0002)\n",
      "epcho= 4031 loss=  tensor(0.0002)\n",
      "epcho= 4032 loss=  tensor(0.0002)\n",
      "epcho= 4033 loss=  tensor(0.0002)\n",
      "epcho= 4034 loss=  tensor(0.0002)\n",
      "epcho= 4035 loss=  tensor(0.0002)\n",
      "epcho= 4036 loss=  tensor(0.0002)\n",
      "epcho= 4037 loss=  tensor(0.0002)\n",
      "epcho= 4038 loss=  tensor(0.0002)\n",
      "epcho= 4039 loss=  tensor(0.0002)\n",
      "epcho= 4040 loss=  tensor(0.0002)\n",
      "epcho= 4041 loss=  tensor(0.0002)\n",
      "epcho= 4042 loss=  tensor(0.0002)\n",
      "epcho= 4043 loss=  tensor(0.0002)\n",
      "epcho= 4044 loss=  tensor(0.0002)\n",
      "epcho= 4045 loss=  tensor(0.0002)\n",
      "epcho= 4046 loss=  tensor(0.0002)\n",
      "epcho= 4047 loss=  tensor(0.0002)\n",
      "epcho= 4048 loss=  tensor(0.0002)\n",
      "epcho= 4049 loss=  tensor(0.0002)\n",
      "epcho= 4050 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1677, grad_fn=<DivBackward0>)\n",
      "epcho= 4051 loss=  tensor(0.0002)\n",
      "epcho= 4052 loss=  tensor(0.0002)\n",
      "epcho= 4053 loss=  tensor(0.0002)\n",
      "epcho= 4054 loss=  tensor(0.0002)\n",
      "epcho= 4055 loss=  tensor(0.0002)\n",
      "epcho= 4056 loss=  tensor(0.0002)\n",
      "epcho= 4057 loss=  tensor(0.0002)\n",
      "epcho= 4058 loss=  tensor(0.0002)\n",
      "epcho= 4059 loss=  tensor(0.0002)\n",
      "epcho= 4060 loss=  tensor(0.0002)\n",
      "epcho= 4061 loss=  tensor(0.0002)\n",
      "epcho= 4062 loss=  tensor(0.0002)\n",
      "epcho= 4063 loss=  tensor(0.0002)\n",
      "epcho= 4064 loss=  tensor(0.0002)\n",
      "epcho= 4065 loss=  tensor(0.0002)\n",
      "epcho= 4066 loss=  tensor(0.0002)\n",
      "epcho= 4067 loss=  tensor(0.0002)\n",
      "epcho= 4068 loss=  tensor(0.0002)\n",
      "epcho= 4069 loss=  tensor(0.0002)\n",
      "epcho= 4070 loss=  tensor(0.0002)\n",
      "epcho= 4071 loss=  tensor(0.0002)\n",
      "epcho= 4072 loss=  tensor(0.0002)\n",
      "epcho= 4073 loss=  tensor(0.0002)\n",
      "epcho= 4074 loss=  tensor(0.0002)\n",
      "epcho= 4075 loss=  tensor(0.0002)\n",
      "epcho= 4076 loss=  tensor(0.0002)\n",
      "epcho= 4077 loss=  tensor(0.0002)\n",
      "epcho= 4078 loss=  tensor(0.0002)\n",
      "epcho= 4079 loss=  tensor(0.0002)\n",
      "epcho= 4080 loss=  tensor(0.0002)\n",
      "epcho= 4081 loss=  tensor(0.0002)\n",
      "epcho= 4082 loss=  tensor(0.0002)\n",
      "epcho= 4083 loss=  tensor(0.0002)\n",
      "epcho= 4084 loss=  tensor(0.0002)\n",
      "epcho= 4085 loss=  tensor(0.0002)\n",
      "epcho= 4086 loss=  tensor(0.0002)\n",
      "epcho= 4087 loss=  tensor(0.0002)\n",
      "epcho= 4088 loss=  tensor(0.0002)\n",
      "epcho= 4089 loss=  tensor(0.0002)\n",
      "epcho= 4090 loss=  tensor(0.0002)\n",
      "epcho= 4091 loss=  tensor(0.0002)\n",
      "epcho= 4092 loss=  tensor(0.0002)\n",
      "epcho= 4093 loss=  tensor(0.0002)\n",
      "epcho= 4094 loss=  tensor(0.0002)\n",
      "epcho= 4095 loss=  tensor(0.0002)\n",
      "epcho= 4096 loss=  tensor(0.0002)\n",
      "epcho= 4097 loss=  tensor(0.0002)\n",
      "epcho= 4098 loss=  tensor(0.0002)\n",
      "epcho= 4099 loss=  tensor(0.0002)\n",
      "epcho= 4100 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1667, grad_fn=<DivBackward0>)\n",
      "epcho= 4101 loss=  tensor(0.0002)\n",
      "epcho= 4102 loss=  tensor(0.0002)\n",
      "epcho= 4103 loss=  tensor(0.0002)\n",
      "epcho= 4104 loss=  tensor(0.0002)\n",
      "epcho= 4105 loss=  tensor(0.0002)\n",
      "epcho= 4106 loss=  tensor(0.0002)\n",
      "epcho= 4107 loss=  tensor(0.0002)\n",
      "epcho= 4108 loss=  tensor(0.0002)\n",
      "epcho= 4109 loss=  tensor(0.0002)\n",
      "epcho= 4110 loss=  tensor(0.0002)\n",
      "epcho= 4111 loss=  tensor(0.0002)\n",
      "epcho= 4112 loss=  tensor(0.0002)\n",
      "epcho= 4113 loss=  tensor(0.0002)\n",
      "epcho= 4114 loss=  tensor(0.0002)\n",
      "epcho= 4115 loss=  tensor(0.0002)\n",
      "epcho= 4116 loss=  tensor(0.0002)\n",
      "epcho= 4117 loss=  tensor(0.0002)\n",
      "epcho= 4118 loss=  tensor(0.0002)\n",
      "epcho= 4119 loss=  tensor(0.0002)\n",
      "epcho= 4120 loss=  tensor(0.0002)\n",
      "epcho= 4121 loss=  tensor(0.0002)\n",
      "epcho= 4122 loss=  tensor(0.0002)\n",
      "epcho= 4123 loss=  tensor(0.0002)\n",
      "epcho= 4124 loss=  tensor(0.0002)\n",
      "epcho= 4125 loss=  tensor(0.0002)\n",
      "epcho= 4126 loss=  tensor(0.0002)\n",
      "epcho= 4127 loss=  tensor(0.0002)\n",
      "epcho= 4128 loss=  tensor(0.0002)\n",
      "epcho= 4129 loss=  tensor(0.0002)\n",
      "epcho= 4130 loss=  tensor(0.0002)\n",
      "epcho= 4131 loss=  tensor(0.0002)\n",
      "epcho= 4132 loss=  tensor(0.0002)\n",
      "epcho= 4133 loss=  tensor(0.0002)\n",
      "epcho= 4134 loss=  tensor(0.0002)\n",
      "epcho= 4135 loss=  tensor(0.0002)\n",
      "epcho= 4136 loss=  tensor(0.0002)\n",
      "epcho= 4137 loss=  tensor(0.0002)\n",
      "epcho= 4138 loss=  tensor(0.0002)\n",
      "epcho= 4139 loss=  tensor(0.0002)\n",
      "epcho= 4140 loss=  tensor(0.0002)\n",
      "epcho= 4141 loss=  tensor(0.0002)\n",
      "epcho= 4142 loss=  tensor(0.0002)\n",
      "epcho= 4143 loss=  tensor(0.0002)\n",
      "epcho= 4144 loss=  tensor(0.0002)\n",
      "epcho= 4145 loss=  tensor(0.0002)\n",
      "epcho= 4146 loss=  tensor(0.0002)\n",
      "epcho= 4147 loss=  tensor(0.0002)\n",
      "epcho= 4148 loss=  tensor(0.0002)\n",
      "epcho= 4149 loss=  tensor(0.0002)\n",
      "epcho= 4150 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1656, grad_fn=<DivBackward0>)\n",
      "epcho= 4151 loss=  tensor(0.0002)\n",
      "epcho= 4152 loss=  tensor(0.0002)\n",
      "epcho= 4153 loss=  tensor(0.0002)\n",
      "epcho= 4154 loss=  tensor(0.0002)\n",
      "epcho= 4155 loss=  tensor(0.0002)\n",
      "epcho= 4156 loss=  tensor(0.0002)\n",
      "epcho= 4157 loss=  tensor(0.0002)\n",
      "epcho= 4158 loss=  tensor(0.0002)\n",
      "epcho= 4159 loss=  tensor(0.0002)\n",
      "epcho= 4160 loss=  tensor(0.0002)\n",
      "epcho= 4161 loss=  tensor(0.0002)\n",
      "epcho= 4162 loss=  tensor(0.0002)\n",
      "epcho= 4163 loss=  tensor(0.0002)\n",
      "epcho= 4164 loss=  tensor(0.0002)\n",
      "epcho= 4165 loss=  tensor(0.0002)\n",
      "epcho= 4166 loss=  tensor(0.0002)\n",
      "epcho= 4167 loss=  tensor(0.0002)\n",
      "epcho= 4168 loss=  tensor(0.0002)\n",
      "epcho= 4169 loss=  tensor(0.0002)\n",
      "epcho= 4170 loss=  tensor(0.0002)\n",
      "epcho= 4171 loss=  tensor(0.0002)\n",
      "epcho= 4172 loss=  tensor(0.0002)\n",
      "epcho= 4173 loss=  tensor(0.0002)\n",
      "epcho= 4174 loss=  tensor(0.0002)\n",
      "epcho= 4175 loss=  tensor(0.0002)\n",
      "epcho= 4176 loss=  tensor(0.0002)\n",
      "epcho= 4177 loss=  tensor(0.0002)\n",
      "epcho= 4178 loss=  tensor(0.0002)\n",
      "epcho= 4179 loss=  tensor(0.0002)\n",
      "epcho= 4180 loss=  tensor(0.0002)\n",
      "epcho= 4181 loss=  tensor(0.0002)\n",
      "epcho= 4182 loss=  tensor(0.0002)\n",
      "epcho= 4183 loss=  tensor(0.0002)\n",
      "epcho= 4184 loss=  tensor(0.0002)\n",
      "epcho= 4185 loss=  tensor(0.0002)\n",
      "epcho= 4186 loss=  tensor(0.0002)\n",
      "epcho= 4187 loss=  tensor(0.0002)\n",
      "epcho= 4188 loss=  tensor(0.0002)\n",
      "epcho= 4189 loss=  tensor(0.0002)\n",
      "epcho= 4190 loss=  tensor(0.0002)\n",
      "epcho= 4191 loss=  tensor(0.0002)\n",
      "epcho= 4192 loss=  tensor(0.0002)\n",
      "epcho= 4193 loss=  tensor(0.0002)\n",
      "epcho= 4194 loss=  tensor(0.0002)\n",
      "epcho= 4195 loss=  tensor(0.0002)\n",
      "epcho= 4196 loss=  tensor(0.0003)\n",
      "epcho= 4197 loss=  tensor(0.0003)\n",
      "epcho= 4198 loss=  tensor(0.0003)\n",
      "epcho= 4199 loss=  tensor(0.0004)\n",
      "epcho= 4200 loss=  tensor(0.0005)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>) tensor(0.1534, grad_fn=<DivBackward0>)\n",
      "epcho= 4201 loss=  tensor(0.0006)\n",
      "epcho= 4202 loss=  tensor(0.0007)\n",
      "epcho= 4203 loss=  tensor(0.0008)\n",
      "epcho= 4204 loss=  tensor(0.0009)\n",
      "epcho= 4205 loss=  tensor(0.0010)\n",
      "epcho= 4206 loss=  tensor(0.0010)\n",
      "epcho= 4207 loss=  tensor(0.0009)\n",
      "epcho= 4208 loss=  tensor(0.0008)\n",
      "epcho= 4209 loss=  tensor(0.0006)\n",
      "epcho= 4210 loss=  tensor(0.0004)\n",
      "epcho= 4211 loss=  tensor(0.0003)\n",
      "epcho= 4212 loss=  tensor(0.0002)\n",
      "epcho= 4213 loss=  tensor(0.0002)\n",
      "epcho= 4214 loss=  tensor(0.0003)\n",
      "epcho= 4215 loss=  tensor(0.0004)\n",
      "epcho= 4216 loss=  tensor(0.0004)\n",
      "epcho= 4217 loss=  tensor(0.0005)\n",
      "epcho= 4218 loss=  tensor(0.0004)\n",
      "epcho= 4219 loss=  tensor(0.0004)\n",
      "epcho= 4220 loss=  tensor(0.0003)\n",
      "epcho= 4221 loss=  tensor(0.0003)\n",
      "epcho= 4222 loss=  tensor(0.0002)\n",
      "epcho= 4223 loss=  tensor(0.0002)\n",
      "epcho= 4224 loss=  tensor(0.0003)\n",
      "epcho= 4225 loss=  tensor(0.0003)\n",
      "epcho= 4226 loss=  tensor(0.0003)\n",
      "epcho= 4227 loss=  tensor(0.0003)\n",
      "epcho= 4228 loss=  tensor(0.0003)\n",
      "epcho= 4229 loss=  tensor(0.0003)\n",
      "epcho= 4230 loss=  tensor(0.0003)\n",
      "epcho= 4231 loss=  tensor(0.0002)\n",
      "epcho= 4232 loss=  tensor(0.0002)\n",
      "epcho= 4233 loss=  tensor(0.0002)\n",
      "epcho= 4234 loss=  tensor(0.0002)\n",
      "epcho= 4235 loss=  tensor(0.0002)\n",
      "epcho= 4236 loss=  tensor(0.0002)\n",
      "epcho= 4237 loss=  tensor(0.0003)\n",
      "epcho= 4238 loss=  tensor(0.0003)\n",
      "epcho= 4239 loss=  tensor(0.0003)\n",
      "epcho= 4240 loss=  tensor(0.0002)\n",
      "epcho= 4241 loss=  tensor(0.0002)\n",
      "epcho= 4242 loss=  tensor(0.0002)\n",
      "epcho= 4243 loss=  tensor(0.0002)\n",
      "epcho= 4244 loss=  tensor(0.0002)\n",
      "epcho= 4245 loss=  tensor(0.0002)\n",
      "epcho= 4246 loss=  tensor(0.0002)\n",
      "epcho= 4247 loss=  tensor(0.0002)\n",
      "epcho= 4248 loss=  tensor(0.0002)\n",
      "epcho= 4249 loss=  tensor(0.0002)\n",
      "epcho= 4250 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1621, grad_fn=<DivBackward0>)\n",
      "epcho= 4251 loss=  tensor(0.0002)\n",
      "epcho= 4252 loss=  tensor(0.0002)\n",
      "epcho= 4253 loss=  tensor(0.0002)\n",
      "epcho= 4254 loss=  tensor(0.0002)\n",
      "epcho= 4255 loss=  tensor(0.0002)\n",
      "epcho= 4256 loss=  tensor(0.0002)\n",
      "epcho= 4257 loss=  tensor(0.0002)\n",
      "epcho= 4258 loss=  tensor(0.0002)\n",
      "epcho= 4259 loss=  tensor(0.0002)\n",
      "epcho= 4260 loss=  tensor(0.0002)\n",
      "epcho= 4261 loss=  tensor(0.0002)\n",
      "epcho= 4262 loss=  tensor(0.0002)\n",
      "epcho= 4263 loss=  tensor(0.0002)\n",
      "epcho= 4264 loss=  tensor(0.0002)\n",
      "epcho= 4265 loss=  tensor(0.0002)\n",
      "epcho= 4266 loss=  tensor(0.0002)\n",
      "epcho= 4267 loss=  tensor(0.0002)\n",
      "epcho= 4268 loss=  tensor(0.0002)\n",
      "epcho= 4269 loss=  tensor(0.0002)\n",
      "epcho= 4270 loss=  tensor(0.0002)\n",
      "epcho= 4271 loss=  tensor(0.0002)\n",
      "epcho= 4272 loss=  tensor(0.0002)\n",
      "epcho= 4273 loss=  tensor(0.0002)\n",
      "epcho= 4274 loss=  tensor(0.0002)\n",
      "epcho= 4275 loss=  tensor(0.0002)\n",
      "epcho= 4276 loss=  tensor(0.0002)\n",
      "epcho= 4277 loss=  tensor(0.0002)\n",
      "epcho= 4278 loss=  tensor(0.0002)\n",
      "epcho= 4279 loss=  tensor(0.0002)\n",
      "epcho= 4280 loss=  tensor(0.0002)\n",
      "epcho= 4281 loss=  tensor(0.0002)\n",
      "epcho= 4282 loss=  tensor(0.0002)\n",
      "epcho= 4283 loss=  tensor(0.0002)\n",
      "epcho= 4284 loss=  tensor(0.0002)\n",
      "epcho= 4285 loss=  tensor(0.0002)\n",
      "epcho= 4286 loss=  tensor(0.0002)\n",
      "epcho= 4287 loss=  tensor(0.0002)\n",
      "epcho= 4288 loss=  tensor(0.0002)\n",
      "epcho= 4289 loss=  tensor(0.0002)\n",
      "epcho= 4290 loss=  tensor(0.0002)\n",
      "epcho= 4291 loss=  tensor(0.0002)\n",
      "epcho= 4292 loss=  tensor(0.0002)\n",
      "epcho= 4293 loss=  tensor(0.0002)\n",
      "epcho= 4294 loss=  tensor(0.0002)\n",
      "epcho= 4295 loss=  tensor(0.0002)\n",
      "epcho= 4296 loss=  tensor(0.0002)\n",
      "epcho= 4297 loss=  tensor(0.0002)\n",
      "epcho= 4298 loss=  tensor(0.0002)\n",
      "epcho= 4299 loss=  tensor(0.0002)\n",
      "epcho= 4300 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1627, grad_fn=<DivBackward0>)\n",
      "epcho= 4301 loss=  tensor(0.0002)\n",
      "epcho= 4302 loss=  tensor(0.0002)\n",
      "epcho= 4303 loss=  tensor(0.0002)\n",
      "epcho= 4304 loss=  tensor(0.0002)\n",
      "epcho= 4305 loss=  tensor(0.0002)\n",
      "epcho= 4306 loss=  tensor(0.0002)\n",
      "epcho= 4307 loss=  tensor(0.0002)\n",
      "epcho= 4308 loss=  tensor(0.0002)\n",
      "epcho= 4309 loss=  tensor(0.0002)\n",
      "epcho= 4310 loss=  tensor(0.0002)\n",
      "epcho= 4311 loss=  tensor(0.0002)\n",
      "epcho= 4312 loss=  tensor(0.0002)\n",
      "epcho= 4313 loss=  tensor(0.0002)\n",
      "epcho= 4314 loss=  tensor(0.0002)\n",
      "epcho= 4315 loss=  tensor(0.0002)\n",
      "epcho= 4316 loss=  tensor(0.0002)\n",
      "epcho= 4317 loss=  tensor(0.0002)\n",
      "epcho= 4318 loss=  tensor(0.0002)\n",
      "epcho= 4319 loss=  tensor(0.0002)\n",
      "epcho= 4320 loss=  tensor(0.0002)\n",
      "epcho= 4321 loss=  tensor(0.0002)\n",
      "epcho= 4322 loss=  tensor(0.0002)\n",
      "epcho= 4323 loss=  tensor(0.0002)\n",
      "epcho= 4324 loss=  tensor(0.0002)\n",
      "epcho= 4325 loss=  tensor(0.0002)\n",
      "epcho= 4326 loss=  tensor(0.0002)\n",
      "epcho= 4327 loss=  tensor(0.0002)\n",
      "epcho= 4328 loss=  tensor(0.0002)\n",
      "epcho= 4329 loss=  tensor(0.0002)\n",
      "epcho= 4330 loss=  tensor(0.0002)\n",
      "epcho= 4331 loss=  tensor(0.0002)\n",
      "epcho= 4332 loss=  tensor(0.0002)\n",
      "epcho= 4333 loss=  tensor(0.0002)\n",
      "epcho= 4334 loss=  tensor(0.0002)\n",
      "epcho= 4335 loss=  tensor(0.0002)\n",
      "epcho= 4336 loss=  tensor(0.0002)\n",
      "epcho= 4337 loss=  tensor(0.0002)\n",
      "epcho= 4338 loss=  tensor(0.0002)\n",
      "epcho= 4339 loss=  tensor(0.0002)\n",
      "epcho= 4340 loss=  tensor(0.0002)\n",
      "epcho= 4341 loss=  tensor(0.0002)\n",
      "epcho= 4342 loss=  tensor(0.0002)\n",
      "epcho= 4343 loss=  tensor(0.0002)\n",
      "epcho= 4344 loss=  tensor(0.0002)\n",
      "epcho= 4345 loss=  tensor(0.0002)\n",
      "epcho= 4346 loss=  tensor(0.0002)\n",
      "epcho= 4347 loss=  tensor(0.0002)\n",
      "epcho= 4348 loss=  tensor(0.0002)\n",
      "epcho= 4349 loss=  tensor(0.0002)\n",
      "epcho= 4350 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1618, grad_fn=<DivBackward0>)\n",
      "epcho= 4351 loss=  tensor(0.0002)\n",
      "epcho= 4352 loss=  tensor(0.0002)\n",
      "epcho= 4353 loss=  tensor(0.0002)\n",
      "epcho= 4354 loss=  tensor(0.0002)\n",
      "epcho= 4355 loss=  tensor(0.0002)\n",
      "epcho= 4356 loss=  tensor(0.0002)\n",
      "epcho= 4357 loss=  tensor(0.0002)\n",
      "epcho= 4358 loss=  tensor(0.0002)\n",
      "epcho= 4359 loss=  tensor(0.0002)\n",
      "epcho= 4360 loss=  tensor(0.0002)\n",
      "epcho= 4361 loss=  tensor(0.0002)\n",
      "epcho= 4362 loss=  tensor(0.0002)\n",
      "epcho= 4363 loss=  tensor(0.0002)\n",
      "epcho= 4364 loss=  tensor(0.0002)\n",
      "epcho= 4365 loss=  tensor(0.0002)\n",
      "epcho= 4366 loss=  tensor(0.0002)\n",
      "epcho= 4367 loss=  tensor(0.0002)\n",
      "epcho= 4368 loss=  tensor(0.0002)\n",
      "epcho= 4369 loss=  tensor(0.0002)\n",
      "epcho= 4370 loss=  tensor(0.0002)\n",
      "epcho= 4371 loss=  tensor(0.0002)\n",
      "epcho= 4372 loss=  tensor(0.0002)\n",
      "epcho= 4373 loss=  tensor(0.0002)\n",
      "epcho= 4374 loss=  tensor(0.0002)\n",
      "epcho= 4375 loss=  tensor(0.0002)\n",
      "epcho= 4376 loss=  tensor(0.0002)\n",
      "epcho= 4377 loss=  tensor(0.0002)\n",
      "epcho= 4378 loss=  tensor(0.0002)\n",
      "epcho= 4379 loss=  tensor(0.0002)\n",
      "epcho= 4380 loss=  tensor(0.0002)\n",
      "epcho= 4381 loss=  tensor(0.0002)\n",
      "epcho= 4382 loss=  tensor(0.0002)\n",
      "epcho= 4383 loss=  tensor(0.0002)\n",
      "epcho= 4384 loss=  tensor(0.0002)\n",
      "epcho= 4385 loss=  tensor(0.0002)\n",
      "epcho= 4386 loss=  tensor(0.0002)\n",
      "epcho= 4387 loss=  tensor(0.0002)\n",
      "epcho= 4388 loss=  tensor(0.0002)\n",
      "epcho= 4389 loss=  tensor(0.0002)\n",
      "epcho= 4390 loss=  tensor(0.0002)\n",
      "epcho= 4391 loss=  tensor(0.0002)\n",
      "epcho= 4392 loss=  tensor(0.0002)\n",
      "epcho= 4393 loss=  tensor(0.0002)\n",
      "epcho= 4394 loss=  tensor(0.0002)\n",
      "epcho= 4395 loss=  tensor(0.0002)\n",
      "epcho= 4396 loss=  tensor(0.0002)\n",
      "epcho= 4397 loss=  tensor(0.0002)\n",
      "epcho= 4398 loss=  tensor(0.0002)\n",
      "epcho= 4399 loss=  tensor(0.0002)\n",
      "epcho= 4400 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1615, grad_fn=<DivBackward0>)\n",
      "epcho= 4401 loss=  tensor(0.0002)\n",
      "epcho= 4402 loss=  tensor(0.0002)\n",
      "epcho= 4403 loss=  tensor(0.0002)\n",
      "epcho= 4404 loss=  tensor(0.0002)\n",
      "epcho= 4405 loss=  tensor(0.0002)\n",
      "epcho= 4406 loss=  tensor(0.0002)\n",
      "epcho= 4407 loss=  tensor(0.0002)\n",
      "epcho= 4408 loss=  tensor(0.0002)\n",
      "epcho= 4409 loss=  tensor(0.0002)\n",
      "epcho= 4410 loss=  tensor(0.0002)\n",
      "epcho= 4411 loss=  tensor(0.0002)\n",
      "epcho= 4412 loss=  tensor(0.0002)\n",
      "epcho= 4413 loss=  tensor(0.0002)\n",
      "epcho= 4414 loss=  tensor(0.0002)\n",
      "epcho= 4415 loss=  tensor(0.0002)\n",
      "epcho= 4416 loss=  tensor(0.0002)\n",
      "epcho= 4417 loss=  tensor(0.0002)\n",
      "epcho= 4418 loss=  tensor(0.0003)\n",
      "epcho= 4419 loss=  tensor(0.0003)\n",
      "epcho= 4420 loss=  tensor(0.0003)\n",
      "epcho= 4421 loss=  tensor(0.0003)\n",
      "epcho= 4422 loss=  tensor(0.0003)\n",
      "epcho= 4423 loss=  tensor(0.0004)\n",
      "epcho= 4424 loss=  tensor(0.0004)\n",
      "epcho= 4425 loss=  tensor(0.0005)\n",
      "epcho= 4426 loss=  tensor(0.0006)\n",
      "epcho= 4427 loss=  tensor(0.0007)\n",
      "epcho= 4428 loss=  tensor(0.0009)\n",
      "epcho= 4429 loss=  tensor(0.0010)\n",
      "epcho= 4430 loss=  tensor(0.0011)\n",
      "epcho= 4431 loss=  tensor(0.0012)\n",
      "epcho= 4432 loss=  tensor(0.0012)\n",
      "epcho= 4433 loss=  tensor(0.0010)\n",
      "epcho= 4434 loss=  tensor(0.0008)\n",
      "epcho= 4435 loss=  tensor(0.0005)\n",
      "epcho= 4436 loss=  tensor(0.0003)\n",
      "epcho= 4437 loss=  tensor(0.0002)\n",
      "epcho= 4438 loss=  tensor(0.0002)\n",
      "epcho= 4439 loss=  tensor(0.0003)\n",
      "epcho= 4440 loss=  tensor(0.0005)\n",
      "epcho= 4441 loss=  tensor(0.0005)\n",
      "epcho= 4442 loss=  tensor(0.0005)\n",
      "epcho= 4443 loss=  tensor(0.0004)\n",
      "epcho= 4444 loss=  tensor(0.0003)\n",
      "epcho= 4445 loss=  tensor(0.0002)\n",
      "epcho= 4446 loss=  tensor(0.0002)\n",
      "epcho= 4447 loss=  tensor(0.0002)\n",
      "epcho= 4448 loss=  tensor(0.0003)\n",
      "epcho= 4449 loss=  tensor(0.0003)\n",
      "epcho= 4450 loss=  tensor(0.0003)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>) tensor(0.1670, grad_fn=<DivBackward0>)\n",
      "epcho= 4451 loss=  tensor(0.0003)\n",
      "epcho= 4452 loss=  tensor(0.0003)\n",
      "epcho= 4453 loss=  tensor(0.0002)\n",
      "epcho= 4454 loss=  tensor(0.0002)\n",
      "epcho= 4455 loss=  tensor(0.0002)\n",
      "epcho= 4456 loss=  tensor(0.0003)\n",
      "epcho= 4457 loss=  tensor(0.0003)\n",
      "epcho= 4458 loss=  tensor(0.0003)\n",
      "epcho= 4459 loss=  tensor(0.0002)\n",
      "epcho= 4460 loss=  tensor(0.0002)\n",
      "epcho= 4461 loss=  tensor(0.0002)\n",
      "epcho= 4462 loss=  tensor(0.0002)\n",
      "epcho= 4463 loss=  tensor(0.0002)\n",
      "epcho= 4464 loss=  tensor(0.0002)\n",
      "epcho= 4465 loss=  tensor(0.0002)\n",
      "epcho= 4466 loss=  tensor(0.0002)\n",
      "epcho= 4467 loss=  tensor(0.0002)\n",
      "epcho= 4468 loss=  tensor(0.0002)\n",
      "epcho= 4469 loss=  tensor(0.0002)\n",
      "epcho= 4470 loss=  tensor(0.0002)\n",
      "epcho= 4471 loss=  tensor(0.0002)\n",
      "epcho= 4472 loss=  tensor(0.0002)\n",
      "epcho= 4473 loss=  tensor(0.0002)\n",
      "epcho= 4474 loss=  tensor(0.0002)\n",
      "epcho= 4475 loss=  tensor(0.0002)\n",
      "epcho= 4476 loss=  tensor(0.0002)\n",
      "epcho= 4477 loss=  tensor(0.0002)\n",
      "epcho= 4478 loss=  tensor(0.0002)\n",
      "epcho= 4479 loss=  tensor(0.0002)\n",
      "epcho= 4480 loss=  tensor(0.0002)\n",
      "epcho= 4481 loss=  tensor(0.0002)\n",
      "epcho= 4482 loss=  tensor(0.0002)\n",
      "epcho= 4483 loss=  tensor(0.0002)\n",
      "epcho= 4484 loss=  tensor(0.0002)\n",
      "epcho= 4485 loss=  tensor(0.0002)\n",
      "epcho= 4486 loss=  tensor(0.0002)\n",
      "epcho= 4487 loss=  tensor(0.0002)\n",
      "epcho= 4488 loss=  tensor(0.0002)\n",
      "epcho= 4489 loss=  tensor(0.0002)\n",
      "epcho= 4490 loss=  tensor(0.0002)\n",
      "epcho= 4491 loss=  tensor(0.0002)\n",
      "epcho= 4492 loss=  tensor(0.0002)\n",
      "epcho= 4493 loss=  tensor(0.0002)\n",
      "epcho= 4494 loss=  tensor(0.0002)\n",
      "epcho= 4495 loss=  tensor(0.0002)\n",
      "epcho= 4496 loss=  tensor(0.0002)\n",
      "epcho= 4497 loss=  tensor(0.0002)\n",
      "epcho= 4498 loss=  tensor(0.0002)\n",
      "epcho= 4499 loss=  tensor(0.0002)\n",
      "epcho= 4500 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1591, grad_fn=<DivBackward0>)\n",
      "epcho= 4501 loss=  tensor(0.0002)\n",
      "epcho= 4502 loss=  tensor(0.0002)\n",
      "epcho= 4503 loss=  tensor(0.0002)\n",
      "epcho= 4504 loss=  tensor(0.0002)\n",
      "epcho= 4505 loss=  tensor(0.0002)\n",
      "epcho= 4506 loss=  tensor(0.0002)\n",
      "epcho= 4507 loss=  tensor(0.0002)\n",
      "epcho= 4508 loss=  tensor(0.0002)\n",
      "epcho= 4509 loss=  tensor(0.0002)\n",
      "epcho= 4510 loss=  tensor(0.0002)\n",
      "epcho= 4511 loss=  tensor(0.0002)\n",
      "epcho= 4512 loss=  tensor(0.0002)\n",
      "epcho= 4513 loss=  tensor(0.0002)\n",
      "epcho= 4514 loss=  tensor(0.0002)\n",
      "epcho= 4515 loss=  tensor(0.0002)\n",
      "epcho= 4516 loss=  tensor(0.0002)\n",
      "epcho= 4517 loss=  tensor(0.0002)\n",
      "epcho= 4518 loss=  tensor(0.0002)\n",
      "epcho= 4519 loss=  tensor(0.0002)\n",
      "epcho= 4520 loss=  tensor(0.0002)\n",
      "epcho= 4521 loss=  tensor(0.0002)\n",
      "epcho= 4522 loss=  tensor(0.0002)\n",
      "epcho= 4523 loss=  tensor(0.0002)\n",
      "epcho= 4524 loss=  tensor(0.0002)\n",
      "epcho= 4525 loss=  tensor(0.0002)\n",
      "epcho= 4526 loss=  tensor(0.0002)\n",
      "epcho= 4527 loss=  tensor(0.0002)\n",
      "epcho= 4528 loss=  tensor(0.0002)\n",
      "epcho= 4529 loss=  tensor(0.0002)\n",
      "epcho= 4530 loss=  tensor(0.0002)\n",
      "epcho= 4531 loss=  tensor(0.0002)\n",
      "epcho= 4532 loss=  tensor(0.0002)\n",
      "epcho= 4533 loss=  tensor(0.0002)\n",
      "epcho= 4534 loss=  tensor(0.0002)\n",
      "epcho= 4535 loss=  tensor(0.0002)\n",
      "epcho= 4536 loss=  tensor(0.0002)\n",
      "epcho= 4537 loss=  tensor(0.0002)\n",
      "epcho= 4538 loss=  tensor(0.0002)\n",
      "epcho= 4539 loss=  tensor(0.0002)\n",
      "epcho= 4540 loss=  tensor(0.0002)\n",
      "epcho= 4541 loss=  tensor(0.0002)\n",
      "epcho= 4542 loss=  tensor(0.0002)\n",
      "epcho= 4543 loss=  tensor(0.0002)\n",
      "epcho= 4544 loss=  tensor(0.0002)\n",
      "epcho= 4545 loss=  tensor(0.0002)\n",
      "epcho= 4546 loss=  tensor(0.0002)\n",
      "epcho= 4547 loss=  tensor(0.0002)\n",
      "epcho= 4548 loss=  tensor(0.0002)\n",
      "epcho= 4549 loss=  tensor(0.0002)\n",
      "epcho= 4550 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1581, grad_fn=<DivBackward0>)\n",
      "epcho= 4551 loss=  tensor(0.0002)\n",
      "epcho= 4552 loss=  tensor(0.0002)\n",
      "epcho= 4553 loss=  tensor(0.0002)\n",
      "epcho= 4554 loss=  tensor(0.0002)\n",
      "epcho= 4555 loss=  tensor(0.0002)\n",
      "epcho= 4556 loss=  tensor(0.0002)\n",
      "epcho= 4557 loss=  tensor(0.0002)\n",
      "epcho= 4558 loss=  tensor(0.0002)\n",
      "epcho= 4559 loss=  tensor(0.0002)\n",
      "epcho= 4560 loss=  tensor(0.0002)\n",
      "epcho= 4561 loss=  tensor(0.0002)\n",
      "epcho= 4562 loss=  tensor(0.0002)\n",
      "epcho= 4563 loss=  tensor(0.0002)\n",
      "epcho= 4564 loss=  tensor(0.0002)\n",
      "epcho= 4565 loss=  tensor(0.0002)\n",
      "epcho= 4566 loss=  tensor(0.0002)\n",
      "epcho= 4567 loss=  tensor(0.0002)\n",
      "epcho= 4568 loss=  tensor(0.0002)\n",
      "epcho= 4569 loss=  tensor(0.0002)\n",
      "epcho= 4570 loss=  tensor(0.0002)\n",
      "epcho= 4571 loss=  tensor(0.0002)\n",
      "epcho= 4572 loss=  tensor(0.0002)\n",
      "epcho= 4573 loss=  tensor(0.0002)\n",
      "epcho= 4574 loss=  tensor(0.0002)\n",
      "epcho= 4575 loss=  tensor(0.0002)\n",
      "epcho= 4576 loss=  tensor(0.0002)\n",
      "epcho= 4577 loss=  tensor(0.0002)\n",
      "epcho= 4578 loss=  tensor(0.0002)\n",
      "epcho= 4579 loss=  tensor(0.0002)\n",
      "epcho= 4580 loss=  tensor(0.0002)\n",
      "epcho= 4581 loss=  tensor(0.0002)\n",
      "epcho= 4582 loss=  tensor(0.0002)\n",
      "epcho= 4583 loss=  tensor(0.0002)\n",
      "epcho= 4584 loss=  tensor(0.0002)\n",
      "epcho= 4585 loss=  tensor(0.0002)\n",
      "epcho= 4586 loss=  tensor(0.0002)\n",
      "epcho= 4587 loss=  tensor(0.0002)\n",
      "epcho= 4588 loss=  tensor(0.0002)\n",
      "epcho= 4589 loss=  tensor(0.0002)\n",
      "epcho= 4590 loss=  tensor(0.0002)\n",
      "epcho= 4591 loss=  tensor(0.0002)\n",
      "epcho= 4592 loss=  tensor(0.0002)\n",
      "epcho= 4593 loss=  tensor(0.0002)\n",
      "epcho= 4594 loss=  tensor(0.0002)\n",
      "epcho= 4595 loss=  tensor(0.0002)\n",
      "epcho= 4596 loss=  tensor(0.0002)\n",
      "epcho= 4597 loss=  tensor(0.0002)\n",
      "epcho= 4598 loss=  tensor(0.0002)\n",
      "epcho= 4599 loss=  tensor(0.0002)\n",
      "epcho= 4600 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1570, grad_fn=<DivBackward0>)\n",
      "epcho= 4601 loss=  tensor(0.0002)\n",
      "epcho= 4602 loss=  tensor(0.0002)\n",
      "epcho= 4603 loss=  tensor(0.0002)\n",
      "epcho= 4604 loss=  tensor(0.0002)\n",
      "epcho= 4605 loss=  tensor(0.0002)\n",
      "epcho= 4606 loss=  tensor(0.0002)\n",
      "epcho= 4607 loss=  tensor(0.0002)\n",
      "epcho= 4608 loss=  tensor(0.0002)\n",
      "epcho= 4609 loss=  tensor(0.0002)\n",
      "epcho= 4610 loss=  tensor(0.0002)\n",
      "epcho= 4611 loss=  tensor(0.0002)\n",
      "epcho= 4612 loss=  tensor(0.0002)\n",
      "epcho= 4613 loss=  tensor(0.0002)\n",
      "epcho= 4614 loss=  tensor(0.0002)\n",
      "epcho= 4615 loss=  tensor(0.0002)\n",
      "epcho= 4616 loss=  tensor(0.0002)\n",
      "epcho= 4617 loss=  tensor(0.0002)\n",
      "epcho= 4618 loss=  tensor(0.0002)\n",
      "epcho= 4619 loss=  tensor(0.0002)\n",
      "epcho= 4620 loss=  tensor(0.0002)\n",
      "epcho= 4621 loss=  tensor(0.0002)\n",
      "epcho= 4622 loss=  tensor(0.0002)\n",
      "epcho= 4623 loss=  tensor(0.0002)\n",
      "epcho= 4624 loss=  tensor(0.0002)\n",
      "epcho= 4625 loss=  tensor(0.0002)\n",
      "epcho= 4626 loss=  tensor(0.0002)\n",
      "epcho= 4627 loss=  tensor(0.0002)\n",
      "epcho= 4628 loss=  tensor(0.0002)\n",
      "epcho= 4629 loss=  tensor(0.0002)\n",
      "epcho= 4630 loss=  tensor(0.0002)\n",
      "epcho= 4631 loss=  tensor(0.0002)\n",
      "epcho= 4632 loss=  tensor(0.0002)\n",
      "epcho= 4633 loss=  tensor(0.0002)\n",
      "epcho= 4634 loss=  tensor(0.0002)\n",
      "epcho= 4635 loss=  tensor(0.0002)\n",
      "epcho= 4636 loss=  tensor(0.0002)\n",
      "epcho= 4637 loss=  tensor(0.0002)\n",
      "epcho= 4638 loss=  tensor(0.0002)\n",
      "epcho= 4639 loss=  tensor(0.0002)\n",
      "epcho= 4640 loss=  tensor(0.0002)\n",
      "epcho= 4641 loss=  tensor(0.0002)\n",
      "epcho= 4642 loss=  tensor(0.0002)\n",
      "epcho= 4643 loss=  tensor(0.0002)\n",
      "epcho= 4644 loss=  tensor(0.0002)\n",
      "epcho= 4645 loss=  tensor(0.0002)\n",
      "epcho= 4646 loss=  tensor(0.0002)\n",
      "epcho= 4647 loss=  tensor(0.0002)\n",
      "epcho= 4648 loss=  tensor(0.0002)\n",
      "epcho= 4649 loss=  tensor(0.0002)\n",
      "epcho= 4650 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1559, grad_fn=<DivBackward0>)\n",
      "epcho= 4651 loss=  tensor(0.0002)\n",
      "epcho= 4652 loss=  tensor(0.0002)\n",
      "epcho= 4653 loss=  tensor(0.0002)\n",
      "epcho= 4654 loss=  tensor(0.0002)\n",
      "epcho= 4655 loss=  tensor(0.0002)\n",
      "epcho= 4656 loss=  tensor(0.0002)\n",
      "epcho= 4657 loss=  tensor(0.0002)\n",
      "epcho= 4658 loss=  tensor(0.0002)\n",
      "epcho= 4659 loss=  tensor(0.0002)\n",
      "epcho= 4660 loss=  tensor(0.0002)\n",
      "epcho= 4661 loss=  tensor(0.0002)\n",
      "epcho= 4662 loss=  tensor(0.0002)\n",
      "epcho= 4663 loss=  tensor(0.0002)\n",
      "epcho= 4664 loss=  tensor(0.0002)\n",
      "epcho= 4665 loss=  tensor(0.0002)\n",
      "epcho= 4666 loss=  tensor(0.0002)\n",
      "epcho= 4667 loss=  tensor(0.0002)\n",
      "epcho= 4668 loss=  tensor(0.0002)\n",
      "epcho= 4669 loss=  tensor(0.0002)\n",
      "epcho= 4670 loss=  tensor(0.0002)\n",
      "epcho= 4671 loss=  tensor(0.0002)\n",
      "epcho= 4672 loss=  tensor(0.0002)\n",
      "epcho= 4673 loss=  tensor(0.0002)\n",
      "epcho= 4674 loss=  tensor(0.0002)\n",
      "epcho= 4675 loss=  tensor(0.0002)\n",
      "epcho= 4676 loss=  tensor(0.0002)\n",
      "epcho= 4677 loss=  tensor(0.0002)\n",
      "epcho= 4678 loss=  tensor(0.0002)\n",
      "epcho= 4679 loss=  tensor(0.0002)\n",
      "epcho= 4680 loss=  tensor(0.0002)\n",
      "epcho= 4681 loss=  tensor(0.0002)\n",
      "epcho= 4682 loss=  tensor(0.0002)\n",
      "epcho= 4683 loss=  tensor(0.0002)\n",
      "epcho= 4684 loss=  tensor(0.0002)\n",
      "epcho= 4685 loss=  tensor(0.0002)\n",
      "epcho= 4686 loss=  tensor(0.0002)\n",
      "epcho= 4687 loss=  tensor(0.0002)\n",
      "epcho= 4688 loss=  tensor(0.0002)\n",
      "epcho= 4689 loss=  tensor(0.0002)\n",
      "epcho= 4690 loss=  tensor(0.0002)\n",
      "epcho= 4691 loss=  tensor(0.0002)\n",
      "epcho= 4692 loss=  tensor(0.0002)\n",
      "epcho= 4693 loss=  tensor(0.0002)\n",
      "epcho= 4694 loss=  tensor(0.0002)\n",
      "epcho= 4695 loss=  tensor(0.0002)\n",
      "epcho= 4696 loss=  tensor(0.0002)\n",
      "epcho= 4697 loss=  tensor(0.0002)\n",
      "epcho= 4698 loss=  tensor(0.0002)\n",
      "epcho= 4699 loss=  tensor(0.0002)\n",
      "epcho= 4700 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1548, grad_fn=<DivBackward0>)\n",
      "epcho= 4701 loss=  tensor(0.0002)\n",
      "epcho= 4702 loss=  tensor(0.0002)\n",
      "epcho= 4703 loss=  tensor(0.0002)\n",
      "epcho= 4704 loss=  tensor(0.0002)\n",
      "epcho= 4705 loss=  tensor(0.0002)\n",
      "epcho= 4706 loss=  tensor(0.0002)\n",
      "epcho= 4707 loss=  tensor(0.0002)\n",
      "epcho= 4708 loss=  tensor(0.0002)\n",
      "epcho= 4709 loss=  tensor(0.0002)\n",
      "epcho= 4710 loss=  tensor(0.0002)\n",
      "epcho= 4711 loss=  tensor(0.0002)\n",
      "epcho= 4712 loss=  tensor(0.0002)\n",
      "epcho= 4713 loss=  tensor(0.0002)\n",
      "epcho= 4714 loss=  tensor(0.0002)\n",
      "epcho= 4715 loss=  tensor(0.0002)\n",
      "epcho= 4716 loss=  tensor(0.0002)\n",
      "epcho= 4717 loss=  tensor(0.0002)\n",
      "epcho= 4718 loss=  tensor(0.0002)\n",
      "epcho= 4719 loss=  tensor(0.0002)\n",
      "epcho= 4720 loss=  tensor(0.0002)\n",
      "epcho= 4721 loss=  tensor(0.0002)\n",
      "epcho= 4722 loss=  tensor(0.0002)\n",
      "epcho= 4723 loss=  tensor(0.0002)\n",
      "epcho= 4724 loss=  tensor(0.0002)\n",
      "epcho= 4725 loss=  tensor(0.0002)\n",
      "epcho= 4726 loss=  tensor(0.0002)\n",
      "epcho= 4727 loss=  tensor(0.0002)\n",
      "epcho= 4728 loss=  tensor(0.0002)\n",
      "epcho= 4729 loss=  tensor(0.0002)\n",
      "epcho= 4730 loss=  tensor(0.0002)\n",
      "epcho= 4731 loss=  tensor(0.0002)\n",
      "epcho= 4732 loss=  tensor(0.0002)\n",
      "epcho= 4733 loss=  tensor(0.0002)\n",
      "epcho= 4734 loss=  tensor(0.0002)\n",
      "epcho= 4735 loss=  tensor(0.0002)\n",
      "epcho= 4736 loss=  tensor(0.0002)\n",
      "epcho= 4737 loss=  tensor(0.0002)\n",
      "epcho= 4738 loss=  tensor(0.0002)\n",
      "epcho= 4739 loss=  tensor(0.0002)\n",
      "epcho= 4740 loss=  tensor(0.0002)\n",
      "epcho= 4741 loss=  tensor(0.0002)\n",
      "epcho= 4742 loss=  tensor(0.0002)\n",
      "epcho= 4743 loss=  tensor(0.0002)\n",
      "epcho= 4744 loss=  tensor(0.0002)\n",
      "epcho= 4745 loss=  tensor(0.0002)\n",
      "epcho= 4746 loss=  tensor(0.0002)\n",
      "epcho= 4747 loss=  tensor(0.0002)\n",
      "epcho= 4748 loss=  tensor(0.0002)\n",
      "epcho= 4749 loss=  tensor(0.0002)\n",
      "epcho= 4750 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1528, grad_fn=<DivBackward0>)\n",
      "epcho= 4751 loss=  tensor(0.0002)\n",
      "epcho= 4752 loss=  tensor(0.0002)\n",
      "epcho= 4753 loss=  tensor(0.0002)\n",
      "epcho= 4754 loss=  tensor(0.0002)\n",
      "epcho= 4755 loss=  tensor(0.0002)\n",
      "epcho= 4756 loss=  tensor(0.0002)\n",
      "epcho= 4757 loss=  tensor(0.0002)\n",
      "epcho= 4758 loss=  tensor(0.0002)\n",
      "epcho= 4759 loss=  tensor(0.0002)\n",
      "epcho= 4760 loss=  tensor(0.0002)\n",
      "epcho= 4761 loss=  tensor(0.0002)\n",
      "epcho= 4762 loss=  tensor(0.0002)\n",
      "epcho= 4763 loss=  tensor(0.0002)\n",
      "epcho= 4764 loss=  tensor(0.0002)\n",
      "epcho= 4765 loss=  tensor(0.0003)\n",
      "epcho= 4766 loss=  tensor(0.0003)\n",
      "epcho= 4767 loss=  tensor(0.0003)\n",
      "epcho= 4768 loss=  tensor(0.0004)\n",
      "epcho= 4769 loss=  tensor(0.0004)\n",
      "epcho= 4770 loss=  tensor(0.0005)\n",
      "epcho= 4771 loss=  tensor(0.0006)\n",
      "epcho= 4772 loss=  tensor(0.0007)\n",
      "epcho= 4773 loss=  tensor(0.0008)\n",
      "epcho= 4774 loss=  tensor(0.0009)\n",
      "epcho= 4775 loss=  tensor(0.0009)\n",
      "epcho= 4776 loss=  tensor(0.0009)\n",
      "epcho= 4777 loss=  tensor(0.0008)\n",
      "epcho= 4778 loss=  tensor(0.0007)\n",
      "epcho= 4779 loss=  tensor(0.0005)\n",
      "epcho= 4780 loss=  tensor(0.0003)\n",
      "epcho= 4781 loss=  tensor(0.0002)\n",
      "epcho= 4782 loss=  tensor(0.0002)\n",
      "epcho= 4783 loss=  tensor(0.0003)\n",
      "epcho= 4784 loss=  tensor(0.0003)\n",
      "epcho= 4785 loss=  tensor(0.0004)\n",
      "epcho= 4786 loss=  tensor(0.0004)\n",
      "epcho= 4787 loss=  tensor(0.0004)\n",
      "epcho= 4788 loss=  tensor(0.0003)\n",
      "epcho= 4789 loss=  tensor(0.0002)\n",
      "epcho= 4790 loss=  tensor(0.0002)\n",
      "epcho= 4791 loss=  tensor(0.0002)\n",
      "epcho= 4792 loss=  tensor(0.0002)\n",
      "epcho= 4793 loss=  tensor(0.0002)\n",
      "epcho= 4794 loss=  tensor(0.0003)\n",
      "epcho= 4795 loss=  tensor(0.0003)\n",
      "epcho= 4796 loss=  tensor(0.0003)\n",
      "epcho= 4797 loss=  tensor(0.0002)\n",
      "epcho= 4798 loss=  tensor(0.0002)\n",
      "epcho= 4799 loss=  tensor(0.0002)\n",
      "epcho= 4800 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1549, grad_fn=<DivBackward0>)\n",
      "epcho= 4801 loss=  tensor(0.0002)\n",
      "epcho= 4802 loss=  tensor(0.0002)\n",
      "epcho= 4803 loss=  tensor(0.0002)\n",
      "epcho= 4804 loss=  tensor(0.0002)\n",
      "epcho= 4805 loss=  tensor(0.0002)\n",
      "epcho= 4806 loss=  tensor(0.0002)\n",
      "epcho= 4807 loss=  tensor(0.0002)\n",
      "epcho= 4808 loss=  tensor(0.0002)\n",
      "epcho= 4809 loss=  tensor(0.0002)\n",
      "epcho= 4810 loss=  tensor(0.0002)\n",
      "epcho= 4811 loss=  tensor(0.0002)\n",
      "epcho= 4812 loss=  tensor(0.0002)\n",
      "epcho= 4813 loss=  tensor(0.0002)\n",
      "epcho= 4814 loss=  tensor(0.0002)\n",
      "epcho= 4815 loss=  tensor(0.0002)\n",
      "epcho= 4816 loss=  tensor(0.0002)\n",
      "epcho= 4817 loss=  tensor(0.0002)\n",
      "epcho= 4818 loss=  tensor(0.0002)\n",
      "epcho= 4819 loss=  tensor(0.0002)\n",
      "epcho= 4820 loss=  tensor(0.0002)\n",
      "epcho= 4821 loss=  tensor(0.0002)\n",
      "epcho= 4822 loss=  tensor(0.0002)\n",
      "epcho= 4823 loss=  tensor(0.0002)\n",
      "epcho= 4824 loss=  tensor(0.0002)\n",
      "epcho= 4825 loss=  tensor(0.0002)\n",
      "epcho= 4826 loss=  tensor(0.0002)\n",
      "epcho= 4827 loss=  tensor(0.0002)\n",
      "epcho= 4828 loss=  tensor(0.0002)\n",
      "epcho= 4829 loss=  tensor(0.0002)\n",
      "epcho= 4830 loss=  tensor(0.0002)\n",
      "epcho= 4831 loss=  tensor(0.0002)\n",
      "epcho= 4832 loss=  tensor(0.0002)\n",
      "epcho= 4833 loss=  tensor(0.0002)\n",
      "epcho= 4834 loss=  tensor(0.0002)\n",
      "epcho= 4835 loss=  tensor(0.0002)\n",
      "epcho= 4836 loss=  tensor(0.0002)\n",
      "epcho= 4837 loss=  tensor(0.0002)\n",
      "epcho= 4838 loss=  tensor(0.0002)\n",
      "epcho= 4839 loss=  tensor(0.0002)\n",
      "epcho= 4840 loss=  tensor(0.0002)\n",
      "epcho= 4841 loss=  tensor(0.0002)\n",
      "epcho= 4842 loss=  tensor(0.0002)\n",
      "epcho= 4843 loss=  tensor(0.0002)\n",
      "epcho= 4844 loss=  tensor(0.0002)\n",
      "epcho= 4845 loss=  tensor(0.0002)\n",
      "epcho= 4846 loss=  tensor(0.0002)\n",
      "epcho= 4847 loss=  tensor(0.0002)\n",
      "epcho= 4848 loss=  tensor(0.0002)\n",
      "epcho= 4849 loss=  tensor(0.0002)\n",
      "epcho= 4850 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1521, grad_fn=<DivBackward0>)\n",
      "epcho= 4851 loss=  tensor(0.0002)\n",
      "epcho= 4852 loss=  tensor(0.0002)\n",
      "epcho= 4853 loss=  tensor(0.0002)\n",
      "epcho= 4854 loss=  tensor(0.0002)\n",
      "epcho= 4855 loss=  tensor(0.0002)\n",
      "epcho= 4856 loss=  tensor(0.0002)\n",
      "epcho= 4857 loss=  tensor(0.0002)\n",
      "epcho= 4858 loss=  tensor(0.0002)\n",
      "epcho= 4859 loss=  tensor(0.0002)\n",
      "epcho= 4860 loss=  tensor(0.0002)\n",
      "epcho= 4861 loss=  tensor(0.0002)\n",
      "epcho= 4862 loss=  tensor(0.0002)\n",
      "epcho= 4863 loss=  tensor(0.0002)\n",
      "epcho= 4864 loss=  tensor(0.0002)\n",
      "epcho= 4865 loss=  tensor(0.0002)\n",
      "epcho= 4866 loss=  tensor(0.0002)\n",
      "epcho= 4867 loss=  tensor(0.0002)\n",
      "epcho= 4868 loss=  tensor(0.0002)\n",
      "epcho= 4869 loss=  tensor(0.0002)\n",
      "epcho= 4870 loss=  tensor(0.0002)\n",
      "epcho= 4871 loss=  tensor(0.0002)\n",
      "epcho= 4872 loss=  tensor(0.0002)\n",
      "epcho= 4873 loss=  tensor(0.0002)\n",
      "epcho= 4874 loss=  tensor(0.0002)\n",
      "epcho= 4875 loss=  tensor(0.0002)\n",
      "epcho= 4876 loss=  tensor(0.0002)\n",
      "epcho= 4877 loss=  tensor(0.0002)\n",
      "epcho= 4878 loss=  tensor(0.0002)\n",
      "epcho= 4879 loss=  tensor(0.0002)\n",
      "epcho= 4880 loss=  tensor(0.0002)\n",
      "epcho= 4881 loss=  tensor(0.0002)\n",
      "epcho= 4882 loss=  tensor(0.0002)\n",
      "epcho= 4883 loss=  tensor(0.0002)\n",
      "epcho= 4884 loss=  tensor(0.0002)\n",
      "epcho= 4885 loss=  tensor(0.0002)\n",
      "epcho= 4886 loss=  tensor(0.0002)\n",
      "epcho= 4887 loss=  tensor(0.0002)\n",
      "epcho= 4888 loss=  tensor(0.0002)\n",
      "epcho= 4889 loss=  tensor(0.0002)\n",
      "epcho= 4890 loss=  tensor(0.0002)\n",
      "epcho= 4891 loss=  tensor(0.0002)\n",
      "epcho= 4892 loss=  tensor(0.0002)\n",
      "epcho= 4893 loss=  tensor(0.0002)\n",
      "epcho= 4894 loss=  tensor(0.0002)\n",
      "epcho= 4895 loss=  tensor(0.0002)\n",
      "epcho= 4896 loss=  tensor(0.0002)\n",
      "epcho= 4897 loss=  tensor(0.0002)\n",
      "epcho= 4898 loss=  tensor(0.0002)\n",
      "epcho= 4899 loss=  tensor(0.0002)\n",
      "epcho= 4900 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1508, grad_fn=<DivBackward0>)\n",
      "epcho= 4901 loss=  tensor(0.0002)\n",
      "epcho= 4902 loss=  tensor(0.0002)\n",
      "epcho= 4903 loss=  tensor(0.0002)\n",
      "epcho= 4904 loss=  tensor(0.0002)\n",
      "epcho= 4905 loss=  tensor(0.0002)\n",
      "epcho= 4906 loss=  tensor(0.0002)\n",
      "epcho= 4907 loss=  tensor(0.0002)\n",
      "epcho= 4908 loss=  tensor(0.0002)\n",
      "epcho= 4909 loss=  tensor(0.0002)\n",
      "epcho= 4910 loss=  tensor(0.0002)\n",
      "epcho= 4911 loss=  tensor(0.0002)\n",
      "epcho= 4912 loss=  tensor(0.0002)\n",
      "epcho= 4913 loss=  tensor(0.0002)\n",
      "epcho= 4914 loss=  tensor(0.0002)\n",
      "epcho= 4915 loss=  tensor(0.0002)\n",
      "epcho= 4916 loss=  tensor(0.0002)\n",
      "epcho= 4917 loss=  tensor(0.0002)\n",
      "epcho= 4918 loss=  tensor(0.0002)\n",
      "epcho= 4919 loss=  tensor(0.0002)\n",
      "epcho= 4920 loss=  tensor(0.0002)\n",
      "epcho= 4921 loss=  tensor(0.0002)\n",
      "epcho= 4922 loss=  tensor(0.0002)\n",
      "epcho= 4923 loss=  tensor(0.0002)\n",
      "epcho= 4924 loss=  tensor(0.0002)\n",
      "epcho= 4925 loss=  tensor(0.0002)\n",
      "epcho= 4926 loss=  tensor(0.0002)\n",
      "epcho= 4927 loss=  tensor(0.0002)\n",
      "epcho= 4928 loss=  tensor(0.0002)\n",
      "epcho= 4929 loss=  tensor(0.0002)\n",
      "epcho= 4930 loss=  tensor(0.0002)\n",
      "epcho= 4931 loss=  tensor(0.0002)\n",
      "epcho= 4932 loss=  tensor(0.0002)\n",
      "epcho= 4933 loss=  tensor(0.0002)\n",
      "epcho= 4934 loss=  tensor(0.0002)\n",
      "epcho= 4935 loss=  tensor(0.0002)\n",
      "epcho= 4936 loss=  tensor(0.0002)\n",
      "epcho= 4937 loss=  tensor(0.0002)\n",
      "epcho= 4938 loss=  tensor(0.0002)\n",
      "epcho= 4939 loss=  tensor(0.0002)\n",
      "epcho= 4940 loss=  tensor(0.0002)\n",
      "epcho= 4941 loss=  tensor(0.0002)\n",
      "epcho= 4942 loss=  tensor(0.0002)\n",
      "epcho= 4943 loss=  tensor(0.0002)\n",
      "epcho= 4944 loss=  tensor(0.0002)\n",
      "epcho= 4945 loss=  tensor(0.0002)\n",
      "epcho= 4946 loss=  tensor(0.0002)\n",
      "epcho= 4947 loss=  tensor(0.0002)\n",
      "epcho= 4948 loss=  tensor(0.0002)\n",
      "epcho= 4949 loss=  tensor(0.0002)\n",
      "epcho= 4950 loss=  tensor(0.0002)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>) tensor(0.1497, grad_fn=<DivBackward0>)\n",
      "epcho= 4951 loss=  tensor(0.0002)\n",
      "epcho= 4952 loss=  tensor(0.0002)\n",
      "epcho= 4953 loss=  tensor(0.0002)\n",
      "epcho= 4954 loss=  tensor(0.0002)\n",
      "epcho= 4955 loss=  tensor(0.0002)\n",
      "epcho= 4956 loss=  tensor(0.0002)\n",
      "epcho= 4957 loss=  tensor(0.0002)\n",
      "epcho= 4958 loss=  tensor(0.0002)\n",
      "epcho= 4959 loss=  tensor(0.0002)\n",
      "epcho= 4960 loss=  tensor(0.0002)\n",
      "epcho= 4961 loss=  tensor(0.0002)\n",
      "epcho= 4962 loss=  tensor(0.0002)\n",
      "epcho= 4963 loss=  tensor(0.0002)\n",
      "epcho= 4964 loss=  tensor(0.0002)\n",
      "epcho= 4965 loss=  tensor(0.0002)\n",
      "epcho= 4966 loss=  tensor(0.0002)\n",
      "epcho= 4967 loss=  tensor(0.0002)\n",
      "epcho= 4968 loss=  tensor(0.0002)\n",
      "epcho= 4969 loss=  tensor(0.0002)\n",
      "epcho= 4970 loss=  tensor(0.0002)\n",
      "epcho= 4971 loss=  tensor(0.0002)\n",
      "epcho= 4972 loss=  tensor(0.0002)\n",
      "epcho= 4973 loss=  tensor(0.0002)\n",
      "epcho= 4974 loss=  tensor(0.0002)\n",
      "epcho= 4975 loss=  tensor(0.0002)\n",
      "epcho= 4976 loss=  tensor(0.0002)\n",
      "epcho= 4977 loss=  tensor(0.0002)\n",
      "epcho= 4978 loss=  tensor(0.0002)\n",
      "epcho= 4979 loss=  tensor(0.0002)\n",
      "epcho= 4980 loss=  tensor(0.0002)\n",
      "epcho= 4981 loss=  tensor(0.0002)\n",
      "epcho= 4982 loss=  tensor(0.0002)\n",
      "epcho= 4983 loss=  tensor(0.0002)\n",
      "epcho= 4984 loss=  tensor(0.0002)\n",
      "epcho= 4985 loss=  tensor(0.0002)\n",
      "epcho= 4986 loss=  tensor(0.0002)\n",
      "epcho= 4987 loss=  tensor(0.0002)\n",
      "epcho= 4988 loss=  tensor(0.0002)\n",
      "epcho= 4989 loss=  tensor(0.0002)\n",
      "epcho= 4990 loss=  tensor(0.0002)\n",
      "epcho= 4991 loss=  tensor(0.0002)\n",
      "epcho= 4992 loss=  tensor(0.0002)\n",
      "epcho= 4993 loss=  tensor(0.0002)\n",
      "epcho= 4994 loss=  tensor(0.0002)\n",
      "epcho= 4995 loss=  tensor(0.0002)\n",
      "epcho= 4996 loss=  tensor(0.0002)\n",
      "epcho= 4997 loss=  tensor(0.0002)\n",
      "epcho= 4998 loss=  tensor(0.0002)\n",
      "epcho= 4999 loss=  tensor(0.0002)\n",
      "Training time: 230.22\n",
      "Test Error: 0.14867\n"
     ]
    },
    {
     "data": {
      "text/plain": "' Solution Plot '"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_u = 200 #Total number of data points for 'u'\n",
    "N_f = 4000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train_np_array, X_u_train_np_array, u_train_np_array, x_00_np_array, u_00_np_array,leftedge_x_np_array, rightedge_x_np_array, topedge_x_np_array,bottomedge_x_np_array = trainingdata(N_u,N_f)\n",
    "\n",
    "'Convert to tensor and send to GPU'\n",
    "X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "xy_0 = torch.from_numpy(x_00_np_array).float().to(device)\n",
    "xy_left = torch.from_numpy(leftedge_x_np_array).float().to(device)\n",
    "xy_right = torch.from_numpy(rightedge_x_np_array).float().to(device)\n",
    "xy_top = torch.from_numpy(topedge_x_np_array).float().to(device)\n",
    "xy_bottom = torch.from_numpy(bottomedge_x_np_array).float().to(device)\n",
    "u_0 = torch.from_numpy(u_00_np_array).float().to(device)\n",
    "X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "u = torch.from_numpy(u_true).float().to(device)\n",
    "# f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "\n",
    "layers = np.array([2, 50, 50, 50, 1]) #3 hidden layers\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "       \n",
    "PINN.to(device)\n",
    "\n",
    "'Neural Network Summary'\n",
    "\n",
    "print(PINN)\n",
    "\n",
    "params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'Adam Optimizer'\n",
    "\n",
    "optimizer = optim.Adam(PINN.parameters(), lr=0.01,foreach=True)\n",
    "scd = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.98)\n",
    "\n",
    "max_iter = 5000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    loss = PINN.loss(xy_0,u_0,xy_left,xy_right,xy_top,xy_bottom, X_f_train)\n",
    "    \n",
    "    print('epcho=',i,'loss= ',loss.data)\n",
    "           \n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    loss.backward() #backprop\n",
    "\n",
    "    optimizer.step()\n",
    "    scd.step()\n",
    "    \n",
    "    if i % (max_iter/100) == 0:\n",
    "\n",
    "        error_vec, _ = PINN.test()\n",
    "\n",
    "        print(loss,error_vec)\n",
    "    \n",
    "    \n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "error_vec, u_pred = PINN.test()\n",
    "\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "\n",
    "\n",
    "''' Solution Plot '''\n",
    "# solutionplot(u_pred,X_u_train,u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce8873c-017a-491b-94d0-d45b6a8c5e43",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-29T01:21:29.155277Z",
     "iopub.status.busy": "2023-11-29T01:21:29.155277Z",
     "iopub.status.idle": "2023-11-29T01:21:39.225537Z",
     "shell.execute_reply": "2023-11-29T01:21:39.224536Z",
     "shell.execute_reply.started": "2023-11-29T01:21:29.155277Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-11T12:30:23.235180500Z",
     "start_time": "2024-11-11T12:29:41.795191600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= {} loss=  0.00017141982971224934\n",
      "epoch= {} loss=  0.00017147863400168717\n",
      "epoch= {} loss=  0.00017140017007477582\n",
      "epoch= {} loss=  0.0001713653327897191\n",
      "epoch= {} loss=  0.00017112593923229724\n",
      "epoch= {} loss=  0.000171012245118618\n",
      "epoch= {} loss=  0.00017086893785744905\n",
      "epoch= {} loss=  0.00017040452803485096\n",
      "epoch= {} loss=  0.00017013672913890332\n",
      "epoch= {} loss=  0.00016963380039669573\n",
      "epoch= {} loss=  0.00016859066090546548\n",
      "epoch= {} loss=  0.00016735121607780457\n",
      "epoch= {} loss=  0.00016462302301079035\n",
      "epoch= {} loss=  0.00015885259199421853\n",
      "epoch= {} loss=  0.00015112492837943137\n",
      "epoch= {} loss=  0.0001419794571120292\n",
      "epoch= {} loss=  0.0001373617851641029\n",
      "epoch= {} loss=  0.00013529520947486162\n",
      "epoch= {} loss=  0.00013169858721084893\n",
      "epoch= {} loss=  0.00012826365127693862\n",
      "epoch= {} loss=  0.00012379203690215945\n",
      "epoch= {} loss=  0.0001201740451506339\n",
      "epoch= {} loss=  0.00011676225403789431\n",
      "epoch= {} loss=  0.00011595510295592248\n",
      "epoch= {} loss=  0.00011581923899939284\n",
      "epoch= {} loss=  0.00011523735884111375\n",
      "epoch= {} loss=  0.0001148398732766509\n",
      "epoch= {} loss=  0.00011382876255083829\n",
      "epoch= {} loss=  0.00011284888751106337\n",
      "epoch= {} loss=  0.00011123971489723772\n",
      "epoch= {} loss=  0.0001100368972402066\n",
      "epoch= {} loss=  0.0001098267239285633\n",
      "epoch= {} loss=  0.00010979124635923654\n",
      "epoch= {} loss=  0.00010972029849654064\n",
      "epoch= {} loss=  0.00010947004921035841\n",
      "epoch= {} loss=  0.00010892505815718323\n",
      "epoch= {} loss=  0.00010777454735944048\n",
      "epoch= {} loss=  0.00010608183220028877\n",
      "epoch= {} loss=  0.0001039231865433976\n",
      "epoch= {} loss=  0.00010178032971452922\n",
      "epoch= {} loss=  0.00010088922863360494\n",
      "epoch= {} loss=  9.997916640713811e-05\n",
      "epoch= {} loss=  9.841558494372293e-05\n",
      "epoch= {} loss=  9.701699309516698e-05\n",
      "epoch= {} loss=  9.55125069594942e-05\n",
      "epoch= {} loss=  9.511378448223695e-05\n",
      "epoch= {} loss=  9.503879846306518e-05\n",
      "epoch= {} loss=  9.498154395259917e-05\n",
      "epoch= {} loss=  9.472336387261748e-05\n",
      "epoch= {} loss=  9.440384019399062e-05\n",
      "epoch= {} loss=  9.381460404256359e-05\n",
      "epoch= {} loss=  9.265161497751251e-05\n",
      "epoch= {} loss=  9.116328146774322e-05\n",
      "epoch= {} loss=  8.843398973112926e-05\n",
      "epoch= {} loss=  8.65191759658046e-05\n",
      "epoch= {} loss=  0.00013508659321814775\n",
      "epoch= {} loss=  8.35796381579712e-05\n",
      "epoch= {} loss=  7.971531886141747e-05\n",
      "epoch= {} loss=  7.832433038856834e-05\n",
      "epoch= {} loss=  7.746893243165687e-05\n",
      "epoch= {} loss=  7.711317448411137e-05\n",
      "epoch= {} loss=  7.706814358243719e-05\n",
      "epoch= {} loss=  7.692188955843449e-05\n",
      "epoch= {} loss=  7.689531776122749e-05\n",
      "epoch= {} loss=  7.680320413783193e-05\n",
      "epoch= {} loss=  7.661285053472966e-05\n",
      "epoch= {} loss=  7.612191257067025e-05\n",
      "epoch= {} loss=  7.508509588660672e-05\n",
      "epoch= {} loss=  7.409240788547322e-05\n",
      "epoch= {} loss=  7.26396610843949e-05\n",
      "epoch= {} loss=  7.183147681644186e-05\n",
      "epoch= {} loss=  7.092564192134887e-05\n",
      "epoch= {} loss=  6.965475040487945e-05\n",
      "epoch= {} loss=  6.871891673654318e-05\n",
      "epoch= {} loss=  6.812655192334205e-05\n",
      "epoch= {} loss=  6.726017454639077e-05\n",
      "epoch= {} loss=  6.702427344862372e-05\n",
      "epoch= {} loss=  6.689984002150595e-05\n",
      "epoch= {} loss=  6.685474363621324e-05\n",
      "epoch= {} loss=  6.677000055788085e-05\n",
      "epoch= {} loss=  6.640537321800366e-05\n",
      "epoch= {} loss=  6.569770630449057e-05\n",
      "epoch= {} loss=  6.399474659701809e-05\n",
      "epoch= {} loss=  6.166166713228449e-05\n",
      "epoch= {} loss=  5.880720345885493e-05\n",
      "epoch= {} loss=  5.699213215848431e-05\n",
      "epoch= {} loss=  5.68819377804175e-05\n",
      "epoch= {} loss=  5.673473060596734e-05\n",
      "epoch= {} loss=  5.666744982590899e-05\n",
      "epoch= {} loss=  5.663762931362726e-05\n",
      "epoch= {} loss=  5.658251029672101e-05\n",
      "epoch= {} loss=  5.643520853482187e-05\n",
      "epoch= {} loss=  5.622190292342566e-05\n",
      "epoch= {} loss=  5.570313078351319e-05\n",
      "epoch= {} loss=  5.483713903231546e-05\n",
      "epoch= {} loss=  5.360443174140528e-05\n",
      "epoch= {} loss=  5.233183765085414e-05\n",
      "epoch= {} loss=  5.104599040350877e-05\n",
      "epoch= {} loss=  5.0073656893800944e-05\n",
      "epoch= {} loss=  4.919966158922762e-05\n",
      "epoch= {} loss=  4.926823385176249e-05\n",
      "epoch= {} loss=  4.885372982244007e-05\n",
      "epoch= {} loss=  4.8250090912915766e-05\n",
      "epoch= {} loss=  4.805017670150846e-05\n",
      "epoch= {} loss=  4.792306572198868e-05\n",
      "epoch= {} loss=  4.785271448781714e-05\n",
      "epoch= {} loss=  4.783801341545768e-05\n",
      "epoch= {} loss=  4.78049369121436e-05\n",
      "epoch= {} loss=  4.779346636496484e-05\n",
      "epoch= {} loss=  4.7777713916730136e-05\n",
      "epoch= {} loss=  4.7737987188156694e-05\n",
      "epoch= {} loss=  4.7616573283448815e-05\n",
      "epoch= {} loss=  4.7387307859025896e-05\n",
      "epoch= {} loss=  4.690763307735324e-05\n",
      "epoch= {} loss=  4.6203869715100154e-05\n",
      "epoch= {} loss=  8.794432505965233e-05\n",
      "epoch= {} loss=  4.602004628395662e-05\n",
      "epoch= {} loss=  4.588244701153599e-05\n",
      "epoch= {} loss=  4.499612987274304e-05\n",
      "epoch= {} loss=  4.1526011045789346e-05\n",
      "epoch= {} loss=  4.128463478991762e-05\n",
      "epoch= {} loss=  3.994640428572893e-05\n",
      "epoch= {} loss=  4.374338095658459e-05\n",
      "epoch= {} loss=  3.847428524750285e-05\n",
      "epoch= {} loss=  3.748838935280219e-05\n",
      "epoch= {} loss=  3.561037010513246e-05\n",
      "epoch= {} loss=  3.5175689845345914e-05\n",
      "epoch= {} loss=  3.476922938716598e-05\n",
      "epoch= {} loss=  3.466042835498229e-05\n",
      "epoch= {} loss=  3.4589280403451994e-05\n",
      "epoch= {} loss=  3.4575106838019565e-05\n",
      "epoch= {} loss=  3.455913611105643e-05\n",
      "epoch= {} loss=  3.454804391367361e-05\n",
      "epoch= {} loss=  3.454192483332008e-05\n",
      "epoch= {} loss=  3.452079545240849e-05\n",
      "epoch= {} loss=  3.4491284168325365e-05\n",
      "epoch= {} loss=  3.4448028600309044e-05\n",
      "epoch= {} loss=  3.4369921195320785e-05\n",
      "epoch= {} loss=  3.420913344598375e-05\n",
      "epoch= {} loss=  3.386345997569151e-05\n",
      "epoch= {} loss=  3.311436012154445e-05\n",
      "epoch= {} loss=  3.1983789085643366e-05\n",
      "epoch= {} loss=  3.103733615716919e-05\n",
      "epoch= {} loss=  3.066571298404597e-05\n",
      "epoch= {} loss=  3.057487265323289e-05\n",
      "epoch= {} loss=  3.0498589694616385e-05\n",
      "epoch= {} loss=  3.0466255338978954e-05\n",
      "epoch= {} loss=  3.040516639885027e-05\n",
      "epoch= {} loss=  3.0298313504317775e-05\n",
      "epoch= {} loss=  3.0114098990452476e-05\n",
      "epoch= {} loss=  2.9887207347201183e-05\n",
      "epoch= {} loss=  2.9632796213263646e-05\n",
      "epoch= {} loss=  2.940164995379746e-05\n",
      "epoch= {} loss=  2.9300017558853142e-05\n",
      "epoch= {} loss=  2.9216118491603993e-05\n",
      "epoch= {} loss=  2.9151262424420565e-05\n",
      "epoch= {} loss=  2.9118435122654773e-05\n",
      "epoch= {} loss=  2.9073822588543408e-05\n",
      "epoch= {} loss=  2.9053702746750787e-05\n",
      "epoch= {} loss=  2.903672429965809e-05\n",
      "epoch= {} loss=  2.9022146918578073e-05\n",
      "epoch= {} loss=  2.8995553293498233e-05\n",
      "epoch= {} loss=  2.8935592126799747e-05\n",
      "epoch= {} loss=  2.879243766074069e-05\n",
      "epoch= {} loss=  2.856973878806457e-05\n",
      "epoch= {} loss=  2.855090679076966e-05\n",
      "epoch= {} loss=  2.8419557565939613e-05\n",
      "epoch= {} loss=  2.807301098073367e-05\n",
      "epoch= {} loss=  2.806671182042919e-05\n",
      "epoch= {} loss=  2.7452837457531132e-05\n",
      "epoch= {} loss=  2.690320798137691e-05\n",
      "epoch= {} loss=  2.6111154511454515e-05\n",
      "epoch= {} loss=  2.471860898367595e-05\n",
      "epoch= {} loss=  2.3964639694895595e-05\n",
      "epoch= {} loss=  2.3794202206772752e-05\n",
      "epoch= {} loss=  2.3726694053038955e-05\n",
      "epoch= {} loss=  2.3635118850506842e-05\n",
      "epoch= {} loss=  2.358077654207591e-05\n",
      "epoch= {} loss=  2.3515056454925798e-05\n",
      "epoch= {} loss=  2.3480171876144595e-05\n",
      "epoch= {} loss=  2.342228799534496e-05\n",
      "epoch= {} loss=  2.3358425096375868e-05\n",
      "epoch= {} loss=  2.3262642571353354e-05\n",
      "epoch= {} loss=  2.32381971727591e-05\n",
      "epoch= {} loss=  2.3225942641147412e-05\n",
      "epoch= {} loss=  2.321783540537581e-05\n",
      "epoch= {} loss=  2.321259307791479e-05\n",
      "epoch= {} loss=  2.320778730791062e-05\n",
      "epoch= {} loss=  2.3201944713946432e-05\n",
      "epoch= {} loss=  2.3197571863420308e-05\n",
      "epoch= {} loss=  2.3185608370113187e-05\n",
      "epoch= {} loss=  2.316415157110896e-05\n",
      "epoch= {} loss=  2.311627940798644e-05\n",
      "epoch= {} loss=  2.3018794308882207e-05\n",
      "epoch= {} loss=  2.2815014744992368e-05\n",
      "epoch= {} loss=  2.2412481484934688e-05\n",
      "epoch= {} loss=  2.1659081539837644e-05\n",
      "epoch= {} loss=  2.0512357878033072e-05\n",
      "epoch= {} loss=  2.1362449842854403e-05\n",
      "epoch= {} loss=  1.975931809283793e-05\n",
      "epoch= {} loss=  2.17206179513596e-05\n",
      "epoch= {} loss=  1.8989394447999075e-05\n",
      "epoch= {} loss=  2.160557414754294e-05\n",
      "epoch= {} loss=  1.8655022358871065e-05\n",
      "epoch= {} loss=  1.8482041923562065e-05\n",
      "epoch= {} loss=  1.8205002561444417e-05\n",
      "epoch= {} loss=  1.8128259398508817e-05\n",
      "epoch= {} loss=  1.801936923584435e-05\n",
      "epoch= {} loss=  1.794081435946282e-05\n",
      "epoch= {} loss=  1.786238863132894e-05\n",
      "epoch= {} loss=  1.7718119124765508e-05\n",
      "epoch= {} loss=  1.761648672982119e-05\n",
      "epoch= {} loss=  1.7566730093676597e-05\n",
      "epoch= {} loss=  1.7546142771607265e-05\n",
      "epoch= {} loss=  1.7546515664434992e-05\n",
      "epoch= {} loss=  1.7525017028674483e-05\n",
      "epoch= {} loss=  1.7509377357782796e-05\n",
      "epoch= {} loss=  1.7502539776614867e-05\n",
      "epoch= {} loss=  1.750002229528036e-05\n",
      "epoch= {} loss=  1.7498405213700607e-05\n",
      "epoch= {} loss=  1.7490330719738267e-05\n",
      "epoch= {} loss=  1.748226713971235e-05\n",
      "epoch= {} loss=  1.746260932122823e-05\n",
      "epoch= {} loss=  1.7438282156945206e-05\n",
      "epoch= {} loss=  1.7406782717444003e-05\n",
      "epoch= {} loss=  1.7376929463353008e-05\n",
      "epoch= {} loss=  1.7354095689370297e-05\n",
      "epoch= {} loss=  1.7342690625810064e-05\n",
      "epoch= {} loss=  1.7319422113359906e-05\n",
      "epoch= {} loss=  1.7295033103437163e-05\n",
      "epoch= {} loss=  1.7257298168260604e-05\n",
      "epoch= {} loss=  1.7200105503434315e-05\n",
      "epoch= {} loss=  1.7095475413952954e-05\n",
      "epoch= {} loss=  1.6944550225161947e-05\n",
      "epoch= {} loss=  1.6813351976452395e-05\n",
      "epoch= {} loss=  1.6843550838530064e-05\n",
      "epoch= {} loss=  1.661655005591456e-05\n",
      "epoch= {} loss=  1.6443551430711523e-05\n",
      "epoch= {} loss=  1.6280730051221326e-05\n",
      "epoch= {} loss=  1.6111249351524748e-05\n",
      "epoch= {} loss=  1.600961149961222e-05\n",
      "epoch= {} loss=  1.5895684555289336e-05\n",
      "epoch= {} loss=  1.57634167408105e-05\n",
      "epoch= {} loss=  1.5591573173878714e-05\n",
      "epoch= {} loss=  1.5428806364070624e-05\n",
      "epoch= {} loss=  1.5217760847008321e-05\n",
      "epoch= {} loss=  1.5044975953060202e-05\n",
      "epoch= {} loss=  1.5054956747917458e-05\n",
      "epoch= {} loss=  1.50029609358171e-05\n",
      "epoch= {} loss=  1.5157411326072179e-05\n",
      "epoch= {} loss=  1.4964384718041401e-05\n",
      "epoch= {} loss=  1.4948723219276872e-05\n",
      "epoch= {} loss=  1.4940920664230362e-05\n",
      "epoch= {} loss=  1.4933002603356726e-05\n",
      "epoch= {} loss=  1.4917266526026651e-05\n",
      "epoch= {} loss=  1.490128124714829e-05\n",
      "epoch= {} loss=  1.4887006727803964e-05\n",
      "epoch= {} loss=  1.487884765083436e-05\n",
      "epoch= {} loss=  1.4874392945785075e-05\n",
      "epoch= {} loss=  1.4870098311803304e-05\n",
      "epoch= {} loss=  1.486250948801171e-05\n",
      "epoch= {} loss=  1.4854826076771133e-05\n",
      "epoch= {} loss=  1.4839328287052922e-05\n",
      "epoch= {} loss=  1.481376511947019e-05\n",
      "epoch= {} loss=  1.4760464182472788e-05\n",
      "epoch= {} loss=  1.4671049029857386e-05\n",
      "epoch= {} loss=  1.453969525755383e-05\n",
      "epoch= {} loss=  1.4436144738283474e-05\n",
      "epoch= {} loss=  1.4208594620868098e-05\n",
      "epoch= {} loss=  1.398332642565947e-05\n",
      "epoch= {} loss=  1.3657356248586439e-05\n",
      "epoch= {} loss=  1.3494270206138026e-05\n",
      "epoch= {} loss=  1.322588650509715e-05\n",
      "epoch= {} loss=  1.333988257101737e-05\n",
      "epoch= {} loss=  1.2970270290679764e-05\n",
      "epoch= {} loss=  1.2630644050659612e-05\n",
      "epoch= {} loss=  1.2162978237029165e-05\n",
      "epoch= {} loss=  1.163428805739386e-05\n",
      "epoch= {} loss=  1.2272583262529224e-05\n",
      "epoch= {} loss=  1.1508735042298213e-05\n",
      "epoch= {} loss=  1.4299153008323628e-05\n",
      "epoch= {} loss=  1.1448633813415654e-05\n",
      "epoch= {} loss=  1.1460976566013414e-05\n",
      "epoch= {} loss=  1.1368614650564268e-05\n",
      "epoch= {} loss=  1.1305423868179787e-05\n",
      "epoch= {} loss=  1.1341099707351532e-05\n",
      "epoch= {} loss=  1.1258188351348508e-05\n",
      "epoch= {} loss=  1.1202077985217329e-05\n",
      "epoch= {} loss=  1.1156761502206791e-05\n",
      "epoch= {} loss=  1.1115236702607945e-05\n",
      "epoch= {} loss=  1.1090475709352177e-05\n",
      "epoch= {} loss=  1.1065430953749456e-05\n",
      "epoch= {} loss=  1.1050610737584066e-05\n",
      "epoch= {} loss=  1.1040201570722274e-05\n",
      "epoch= {} loss=  1.1032871043425985e-05\n",
      "epoch= {} loss=  1.1024335435649846e-05\n",
      "epoch= {} loss=  1.101230282074539e-05\n",
      "epoch= {} loss=  1.099904693546705e-05\n",
      "epoch= {} loss=  1.097726089938078e-05\n",
      "epoch= {} loss=  1.094326489692321e-05\n",
      "epoch= {} loss=  1.0911826393567026e-05\n",
      "epoch= {} loss=  1.0866297998290975e-05\n",
      "epoch= {} loss=  1.0814917004609015e-05\n",
      "epoch= {} loss=  1.075531054084422e-05\n",
      "epoch= {} loss=  1.0715679309214465e-05\n",
      "epoch= {} loss=  1.0604268027236685e-05\n",
      "epoch= {} loss=  1.0470532288309187e-05\n",
      "epoch= {} loss=  1.0407635272713378e-05\n",
      "epoch= {} loss=  1.0352591743867379e-05\n",
      "epoch= {} loss=  1.0269399354001507e-05\n",
      "epoch= {} loss=  1.0692261639633216e-05\n",
      "epoch= {} loss=  1.0074882084154524e-05\n",
      "epoch= {} loss=  1.0001508599088993e-05\n",
      "epoch= {} loss=  9.783625500858761e-06\n",
      "epoch= {} loss=  9.6375970315421e-06\n",
      "epoch= {} loss=  9.281677193939686e-06\n",
      "epoch= {} loss=  9.974377462640405e-06\n",
      "epoch= {} loss=  9.01402199815493e-06\n",
      "epoch= {} loss=  1.0703725820349064e-05\n",
      "epoch= {} loss=  8.768285624682903e-06\n",
      "epoch= {} loss=  8.261758921435103e-06\n",
      "epoch= {} loss=  7.770309821353294e-06\n",
      "epoch= {} loss=  7.527291018050164e-06\n",
      "epoch= {} loss=  7.225753051898209e-06\n",
      "epoch= {} loss=  7.021827514108736e-06\n",
      "epoch= {} loss=  6.967343324504327e-06\n",
      "epoch= {} loss=  7.091325642250013e-06\n",
      "epoch= {} loss=  6.872982339700684e-06\n",
      "epoch= {} loss=  6.817261237301864e-06\n",
      "epoch= {} loss=  6.6958691604668275e-06\n",
      "epoch= {} loss=  6.6479174165579025e-06\n",
      "epoch= {} loss=  6.593591933778953e-06\n",
      "epoch= {} loss=  6.557663255080115e-06\n",
      "epoch= {} loss=  6.526610832224833e-06\n",
      "epoch= {} loss=  6.499567007267615e-06\n",
      "epoch= {} loss=  6.45611953586922e-06\n",
      "epoch= {} loss=  6.391873739630682e-06\n",
      "epoch= {} loss=  6.359348390105879e-06\n",
      "epoch= {} loss=  6.334888894343749e-06\n",
      "epoch= {} loss=  6.314578058663756e-06\n",
      "epoch= {} loss=  6.3022825997904874e-06\n",
      "epoch= {} loss=  6.28581165074138e-06\n",
      "epoch= {} loss=  6.270719950407511e-06\n",
      "epoch= {} loss=  6.260878762986977e-06\n",
      "epoch= {} loss=  6.248356385185616e-06\n",
      "epoch= {} loss=  6.232270607142709e-06\n",
      "epoch= {} loss=  6.215191206138115e-06\n",
      "epoch= {} loss=  6.200641564646503e-06\n",
      "epoch= {} loss=  6.182465767778922e-06\n",
      "epoch= {} loss=  6.154677066660952e-06\n",
      "epoch= {} loss=  6.113518338679569e-06\n",
      "epoch= {} loss=  6.068967650207924e-06\n",
      "epoch= {} loss=  6.039060281182174e-06\n",
      "epoch= {} loss=  6.026330083841458e-06\n",
      "epoch= {} loss=  6.017628948029596e-06\n",
      "epoch= {} loss=  6.002691861795029e-06\n",
      "epoch= {} loss=  5.963793228147551e-06\n",
      "epoch= {} loss=  5.904753834329313e-06\n",
      "epoch= {} loss=  5.857314590684837e-06\n",
      "epoch= {} loss=  5.822154889756348e-06\n",
      "epoch= {} loss=  5.797377070848597e-06\n",
      "epoch= {} loss=  5.78498975301045e-06\n",
      "epoch= {} loss=  5.769977633462986e-06\n",
      "epoch= {} loss=  5.751661319663981e-06\n",
      "epoch= {} loss=  5.72778617424774e-06\n",
      "epoch= {} loss=  5.683414201484993e-06\n",
      "epoch= {} loss=  5.585736289503984e-06\n",
      "epoch= {} loss=  5.413885446614586e-06\n",
      "epoch= {} loss=  5.242497991275741e-06\n",
      "epoch= {} loss=  5.367744506656891e-06\n",
      "epoch= {} loss=  5.175032129045576e-06\n",
      "epoch= {} loss=  5.069764029030921e-06\n",
      "epoch= {} loss=  4.964744675817201e-06\n",
      "epoch= {} loss=  4.933319814881543e-06\n",
      "epoch= {} loss=  4.911768428428331e-06\n",
      "epoch= {} loss=  4.827038083021762e-06\n",
      "epoch= {} loss=  4.789921149495058e-06\n",
      "epoch= {} loss=  4.7385901780216955e-06\n",
      "epoch= {} loss=  4.6705517888767645e-06\n",
      "epoch= {} loss=  4.6337186176970135e-06\n",
      "epoch= {} loss=  4.624668690667022e-06\n",
      "epoch= {} loss=  4.611057192960288e-06\n",
      "epoch= {} loss=  4.595508926286129e-06\n",
      "epoch= {} loss=  4.578861080517527e-06\n",
      "epoch= {} loss=  4.548101969703566e-06\n",
      "epoch= {} loss=  4.524364157987293e-06\n",
      "epoch= {} loss=  4.505213837546762e-06\n",
      "epoch= {} loss=  4.491649633564521e-06\n",
      "epoch= {} loss=  4.481783435039688e-06\n",
      "epoch= {} loss=  4.4750081542588305e-06\n",
      "epoch= {} loss=  4.4711628106597345e-06\n",
      "epoch= {} loss=  4.468676252145087e-06\n",
      "epoch= {} loss=  4.466135123948334e-06\n",
      "epoch= {} loss=  4.464423000172246e-06\n",
      "epoch= {} loss=  4.463113327801693e-06\n",
      "epoch= {} loss=  4.461733624339104e-06\n",
      "epoch= {} loss=  4.460686341190012e-06\n",
      "epoch= {} loss=  4.459117462829454e-06\n",
      "epoch= {} loss=  4.457666364032775e-06\n",
      "epoch= {} loss=  4.456445367395645e-06\n",
      "epoch= {} loss=  4.455848738871282e-06\n",
      "epoch= {} loss=  4.455581347428961e-06\n",
      "epoch= {} loss=  4.4551657083502505e-06\n",
      "epoch= {} loss=  4.454515874385834e-06\n",
      "epoch= {} loss=  4.453364454093389e-06\n",
      "epoch= {} loss=  4.451530458027264e-06\n",
      "epoch= {} loss=  4.448683739610715e-06\n",
      "epoch= {} loss=  4.4438747863750905e-06\n",
      "epoch= {} loss=  4.438504220161121e-06\n",
      "epoch= {} loss=  4.43341969003086e-06\n",
      "epoch= {} loss=  4.429465207067551e-06\n",
      "epoch= {} loss=  4.425641236593947e-06\n",
      "epoch= {} loss=  4.42193413618952e-06\n",
      "epoch= {} loss=  4.418015578266932e-06\n",
      "epoch= {} loss=  4.412358975969255e-06\n",
      "epoch= {} loss=  4.404788796819048e-06\n",
      "epoch= {} loss=  4.394926236273022e-06\n",
      "epoch= {} loss=  4.375859134597704e-06\n",
      "epoch= {} loss=  4.317190814617788e-06\n",
      "epoch= {} loss=  4.253527549735736e-06\n",
      "epoch= {} loss=  4.815446118300315e-06\n",
      "epoch= {} loss=  4.177267328486778e-06\n",
      "epoch= {} loss=  4.921354957332369e-06\n",
      "epoch= {} loss=  4.082634404767305e-06\n",
      "epoch= {} loss=  3.933791958843358e-06\n",
      "epoch= {} loss=  3.799252681346843e-06\n",
      "epoch= {} loss=  5.181079359317664e-06\n",
      "epoch= {} loss=  3.6133981211605715e-06\n",
      "epoch= {} loss=  3.5894711345463293e-06\n",
      "epoch= {} loss=  3.51418498212297e-06\n",
      "epoch= {} loss=  3.734767688001739e-06\n",
      "epoch= {} loss=  3.3781193451432046e-06\n",
      "epoch= {} loss=  5.336987669579685e-06\n",
      "epoch= {} loss=  3.2914203984546475e-06\n",
      "epoch= {} loss=  3.2962677778414218e-06\n",
      "epoch= {} loss=  3.2413151984655997e-06\n",
      "epoch= {} loss=  3.2380144148191903e-06\n",
      "epoch= {} loss=  3.215003516743309e-06\n",
      "epoch= {} loss=  3.256118816352682e-06\n",
      "epoch= {} loss=  3.1741369639348704e-06\n",
      "epoch= {} loss=  3.1602944545738865e-06\n",
      "epoch= {} loss=  3.146054041280877e-06\n",
      "epoch= {} loss=  3.143486082990421e-06\n",
      "epoch= {} loss=  3.1333461265603546e-06\n",
      "epoch= {} loss=  3.1318629680754384e-06\n",
      "epoch= {} loss=  3.1241693250194658e-06\n",
      "epoch= {} loss=  3.1207337087835185e-06\n",
      "epoch= {} loss=  3.11600979330251e-06\n",
      "epoch= {} loss=  3.1110521376831457e-06\n",
      "epoch= {} loss=  3.1087088245840278e-06\n",
      "epoch= {} loss=  3.106049689449719e-06\n",
      "epoch= {} loss=  3.10233872369281e-06\n",
      "epoch= {} loss=  3.1006638891994953e-06\n",
      "epoch= {} loss=  3.0988314847490983e-06\n",
      "epoch= {} loss=  3.0967266866355203e-06\n",
      "epoch= {} loss=  3.0960532058088575e-06\n",
      "epoch= {} loss=  3.095434294664301e-06\n",
      "epoch= {} loss=  3.094945896009449e-06\n",
      "epoch= {} loss=  3.0937221708882134e-06\n",
      "epoch= {} loss=  3.092261522397166e-06\n",
      "epoch= {} loss=  3.090331119892653e-06\n",
      "epoch= {} loss=  3.088324774580542e-06\n",
      "epoch= {} loss=  3.0845262699585874e-06\n",
      "epoch= {} loss=  3.0791118206252577e-06\n",
      "epoch= {} loss=  3.0724099815415684e-06\n",
      "epoch= {} loss=  3.065698820137186e-06\n",
      "epoch= {} loss=  3.0588075787818525e-06\n",
      "epoch= {} loss=  3.0486053219647147e-06\n",
      "epoch= {} loss=  3.0273263291746844e-06\n",
      "epoch= {} loss=  2.9841726245649625e-06\n",
      "epoch= {} loss=  2.920760607594275e-06\n",
      "epoch= {} loss=  2.8621834644582123e-06\n",
      "epoch= {} loss=  2.8620449938898673e-06\n",
      "epoch= {} loss=  2.8229778763488866e-06\n",
      "epoch= {} loss=  2.8181761990708765e-06\n",
      "epoch= {} loss=  2.787229277601e-06\n",
      "epoch= {} loss=  2.910443981818389e-06\n",
      "epoch= {} loss=  2.729489096964244e-06\n",
      "epoch= {} loss=  2.7706760192813817e-06\n",
      "epoch= {} loss=  2.696304818528006e-06\n",
      "epoch= {} loss=  2.6345683181716595e-06\n",
      "epoch= {} loss=  2.5870506306091556e-06\n",
      "epoch= {} loss=  2.5978110897995066e-06\n",
      "epoch= {} loss=  2.5628708044678206e-06\n",
      "epoch= {} loss=  2.533676251914585e-06\n",
      "epoch= {} loss=  2.4952882995421533e-06\n",
      "epoch= {} loss=  2.4676239718246507e-06\n",
      "epoch= {} loss=  2.4445530470984522e-06\n",
      "epoch= {} loss=  2.4230487269960577e-06\n",
      "epoch= {} loss=  2.4154164748324547e-06\n",
      "epoch= {} loss=  2.3589639113197336e-06\n",
      "epoch= {} loss=  2.358215169806499e-06\n",
      "epoch= {} loss=  2.3368663732981076e-06\n",
      "epoch= {} loss=  2.2915819499758072e-06\n",
      "epoch= {} loss=  2.2405477011488983e-06\n",
      "epoch= {} loss=  2.8267631932976656e-06\n",
      "epoch= {} loss=  2.2046892809157725e-06\n",
      "epoch= {} loss=  2.1774449123768136e-06\n",
      "epoch= {} loss=  2.1561486391874496e-06\n",
      "epoch= {} loss=  2.1323764940461842e-06\n",
      "epoch= {} loss=  2.1147830011614133e-06\n",
      "epoch= {} loss=  2.094268893415574e-06\n",
      "epoch= {} loss=  2.0667355329351267e-06\n",
      "epoch= {} loss=  2.0713989670184674e-06\n",
      "epoch= {} loss=  2.052421223197598e-06\n",
      "epoch= {} loss=  2.045044311671518e-06\n",
      "epoch= {} loss=  2.0352993033156963e-06\n",
      "epoch= {} loss=  2.0297657101764344e-06\n",
      "epoch= {} loss=  2.0190161649225047e-06\n",
      "epoch= {} loss=  2.012345476032351e-06\n",
      "epoch= {} loss=  2.0019601834064815e-06\n",
      "epoch= {} loss=  1.9849144337058533e-06\n",
      "epoch= {} loss=  1.968433025467675e-06\n",
      "epoch= {} loss=  1.9474409782560542e-06\n",
      "epoch= {} loss=  1.925194055729662e-06\n",
      "epoch= {} loss=  1.903396764646459e-06\n",
      "epoch= {} loss=  1.8835658011084888e-06\n",
      "epoch= {} loss=  1.8908783658844186e-06\n",
      "epoch= {} loss=  1.8765790628094692e-06\n",
      "epoch= {} loss=  1.8768589598039398e-06\n",
      "epoch= {} loss=  1.8734990590019152e-06\n",
      "epoch= {} loss=  1.8719671288636164e-06\n",
      "epoch= {} loss=  1.8687114788917825e-06\n",
      "epoch= {} loss=  1.8668589518711087e-06\n",
      "epoch= {} loss=  1.8641552514964133e-06\n",
      "epoch= {} loss=  1.8573109628050588e-06\n",
      "epoch= {} loss=  1.8526759504311485e-06\n",
      "epoch= {} loss=  1.847912358243775e-06\n",
      "epoch= {} loss=  1.842911387939239e-06\n",
      "epoch= {} loss=  1.8363357412454206e-06\n",
      "epoch= {} loss=  1.8322135701964726e-06\n",
      "epoch= {} loss=  1.8560823491498013e-06\n",
      "epoch= {} loss=  1.8290287471245392e-06\n",
      "epoch= {} loss=  1.8256528164783958e-06\n",
      "epoch= {} loss=  1.8221601294499123e-06\n",
      "epoch= {} loss=  1.8175416016674717e-06\n",
      "epoch= {} loss=  1.8146088223147672e-06\n",
      "epoch= {} loss=  1.8102124386132346e-06\n",
      "epoch= {} loss=  1.8104742594005074e-06\n",
      "epoch= {} loss=  1.8087317812387482e-06\n",
      "epoch= {} loss=  1.8094923461831058e-06\n",
      "epoch= {} loss=  1.807560238376027e-06\n",
      "epoch= {} loss=  1.806605837373354e-06\n",
      "epoch= {} loss=  1.8058924524666509e-06\n",
      "epoch= {} loss=  1.8038228972727666e-06\n",
      "epoch= {} loss=  1.8017357206190354e-06\n",
      "epoch= {} loss=  1.7994800600718008e-06\n",
      "epoch= {} loss=  1.795270804905158e-06\n",
      "epoch= {} loss=  1.7862685126601718e-06\n",
      "epoch= {} loss=  1.7707742472339305e-06\n",
      "epoch= {} loss=  1.7466782082919963e-06\n",
      "epoch= {} loss=  1.7944018964044517e-06\n",
      "epoch= {} loss=  1.7379134078510106e-06\n",
      "epoch= {} loss=  1.7166113366329228e-06\n",
      "epoch= {} loss=  1.7095212569984142e-06\n",
      "epoch= {} loss=  1.7015863704727963e-06\n",
      "epoch= {} loss=  1.6886071989574702e-06\n",
      "epoch= {} loss=  1.6806077383080265e-06\n",
      "epoch= {} loss=  1.6709850569895934e-06\n",
      "epoch= {} loss=  1.6714811863494106e-06\n",
      "epoch= {} loss=  1.665843683440471e-06\n",
      "epoch= {} loss=  1.6741472563808202e-06\n",
      "epoch= {} loss=  1.6596104615018703e-06\n",
      "epoch= {} loss=  1.6506860447407234e-06\n",
      "epoch= {} loss=  1.6401020275225164e-06\n",
      "epoch= {} loss=  1.6289009181491565e-06\n",
      "epoch= {} loss=  1.656006588746095e-06\n",
      "epoch= {} loss=  1.6232961570494808e-06\n",
      "epoch= {} loss=  1.6147571386682102e-06\n",
      "epoch= {} loss=  1.6069777757365955e-06\n",
      "epoch= {} loss=  1.6008491456886986e-06\n",
      "epoch= {} loss=  1.593956767464988e-06\n",
      "epoch= {} loss=  1.57957106239337e-06\n",
      "epoch= {} loss=  1.5494681520067388e-06\n",
      "epoch= {} loss=  1.5360602674263646e-06\n",
      "epoch= {} loss=  1.5105451893759891e-06\n",
      "epoch= {} loss=  1.4898060953782988e-06\n",
      "epoch= {} loss=  1.4707825357618276e-06\n",
      "epoch= {} loss=  1.4642289443145273e-06\n",
      "epoch= {} loss=  1.457688881600916e-06\n",
      "epoch= {} loss=  1.4489592103927862e-06\n",
      "epoch= {} loss=  1.4393323226613575e-06\n",
      "epoch= {} loss=  1.4313857263914542e-06\n",
      "epoch= {} loss=  1.4287686553871026e-06\n",
      "epoch= {} loss=  1.4236693459679373e-06\n",
      "epoch= {} loss=  1.4221039918993483e-06\n",
      "epoch= {} loss=  1.4207353160600178e-06\n",
      "epoch= {} loss=  1.4183098073772271e-06\n",
      "epoch= {} loss=  1.4164251069814782e-06\n",
      "epoch= {} loss=  1.4112707731328555e-06\n",
      "epoch= {} loss=  1.4036266975381295e-06\n",
      "epoch= {} loss=  1.3934620710642776e-06\n",
      "epoch= {} loss=  1.3957653663965175e-06\n",
      "epoch= {} loss=  1.38926702675235e-06\n",
      "epoch= {} loss=  1.3812879160468583e-06\n",
      "epoch= {} loss=  1.3777282674709568e-06\n",
      "epoch= {} loss=  1.3726158840654534e-06\n",
      "epoch= {} loss=  1.3695587313122815e-06\n",
      "epoch= {} loss=  1.3680615893463255e-06\n",
      "epoch= {} loss=  1.3669244935954339e-06\n",
      "epoch= {} loss=  1.3661957609656383e-06\n",
      "epoch= {} loss=  1.3658451507581049e-06\n",
      "epoch= {} loss=  1.3657059980687336e-06\n",
      "epoch= {} loss=  1.3654594113177154e-06\n",
      "epoch= {} loss=  1.3653059340867912e-06\n",
      "epoch= {} loss=  1.3651961126015522e-06\n",
      "epoch= {} loss=  1.3646937304656603e-06\n",
      "epoch= {} loss=  1.3645535545947496e-06\n",
      "epoch= {} loss=  1.3636131370731164e-06\n",
      "epoch= {} loss=  1.3629020259031677e-06\n",
      "epoch= {} loss=  1.3617459444503766e-06\n",
      "epoch= {} loss=  1.3593650010079728e-06\n",
      "epoch= {} loss=  1.3560959359892877e-06\n",
      "epoch= {} loss=  1.3548765309678856e-06\n",
      "epoch= {} loss=  1.3520142374545685e-06\n",
      "epoch= {} loss=  1.3512194527720567e-06\n",
      "epoch= {} loss=  1.3492696098182932e-06\n",
      "epoch= {} loss=  1.3475562354869908e-06\n",
      "epoch= {} loss=  1.344331167274504e-06\n",
      "epoch= {} loss=  1.3406495327217272e-06\n",
      "epoch= {} loss=  1.3368088502829778e-06\n",
      "epoch= {} loss=  1.335074330199859e-06\n",
      "epoch= {} loss=  1.3333969945961144e-06\n",
      "epoch= {} loss=  1.3314178204382188e-06\n",
      "epoch= {} loss=  1.3264182143757353e-06\n",
      "epoch= {} loss=  1.3225748034528806e-06\n",
      "epoch= {} loss=  1.3153917279851157e-06\n",
      "epoch= {} loss=  1.3130583056408796e-06\n",
      "epoch= {} loss=  1.325238486060698e-06\n",
      "epoch= {} loss=  1.3062156085652532e-06\n",
      "epoch= {} loss=  1.3094177120365202e-06\n",
      "epoch= {} loss=  1.3006740573473508e-06\n",
      "epoch= {} loss=  1.2962341315869708e-06\n",
      "epoch= {} loss=  1.2851634210164775e-06\n",
      "epoch= {} loss=  1.2721151279038168e-06\n",
      "epoch= {} loss=  1.272751319447707e-06\n",
      "epoch= {} loss=  1.2643125728573068e-06\n",
      "epoch= {} loss=  1.2562527444970328e-06\n",
      "epoch= {} loss=  1.2637035524676321e-06\n",
      "epoch= {} loss=  1.2513955880422145e-06\n",
      "epoch= {} loss=  1.2486391369748162e-06\n",
      "epoch= {} loss=  1.246650413122552e-06\n",
      "epoch= {} loss=  1.246255123987794e-06\n",
      "epoch= {} loss=  1.242611119778303e-06\n",
      "epoch= {} loss=  1.2417013977028546e-06\n",
      "epoch= {} loss=  1.240661504198215e-06\n",
      "epoch= {} loss=  1.2394207260513213e-06\n",
      "epoch= {} loss=  1.2386533398967003e-06\n",
      "epoch= {} loss=  1.2378391147649381e-06\n",
      "epoch= {} loss=  1.2368805073492695e-06\n",
      "epoch= {} loss=  1.2358248113741865e-06\n",
      "epoch= {} loss=  1.235083800565917e-06\n",
      "epoch= {} loss=  1.2340464081717073e-06\n",
      "epoch= {} loss=  1.2327254808042198e-06\n",
      "epoch= {} loss=  1.2307432371017057e-06\n",
      "epoch= {} loss=  1.2282829402465723e-06\n",
      "epoch= {} loss=  1.225008645633352e-06\n",
      "epoch= {} loss=  1.2193179372843588e-06\n",
      "epoch= {} loss=  1.2059994105584337e-06\n",
      "epoch= {} loss=  1.2008092653559288e-06\n",
      "epoch= {} loss=  1.1785044762291363e-06\n",
      "epoch= {} loss=  1.2163576457169256e-06\n",
      "epoch= {} loss=  1.170205905509647e-06\n",
      "epoch= {} loss=  1.5094402670001728e-06\n",
      "epoch= {} loss=  1.161296268037404e-06\n",
      "epoch= {} loss=  1.1557971220099716e-06\n",
      "epoch= {} loss=  1.139345727096952e-06\n",
      "epoch= {} loss=  1.137429990194505e-06\n",
      "epoch= {} loss=  1.134106582867389e-06\n",
      "epoch= {} loss=  1.1321776582917664e-06\n",
      "epoch= {} loss=  1.129872543970123e-06\n",
      "epoch= {} loss=  1.1276057421127916e-06\n",
      "epoch= {} loss=  1.1260171959293075e-06\n",
      "epoch= {} loss=  1.1244421784795122e-06\n",
      "epoch= {} loss=  1.1218926374567673e-06\n",
      "epoch= {} loss=  1.1171274536536657e-06\n",
      "epoch= {} loss=  1.106391437133425e-06\n",
      "epoch= {} loss=  1.0936987564491574e-06\n",
      "epoch= {} loss=  1.0778392152133165e-06\n",
      "epoch= {} loss=  1.099432552109647e-06\n",
      "epoch= {} loss=  1.069146605914284e-06\n",
      "epoch= {} loss=  1.0513176675885916e-06\n",
      "epoch= {} loss=  1.0328308235330041e-06\n",
      "epoch= {} loss=  1.0092662705574185e-06\n",
      "epoch= {} loss=  1.0354463029216276e-06\n",
      "epoch= {} loss=  9.969569418899482e-07\n",
      "epoch= {} loss=  1.088952672034793e-06\n",
      "epoch= {} loss=  9.831846909946762e-07\n",
      "epoch= {} loss=  1.0065101605505333e-06\n",
      "epoch= {} loss=  9.683353709988296e-07\n",
      "epoch= {} loss=  9.556684972267249e-07\n",
      "epoch= {} loss=  9.317922149421065e-07\n",
      "epoch= {} loss=  9.208963547280291e-07\n",
      "epoch= {} loss=  9.13368296551198e-07\n",
      "epoch= {} loss=  9.233419859810965e-07\n",
      "epoch= {} loss=  9.028041176861734e-07\n",
      "epoch= {} loss=  9.16003159545653e-07\n",
      "epoch= {} loss=  8.986234547592176e-07\n",
      "epoch= {} loss=  9.016283115670376e-07\n",
      "epoch= {} loss=  8.943272291617177e-07\n",
      "epoch= {} loss=  8.893933340914373e-07\n",
      "epoch= {} loss=  8.852939004100335e-07\n",
      "epoch= {} loss=  8.851806114762439e-07\n",
      "epoch= {} loss=  8.84327903349913e-07\n",
      "epoch= {} loss=  8.865672498359345e-07\n",
      "epoch= {} loss=  8.83649988736579e-07\n",
      "epoch= {} loss=  8.828669706417713e-07\n",
      "epoch= {} loss=  8.823208190733567e-07\n",
      "epoch= {} loss=  8.818125252219033e-07\n",
      "epoch= {} loss=  8.813635758997407e-07\n",
      "epoch= {} loss=  8.80828054050653e-07\n",
      "epoch= {} loss=  8.802512638794724e-07\n",
      "epoch= {} loss=  8.796249062470451e-07\n",
      "epoch= {} loss=  8.790782430878608e-07\n",
      "epoch= {} loss=  8.783521252553328e-07\n",
      "epoch= {} loss=  8.77500553997379e-07\n",
      "epoch= {} loss=  8.763382197685132e-07\n",
      "epoch= {} loss=  8.746802677706e-07\n",
      "epoch= {} loss=  8.723776545593864e-07\n",
      "epoch= {} loss=  8.703267440068885e-07\n",
      "epoch= {} loss=  8.692528581377701e-07\n",
      "epoch= {} loss=  8.69032760419941e-07\n",
      "epoch= {} loss=  8.68372012519103e-07\n",
      "epoch= {} loss=  8.67917492541892e-07\n",
      "epoch= {} loss=  8.67410278715397e-07\n",
      "epoch= {} loss=  8.656565455567033e-07\n",
      "epoch= {} loss=  8.641510476081748e-07\n",
      "epoch= {} loss=  8.617185471848643e-07\n",
      "epoch= {} loss=  8.590749303039047e-07\n",
      "epoch= {} loss=  8.570870591029234e-07\n",
      "epoch= {} loss=  8.54066001920728e-07\n",
      "epoch= {} loss=  8.506506219418952e-07\n",
      "epoch= {} loss=  8.472109698232089e-07\n",
      "epoch= {} loss=  8.491166454405175e-07\n",
      "epoch= {} loss=  8.445645107713062e-07\n",
      "epoch= {} loss=  8.431896389993199e-07\n",
      "epoch= {} loss=  8.410667078351253e-07\n",
      "epoch= {} loss=  8.384395187022164e-07\n",
      "epoch= {} loss=  8.362874268641463e-07\n",
      "epoch= {} loss=  8.33728222460195e-07\n",
      "epoch= {} loss=  8.31308966553479e-07\n",
      "epoch= {} loss=  8.295273801195435e-07\n",
      "epoch= {} loss=  8.307023904308153e-07\n",
      "epoch= {} loss=  8.284990258289326e-07\n",
      "epoch= {} loss=  8.275580967165297e-07\n",
      "epoch= {} loss=  8.265701580967288e-07\n",
      "epoch= {} loss=  8.258767820734647e-07\n",
      "epoch= {} loss=  8.247742471212405e-07\n",
      "epoch= {} loss=  8.240804163506255e-07\n",
      "epoch= {} loss=  8.234231358983379e-07\n",
      "epoch= {} loss=  8.227833063756407e-07\n",
      "epoch= {} loss=  8.219171832024585e-07\n",
      "epoch= {} loss=  8.210921578211128e-07\n",
      "epoch= {} loss=  8.203116976801539e-07\n",
      "epoch= {} loss=  8.196564067475265e-07\n",
      "epoch= {} loss=  8.188366678041348e-07\n",
      "epoch= {} loss=  8.183731097233249e-07\n",
      "epoch= {} loss=  8.175272228072572e-07\n",
      "epoch= {} loss=  8.165370672941208e-07\n",
      "epoch= {} loss=  8.160592983585957e-07\n",
      "epoch= {} loss=  8.15764735762059e-07\n",
      "epoch= {} loss=  8.153813269018428e-07\n",
      "epoch= {} loss=  8.14958980299707e-07\n",
      "epoch= {} loss=  8.146692493937735e-07\n",
      "epoch= {} loss=  8.144842240653816e-07\n",
      "epoch= {} loss=  8.143422292050673e-07\n",
      "epoch= {} loss=  8.142130809574155e-07\n",
      "epoch= {} loss=  8.141171292663785e-07\n",
      "epoch= {} loss=  8.140139016177272e-07\n",
      "epoch= {} loss=  8.139077181112953e-07\n",
      "epoch= {} loss=  8.135250482155243e-07\n",
      "epoch= {} loss=  8.117626748571638e-07\n",
      "epoch= {} loss=  8.08451432021684e-07\n",
      "epoch= {} loss=  8.064388339334982e-07\n",
      "epoch= {} loss=  8.044423225328501e-07\n",
      "epoch= {} loss=  8.062394272201345e-07\n",
      "epoch= {} loss=  8.023764621611917e-07\n",
      "epoch= {} loss=  8.008382792468183e-07\n",
      "epoch= {} loss=  7.990989843165153e-07\n",
      "epoch= {} loss=  7.982694683050795e-07\n",
      "epoch= {} loss=  7.978702001310012e-07\n",
      "epoch= {} loss=  7.973121682880446e-07\n",
      "epoch= {} loss=  7.970391493472562e-07\n",
      "epoch= {} loss=  7.968154704940389e-07\n",
      "epoch= {} loss=  7.967382771312259e-07\n",
      "Training time: 41.43\n",
      "Test Error: 0.00680\n"
     ]
    }
   ],
   "source": [
    "# '''Optimization'''\n",
    "\n",
    "# 'L-BFGS Optimizer'\n",
    "\n",
    "lbfgs_optimizer = optim.LBFGS(PINN.parameters(), lr=1.0, \n",
    "                               max_iter=10000,\n",
    "                               max_eval = None, \n",
    "                               tolerance_grad = 1e-10, \n",
    "                               tolerance_change = 1e-10, \n",
    "                               history_size = 150, \n",
    "                               line_search_fn = 'strong_wolfe')\n",
    "start_time = time.time()\n",
    " # 使用 LBFGS 进行优化\n",
    "def lbfgs_closure():\n",
    "    lbfgs_optimizer.zero_grad()\n",
    "    loss = PINN.loss(xy_0, u_0, xy_left, xy_right, xy_top, xy_bottom, X_f_train)\n",
    "    print('epoch=', lbfgs_optimizer.state['n_iter'], 'loss= ', loss.item())\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# 自动迭代次数\n",
    "lbfgs_optimizer.step(lbfgs_closure)\n",
    "    \n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.2f' % elapsed)\n",
    "\n",
    "''' Model Accuracy '''\n",
    "error_vec, u_pred = PINN.test()\n",
    "print('Test Error: %.5f' % error_vec)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " #optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    " #optimizer.step(PINN.closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_u_test.shape (10000, 2)\n",
      " u_pred.shape =  (100, 100)\n"
     ]
    }
   ],
   "source": [
    " #存数据\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_u_test and u_pred are NumPy arrays\n",
    "data_exact = np.column_stack((X_u_test, u_true))\n",
    "np.savetxt('exact_mm100_TM11.txt', data_exact, delimiter=',', header='x_1,x_2,u_true', comments='')\n",
    "\n",
    "print('X_u_test.shape',X_u_test.shape)\n",
    "print(' u_pred.shape = ', u_pred.shape)\n",
    "\n",
    "data_pinn = np.column_stack((X_u_test, u_pred.reshape(-1,1)))\n",
    "np.savetxt('pinn_mm100_TM11.txt', data_pinn, delimiter=',', header='x_1,x_2,u_pred', comments='')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T12:30:29.476066600Z",
     "start_time": "2024-11-11T12:30:29.208664100Z"
    }
   },
   "id": "92a763afe26205a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0febe97be706365",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-29T01:26:46.834402Z",
     "iopub.status.busy": "2023-11-29T01:26:46.834402Z",
     "iopub.status.idle": "2023-11-29T01:26:49.164104Z",
     "shell.execute_reply": "2023-11-29T01:26:49.164104Z",
     "shell.execute_reply.started": "2023-11-29T01:26:46.834402Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-11T12:31:14.126947600Z",
     "start_time": "2024-11-11T12:31:08.053227500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1800x500 with 6 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solutionplot(u_pred,X_u_train,u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88e1e663713e1b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T06:49:39.217574700Z",
     "start_time": "2023-11-24T06:49:39.210585400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aafd01f20db7c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
